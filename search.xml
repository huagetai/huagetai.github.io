<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>kafka生产者消息序列化</title>
      <link href="/posts/fdfb7393/"/>
      <url>/posts/fdfb7393/</url>
      
        <content type="html"><![CDATA[<h1 id="kafka消息序列化和反序列化"><a href="#kafka消息序列化和反序列化" class="headerlink" title="kafka消息序列化和反序列化"></a>kafka消息序列化和反序列化</h1><p>生产者需要用序列化器把对象转换成字节数组才能通过网络发送给Kafka集群。而在另一端，消费者需要用相应的反序列化器把从Kafka集群收到的字节数组转换成相应的对象才能做后续处理。</p><img src="/posts/fdfb7393/001.png"><p>Serializer是Kafka提供的序列化接口：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">byte</span>[] <span class="title">serialize</span><span class="params">(String topic, T data)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">default</span> <span class="keyword">byte</span>[] <span class="title">serialize</span><span class="params">(String topic, Headers headers, T data)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><p>Deserializer是Kafka提供的反序列化接口：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span> </span></span><br><span class="line"><span class="function"><span class="title">deserialize</span><span class="params">(String topic, <span class="keyword">byte</span>[] data)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">default</span> T <span class="title">deserialize</span><span class="params">(String topic, Headers headers, <span class="keyword">byte</span>[] data)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><p>序列化和反序列化接口中，serialize方法和deserialize方法分别用于执行序列化和反序列化操作。close方法一般是空的不需要重写，如果要实现该方法，必须确保此方法是幂等的，它可能会被调用多次.</p><p>生产者常用的序列化器有：StringSerializer、UUIDSerializer、ByteArraySerializer、ByteBufferSerializer、BytesSerializer、DoubleSerializer、FloatSerializer、IntegerSerializer、LongSerializer、ShortSerializer；消费者常用的反序列化器和上面的生产者常用序列化器一一对应。比如：StringDeserializer、UUIDDeserializer等等</p><p>当常用的无法满足需求时，可以使用Avro、JSON、Thrift、ProtoBuf、Protostuff等等通用的序列化工具来实现。Spring Kafka项目就提供了JsonSerializer和JsonDeserializer的实现。</p><h1 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h1><p><a href="https://www.jianshu.com/p/98b15a0e6806" target="_blank" rel="noopener">什么是序列化和反序列化</a></p>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka生产者 </tag>
            
            <tag> 序列化 </tag>
            
            <tag> 反序列化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MurmurHash算法</title>
      <link href="/posts/fcfde8ff/"/>
      <url>/posts/fcfde8ff/</url>
      
        <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>MurmurHash 是一种非加密型哈希函数，适用于一般的哈希检索操作。由Austin Appleby在2008年发明，并出现了多个变种，都已经发布到了公有领域。该名称来自两个基本操作，乘法（MU）和旋转（R），在其内部循环中使用。与其它流行的哈希函数相比，对于规律性较强的key，MurmurHash的随机分布特征表现更良好。<br>MurmurHash与加密散列函数不同，它不是专门设计为难以被对手逆转，因此不适用于加密目的。它常被应用于分布式系统，很多开源项目如Kafka、Redis，Memcached，Cassandra，HBase，Elasticsearch等等都使用它。<br>MurmurHash的当前的版本是MurmurHash3，能够产生出32-bit或128-bit哈希值。</p><!-- more --><h1 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h1><ul><li>速度快，比安全散列算法快几十倍；</li><li>变化足够激烈，相似的字符串如“abc”和“abd”能够均匀散落在哈希环上；</li><li>不保证安全性（缺点）；</li></ul><h1 id="种类"><a href="#种类" class="headerlink" title="种类"></a>种类</h1><p><strong>Murmurhash3</strong><br>2018年的版本是Murmurhash3，它产生一个32位或128位散列值。 使用128位时，x86和x64版本不会生成相同的值，因为算法针对各自的平台进行了优化。<br><strong>Murmurhash2</strong><br>Murmurhash2产生一个32位或64位的值。 较慢版本的Murmurhash2可用于大端和对齐的机器。 Murmurhash2A变体添加了Merkle-Damgård构造，因此可以逐渐调用它。 有两种变体生成64位值; 针对64位处理器的Murmurhash64A和针对32位处理器的Murmurhash64B。 Murmurhash2-160生成160位散列，而Murmurhash1已过时。</p><h1 id="Murmurhash3算法"><a href="#Murmurhash3算法" class="headerlink" title="Murmurhash3算法"></a>Murmurhash3算法</h1><p><strong>算法描述</strong></p><img src="/posts/fcfde8ff/001.png"><p><strong>算法图例</strong></p><img src="/posts/fcfde8ff/002.png"><h1 id="开源实现"><a href="#开源实现" class="headerlink" title="开源实现"></a>开源实现</h1><p>Guava的Hashing类，Jedis和Cassandra的Util类均提供了MurmurHash算法的Java实现</p><h1 id="PK"><a href="#PK" class="headerlink" title="PK"></a>PK</h1><p><a href="https://www.jianshu.com/p/433d93015467" target="_blank" rel="noopener">Time33算法</a><br><a href="https://github.com/google/cityhash" target="_blank" rel="noopener">CityHash</a></p><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="哈希"><a href="#哈希" class="headerlink" title="哈希"></a>哈希</h2><p><strong>什么是哈希</strong><br>hash（散列、杂凑）函数，是将任意长度的数据映射到固定长度的域上。<br>即将一段数据M进行杂糅，然后输出一段数据h。作为他的数据特征（指纹）<br>即无论m多长，输出的h的长度是固定的。<br>Hash表采用一个映射函数 f : key —&gt; address 将关键字映射到该记录在表中的存储位置，从而在想要查找该记录时，可以直接根据关键字和映射关系计算出该记录在表中的存储位置.<br>通常情况下，这种映射关系称作为Hash函数，而通过Hash函数和关键字计算出来的存储位置(注意这里的存储位置只是表中的存储位置，并不是实际的物理地址)称作为Hash地址.<br>比如上述例子中，假如联系人信息采用Hash表存储，则当想要找到“李四”的信息时，直接根据“李四”和Hash函数计算出Hash地址即可</p><p><strong>哈希函数</strong><br>hash函数就是把任意长的输入字符串变化成固定长的输出字符串的一种函数。输出字符串的长度称为hash函数的位数。<br>一句话：散列（Hashing）通过散列函数将要检索的项与索引（散列，散列值）关联起来，生成一种便于搜索的数据结构（散列表）。</p><p><strong>哈希函数的性质</strong></p><ul><li>同一函数的散列值不相同，那么其原始输入也不相同，上图中k1，k3和k4。（确定性）</li><li>散列函数的输入和输出不是唯一对应关系的，如果两个散列值相同，两个输入值很可能是不相同的，上图中的k2，k5这种情况称为“哈希碰撞”。（不确定性）</li></ul><p><strong>hash函数的构造准则：简单、均匀</strong></p><ul><li>散列函数的计算简单，快速；</li><li>散列函数能将关键字集合K均匀地分布在地址集{0,1，…，m-1}上，使冲突最小。</li></ul>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> murmurhash </tag>
            
            <tag> 哈希函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka生产者消息分区机制</title>
      <link href="/posts/fabbb24d/"/>
      <url>/posts/fabbb24d/</url>
      
        <content type="html"><![CDATA[<p>我们在使用 Apache Kafka 生产和消费消息的时候，肯定是希望能够将数据均匀地分配到所有服务器上。比如很多公司使用 Kafka 收集应用服务器的日志数据，这种数据都是很多的，特别是对于那种大批量机器组成的集群环境，每分钟产生的日志量都能以 GB 数，因此如何将这么大的数据量均匀地分配到 Kafka 的各个 Broker 上，就成为一个非常重要的问题。那么 Kafka 生产者如何实现这个需求呢。</p><h1 id="为什么分区（partition）？"><a href="#为什么分区（partition）？" class="headerlink" title="为什么分区（partition）？"></a>为什么分区（partition）？</h1><p>对于每一个主题（topic）， kafka集群都会维持一个分区日志，如下所示：</p><img src="/posts/fabbb24d/001.jpeg"><p>每个分区都是有序且顺序不可变的消息记录集，消息被不断地追加到commit log文件末尾。主题下的每条消息只会保存在某一个分区中。kafka为什么要分区呢？分区主要有以下几个原因：</p><p>第一，当日志大小超过了单台服务器的限制，允许日志进行扩展。每个单独的分区都必须受限于主机的文件限制，不过一个主题可能有多个分区，因此可以处理无限量的数据。</p><p>第二，提供负载均衡的能力。不同的分区分布在Kafka集群不同的节点服务器上。每个服务器在处理数据和请求时，共享这些分区。每一个分区都会在已配置的服务器上进行备份，确保容错性。每个分区都有一台 server 作为leader，零台或者多台server作为follwers 。leader server 处理一切对分区的读写请求，而follwers只需被动的同步leader上的数据。当leader宕机了，followers 中的一台服务器会自动成为新的 leader。每台 server 都会成为某些分区的 leader 和某些分区的 follower，因此集群的负载是平衡的。并且，我们还可以通过添加新的节点机器来增加整体系统的吞吐量。</p><p>第三，除了提供负载均衡这种最核心的功能之外，利用分区也可以实现其他一些业务级别的需求，例如，如果使用用户ID作为key，则用户相关的所有数据都会被分发到同一个分区上。 这允许消费者在消费数据时做一些特定的本地化处理。</p><h1 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h1><img src="/posts/fabbb24d/002.png"><p>所谓分区策略是决定生产者将消息发送到哪个分区的算法。Kafka 为我们提供了默认的分区策略，同时它也支持你自定义分区策略。</p><h2 id="Partitioner接口"><a href="#Partitioner接口" class="headerlink" title="Partitioner接口"></a>Partitioner接口</h2><p>这个接口很简单，只定义了两个方法:partition()和close()，通常你只需要实现最重要的 partition 方法。我们来看看这个方法的方法签名：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span></span>;</span><br></pre></td></tr></table></figure><p>这里的topic、key、keyBytes、value和valueBytes都属于消息数据，cluster则是集群信息（可以获得一个主题的所有分区信息列表和available分区信息列表等等）。我们能够充分地利用这些信息对消息进行分区，计算出它要被发送到哪个分区中。只要你自己的实现类定义好了 partition 方法，同时设partitioner.class参数为你自己实现类的 Full Qualified Name，那么生产者程序就会按照你的代码逻辑对消息进行分区。</p><h2 id="随机策略"><a href="#随机策略" class="headerlink" title="随机策略"></a>随机策略</h2><p>也称 Randomness 策略。所谓随机就是我们随意地将消息放置到任意一个分区上，如下面这张图所示。</p><img src="/posts/fabbb24d/003.png"><p>如果要实现随机策略版的 partition 方法，很简单，只需要两行代码即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line"><span class="keyword">return</span> ThreadLocalRandom.current().nextInt(partitions.size());`</span><br></pre></td></tr></table></figure><p>先计算出该主题总的分区数，然后随机地返回一个小于它的正整数。</p><p>本质上看随机策略是力求将数据均匀地打散到各个分区，但从实际表现来看，它要逊于下面介绍的轮询策略，目前已经弃用。</p><h2 id="轮询策略"><a href="#轮询策略" class="headerlink" title="轮询策略"></a>轮询策略</h2><p>RoundRobinPartitioner，也称 Round-robin 策略，即顺序分配。比如一个主题下有 3 个分区，那么第一条消息被发送到分区 0，第二条被发送到分区 1，第三条被发送到分区 2，以此类推。当生产第 4 条消息时又会重新开始，即将其分配到分区 0，就像下面这张图展示的那样。</p><img src="/posts/fabbb24d/004.png"><p>这就是所谓的轮询策略。轮询策略有非常优秀的负载均衡表现，它总是能保证消息最大限度地被平均分配到所有分区上。但是在吞吐量较小的情况下，也会导致更多批次，而这些批次的大小更小，其实，将所有记录转到指定的分区（或几个分区）并以更大的批记录一起发送似乎会更好。在key=null时，轮询策略是2.3版本默认使用的分区策略，2.4版本（KIP-480）引入了更好的黏性分区策略。</p><h2 id="黏性分区策略"><a href="#黏性分区策略" class="headerlink" title="黏性分区策略"></a>黏性分区策略</h2><p>UniformStickyPartitioner，黏性分区策略。</p><p>消息记录会成批的从生产者发送到Broker。生产者触发发送请求的时机由批记录的大小参数和linger.ms参数决定，批记录的大小达到设定的值或linger.ms参数时间到，都会触发批记录的发送。因此，批记录的大小对生产者到Broker发送延迟是有影响的。较小的批记录会导致更多的请求和排队从而导致更高的延迟。这意味着，在关闭linger.ms的情况下（即将linger.ms参数设置为零），较大的批记录也会减少延迟。在启用linger.ms的情况下，低吞吐量通常会向系统中注入延迟，因为如果没有足够的记录来填充批记录，则需要等到linger.ms设定的参数值才会发送该批记录。在linger.ms值之前找到增加批记录大小以触发发送的方法将进一步减少延迟。</p><p>黏性分区策略通过“黏贴”到分区直到批记录已满（或在linger.ms启动时发送），与轮询策略相比，我们可以创建更大的批记录并减少系统中的延迟。即使在linger.ms为零立即发送的情况下，也可以看到改进的批处理和减少的延迟。在创建新批处理时更改粘性分区，随着时间的流逝，记录应该在所有分区之间是平均分配的。2.4版本key=null时默认使用黏性分区策略</p><h2 id="按消息key分区策略"><a href="#按消息key分区策略" class="headerlink" title="按消息key分区策略"></a>按消息key分区策略</h2><p>Kafka 允许为每条消息定义消息键，简称为 Key。这个 Key 的作用非常大，它可以是一个有着明确业务含义的字符串，比如客户代码、部门编号或是业务 ID 等；也可以用来表征消息元数据。特别是在 Kafka 不支持时间戳的年代，在一些场景中，工程师们都是直接将消息创建时间封装进 Key 里面的。一旦消息被定义了 Key，那么你就可以保证同一个 Key 的所有消息都进入到相同的分区里面，如下图所示。</p><img src="/posts/fabbb24d/005.png"><h2 id="自定义分区策略"><a href="#自定义分区策略" class="headerlink" title="自定义分区策略"></a>自定义分区策略</h2><p>一种比较常见的，即所谓的基于地理位置的分区策略。当然这种策略一般只针对那些大规模的 Kafka 集群，特别是跨城市、跨国家甚至是跨大洲的集群。</p><p>我就拿“美团app”举个例子吧，假设美团app的所有服务都部署在北京的一个机房（这里我假设它是自建机房，不考虑公有云方案。其实即使是公有云，实现逻辑也差不多），现在美团app考虑在南方找个城市（比如广州）再创建一个机房；另外从两个机房中选取一部分机器共同组成一个大的 Kafka 集群。显然，这个集群中必然有一部分机器在北京，另外一部分机器在广州。</p><p>假设美团app计划为每个新注册用户提供一份注册礼品，比如南方的用户注册极客时间可以免费得到一碗“甜豆腐脑”，而北方的新注册用户可以得到一碗“咸豆腐脑”。如果用 Kafka 来实现则很简单，只需要创建一个双分区的主题，然后再创建两个消费者程序分别处理南北方注册用户逻辑即可。</p><p>但问题是你需要把南北方注册用户的注册消息正确地发送到位于南北方的不同机房中，因为处理这些消息的消费者程序只可能在某一个机房中启动着。换句话说，送甜豆腐脑的消费者程序只在广州机房启动着，而送咸豆腐脑的程序只在北京的机房中，如果你向广州机房中的 Broker 发送北方注册用户的消息，那么这个用户将无法得到礼品！</p><p>此时我们就可以根据 Broker 所在的 IP 地址实现定制化的分区策略。比如下面这段代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); </span><br><span class="line"><span class="keyword">return</span> partitions.stream().filter(p -&gt; isSouth(p.leader().host())).map(PartitionInfo::partition).findAny().get();</span><br></pre></td></tr></table></figure><p>我们可以从所有分区中找出那些 Leader 副本在南方的所有分区，然后随机挑选一个进行消息发送。</p><h2 id="DefaultPartitioner默认分区策略"><a href="#DefaultPartitioner默认分区策略" class="headerlink" title="DefaultPartitioner默认分区策略"></a>DefaultPartitioner默认分区策略</h2><p>key不为null时，对key进行hash（基于murmurHash2算法），根据最终得到的hash值计算分区号，有相同key的消息会被写入同样的分区；key为null时，2.3版本使用轮询分区策略，2.4版本使用黏性分区策略。</p><h1 id="生产者分区逻辑"><a href="#生产者分区逻辑" class="headerlink" title="生产者分区逻辑"></a>生产者分区逻辑</h1><p>指定partition的情况下，直接发往该分区；没有指定partition的情况下，根据分区策略确定发往的分区。配置了自定义分区策略的情况下使用自定义的分区策略，没有配置的情况下使用默认分区策略DefaultPartitioner。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>分区是实现负载均衡以及高吞吐量的关键，故在生产者这一端就要仔细盘算合适的分区策略，避免造成消息数据的“倾斜”，使得某些分区成为性能瓶颈，这样极易引发下游数据消费的性能下降。</p><ul><li><p>Kafka的消息组织方式实际上是三级结构：主题-分区-消息。主题下的每个消息只会保存在某一个分区中</p></li><li><p>为什么要分区</p></li><li><ul><li>无限存储</li><li>负载均衡</li><li>特定业务保序需求</li></ul></li><li><p>分区策略：是决定生产者将消息发送到那个分区的算法</p></li><li><ul><li>随机分区策略-randomness</li><li>轮询分区策略-round-robin</li><li>黏性分区策略-sticky partitioner</li><li>按消息键分区策略</li><li>自定义，比如：基于地理位置的分区策略</li><li>默认分区策略DefaultPartitioner：key不为null时，对key进行hash（基于murmurHash2算法），根据最终得到的hash值计算分区号，有相同key的消息会被写入同样的分区；key为null时，2.3版本使用轮询分区策略，2.4版本使用黏性分区策略。</li></ul></li><li><p>生产者分区逻辑：指定partition的情况下，直接发往该分区；没有指定partition的情况下，根据分区策略确定发往的分区。配置了自定义分区策略的情况下使用自定义的分区策略，没有配置的情况下使用默认分区策略DefaultPartitioner。</p></li></ul><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="KIP-480：黏性分区策略（Sticky-Partitioner）"><a href="#KIP-480：黏性分区策略（Sticky-Partitioner）" class="headerlink" title="KIP-480：黏性分区策略（Sticky Partitioner）"></a>KIP-480：黏性分区策略（Sticky Partitioner）</h2><p>原文：<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-480%3A+Sticky+Partitioner" target="_blank" rel="noopener">KIP480:Sticky Partitioner</a></p><p><strong>动机</strong></p><p>消息记录会成批的从生产者发送到Broker。生产者触发发送批记录（record batches）请求时机由批记录的大小参数和linger.ms参数决定，批记录的大小达到设定的值或linger.ms参数时间到，都会触发批记录的发送。因此，批记录的大小对生产者到Broker发送延迟是有影响的。较小的批记录会导致更多的请求和排队从而导致更高的延迟。这意味着，在关闭linger.ms的情况下（即将linger.ms参数设置为零），较大的批记录也会减少延迟。在启用linger.ms的情况下，低吞吐量通常会向系统中注入延迟，因为如果没有足够的记录来填充批记录，则需要等到linger.ms设定的参数值才会发送该批记录。在linger.ms值之前找到增加批记录大小以触发发送的方法将进一步减少延迟。</p><p>当前（2.3版本），在未指定分区（partition）且未指定key的情况下，默认分区器（DefaultPartitioner）将以循环方式对记录进行分区。这意味着一系列连续记录中的每个记录将被顺序发送到每一个分区，直到我们用尽分区再重新开始。尽管这会在各个分区之间平均分配记录，但也会导致更多批次，而这些批次的大小更小。将所有记录转到指定的分区（或几个分区）并以更大的批记录一起发送似乎会更好。</p><p>黏性分区器尝试在分区器中创建类似行为。通过“黏贴”到分区直到批记录已满（或在linger.ms启动时发送），与默认分区器相比，我们可以创建更大的批记录并减少系统中的延迟。即使在linger.ms为零我们立即发送的情况下，我们也可以看到改进的批处理和减少的延迟。发送批记录后，黏性分区会更改。随着时间的流逝，记录应该在所有分区之间是平均分配的。</p><p>Netflix有一个类似的想法，并创建了一个黏性分区器，该分区器选择一个分区并在给定的时间段内将所有记录发送到该分区，然后再切换到新分区。</p><p>另一种方法是在创建新批处理时更改粘性分区。这样做的目的是最大程度地减少可能在不合时宜的分区交换时机上参生更多的空批记录。我们介绍的黏性分区器将使用这个方法。</p><p><strong>Partitioner接口</strong></p><p>粘性分区将是默认分区器的一部分，因此不会直接有公共接口。<br>Partiton接口增加一个新的Public方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> *Executes right before a new batch will be created. </span><br><span class="line"> *For example, if a sticky partitioner is used, </span><br><span class="line"> *this method can change the chosen sticky partition for the new batch. </span><br><span class="line"> *@param topic The topic name </span><br><span class="line"> *@param cluster The current cluster metadata </span><br><span class="line"> *@param prevPartition The partition of the batch that was just completed </span><br><span class="line"> */</span><br><span class="line"> default public void onNewBatch(String topic, Cluster cluster, int prevPartition) &#123;&#125;</span><br></pre></td></tr></table></figure><p>onNewBatch方法将在创建新批记录之前立即执行代码。 粘性分区程序将定义此方法来更新粘性分区。 这包括更改粘性分区，即使键值上将有新批次也是如此。 测试结果表明，在键值情况下，此更改不会显著影响延迟。</p><p>此方法的默认设置将不会更改当前用户定义的其他分区器的分区行为。 如果用户想在自己的分区器中实现粘性分区，则可以重写此方法。</p><p><strong>拟议的变更</strong></p><ul><li>在无显式分区（key = null）的情况下更改默认分区器的行为。 选择给定Topic的“粘性”分区。 当记录累加器为给定分区上的主题分配新批次时，“粘性”分区将更改。</li><li>这些更改也会略微修改具有键的记录的代码路径，但是这些更改不会显着影响延迟。</li><li>将创建一个名为UniformStickyPartitioner的新分区器，以允许对所有记录进行粘性分区，即使是那些具有非空键的记录也是如此。 这将镜像到RoundRobinPartitioner如何对所有记录（包括具有键的记录）使用循环分区策略。</li></ul><p><strong>兼容性，弃用和迁移计划</strong></p><ul><li>无需兼容处理，无需弃用，无需迁移计划。</li><li>用户可以继续使用自己的分区器-如果要实现粘性分区程序，可以使用onNewBatch方法来实现功能，如果他们不想使用该功能，则行为是相同的。</li><li>默认分区器在key=null，未设置partition值时，用户应看到延迟和CPU使用率是相同的或有减少的</li></ul><p><strong>测试结果</strong></p><p>通常，与当前代码相比，粘性分区器通常会使延迟减少一半。 在最坏的情况下，也能达到默认代码标准。</p><p>随着分区的增加，看到更多的好处。 尽管如此，使用16个分区，仍然可以看到明显的好处。 在1000 msg / sec的吞吐量下，延迟仍然约为默认的一半。</p><img src="/posts/fabbb24d/006.jpeg"><img src="/posts/fabbb24d/007.jpeg"><p>观察到的另一个趋势，尤其是在刷新情况下，随着发送的消息数量从低吞吐量增加到中吞吐量，等待时间减少更多。好处部分取决于每秒消息与分区的比率。</p><p>最后，在linger.ms不为零且吞吐量低到足以让默认代码需要在linger.ms上等待的情况下，显然有好处。例如，以1个生产者，16个分区和1000 msg / sec以及linger.ms = 1000运行时，粘性分区器的p99延迟为204，而默认值为1017。这大约是等待时间的1/3，这是因为批处理不必等待linger.ms。</p><p>除了延迟之外，与默认代码相比，粘性分区程序还发现CPU利用率下降。在观察到的情况下，与默认分区相比，粘性分区的节点通常会降低多达5-15％的CPU使用率（例如，从9-17％降低到5-12.5％或从30-40％降低到15-25％）代码的节点。</p><img src="/posts/fabbb24d/008.jpeg"><img src="/posts/fabbb24d/009.jpeg"><p>（Vnl是默认情况，而Chc是粘性分区程序。这是1个生产者，16个分区，10,000 msg / sec无刷新情况的结果。）</p><p><strong>拒绝选择</strong></p><ul><li><p>拒绝选择可配置的粘性分区器的原因：测试表明，粘性分区程序在cpu利用率和延迟方面均达到或优于默认分区。 将粘性分区程序设置为可配置功能意味着某些用户可能会错过此有益功能</p></li><li><p>拒绝选择基于时间变化粘性分区的原因：变更时间将根据吞吐量而有所不同，需要针对不同的情况进行设置，吞吐量可能不一致</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
            <tag> kafka生产者 </tag>
            
            <tag> 分区策略 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka设计思想</title>
      <link href="/posts/9bcd2d2c/"/>
      <url>/posts/9bcd2d2c/</url>
      
        <content type="html"><![CDATA[<h2 id="动机（Motivation"><a href="#动机（Motivation" class="headerlink" title="动机（Motivation)"></a>动机（Motivation)</h2><p>我们设计的 Kafka 能够作为一个统一的平台来处理大公司可能拥有的所有实时数据feeds。 要做到这点，我们必须考虑相当广泛的用例。</p><p>Kafka 必须具有高吞吐量来支持高容量事件流，例如实时日志聚合。</p><p>Kafka 需要能够正常处理大量的数据积压，以便能够支持来自离线系统的周期性数据加载。</p><p>这也意味着系统必须处理低延迟分发，来处理更传统的消息传递用例。</p><p>我们希望支持对这些feeds进行分区，分布式，以及实时处理来创建新的分发feeds等特性。由此产生了我们的分区模式和消费者模式。</p><p>最后，在数据流被推送到其他数据系统进行服务的情况下，我们要求系统在出现机器故障时必须能够保证容错。</p><p>为支持这些使用场景导致我们设计了一些独特的元素，使得 Kafka 相比传统的消息系统更像是数据库日志。我们将在后面的章节中概述设计中的部分要素。</p><h2 id="持久化（Persistence"><a href="#持久化（Persistence" class="headerlink" title="持久化（Persistence)"></a>持久化（Persistence)</h2><h3 id="不要害怕文件系统"><a href="#不要害怕文件系统" class="headerlink" title="不要害怕文件系统"></a>不要害怕文件系统</h3><p>Kafka 对消息的存储和缓存严重依赖于文件系统。人们对于“磁盘速度慢”的普遍印象，使得人们对于持久化的架构能够提供强有力的性能产生怀疑。事实上，磁盘的速度比人们预期的要慢的多，还是快得多，这取决于人们使用磁盘的方式。而且设计合理的磁盘结构通常可以和网络一样快。</p><p>关于磁盘性能的关键事实是，硬盘的吞吐量与磁盘seek的延迟是不匹配的。因此，使用6个7200rpm、SATA接口、RAID-5的磁盘阵列在<a href="http://en.wikipedia.org/wiki/Non-RAID_drive_architectures" target="_blank" rel="noopener">JBOD</a>配置下的顺序写入的性能约为600MB/秒，但随机写入的性能仅约为100k/秒，相差6000倍以上。因为线性的读取和写入是磁盘使用模式中最有规律的，并且由操作系统进行了大量的优化。现代操作系统提供了read-ahead 和 write-behind 技术，read-ahead 是以大的 data block 为单位预先读取数据，而 write-behind 是将多个小型的逻辑写合并成一次大型的物理磁盘写入。关于该问题的进一步讨论可以参考<a href="http://queue.acm.org/detail.cfm?id=1563874" target="_blank" rel="noopener"> ACM Queue article</a>，他们发现实际上<a href="http://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg" target="_blank" rel="noopener">顺序磁盘访问在某些情况下比随机内存访问还要快</a>！</p><p>为了弥补这种性能差异，现代操作系统在越来越注重使用内存对磁盘进行 cache。现代操作系统主动将所有空闲内存用作 disk caching，代价是在内存回收时性能会有所降低。所有对磁盘的读写操作都会通过这个统一的 cache。如果不使用直接I/O，该功能不能轻易关闭。因此即使进程维护了 in-process cache，该数据也可能会被复制到操作系统的 pagecache 中，事实上所有内容都被存储了两份。</p><p>此外，Kafka 建立在 JVM 之上，任何了解 Java 内存使用的人都知道两点：</p><ol><li>对象的内存开销非常高，通常是所存储的数据的两倍(甚至更多)。</li><li>随着堆中数据的增加，Java 的垃圾回收变得越来越复杂和缓慢。</li></ol><p>受这些因素影响，相比于维护 in-memory cache 或者其他结构，使用文件系统和 pagecache 显得更有优势–我们可以通过自动访问所有空闲内存将可用缓存的容量至少翻倍，并且通过存储紧凑的字节结构而不是独立的对象，有望将缓存容量再翻一番。 这样使得32GB的机器缓存容量可以达到28-30GB,并且不会产生额外的 GC 负担。此外，即使服务重新启动，缓存依旧可用，而 in-process cache 则需要在内存中重建(重建一个10GB的缓存可能需要10分钟)，否则进程就要从 cold cache 的状态开始(这意味着进程最初的性能表现十分糟糕)。 这同时也极大的简化了代码，因为所有保持 cache 和文件系统之间一致性的逻辑现在都被放到了 OS 中，这样做比一次性的进程内缓存更准确、更高效。如果你的磁盘使用更倾向于顺序读取，那么 read-ahead 可以有效的使用每次从磁盘中读取到的有用数据预先填充 cache。</p><p>这里给出了一个非常简单的设计：相比于维护尽可能多的 in-memory cache，并且在空间不足的时候匆忙将数据 flush 到文件系统，我们把这个过程倒过来。所有数据一开始就被写入到文件系统的持久化日志中，而不用在 cache 空间不足的时候 flush 到磁盘。实际上，这表明数据被转移到了内核的 pagecache 中。</p><p>这种 pagecache-centric 的设计风格出现在一篇关于 <a href="http://varnish-cache.org/wiki/ArchitectNotes" target="_blank" rel="noopener">Varnish</a> 设计的文章中。</p><h3 id="常量时间就足够了"><a href="#常量时间就足够了" class="headerlink" title="常量时间就足够了"></a>常量时间就足够了</h3><p>消息系统使用的持久化数据结构通常是和 BTree 相关联的消费者队列或者其他用于存储消息源数据的通用随机访问数据结构。BTree 是最通用的数据结构，可以在消息系统能够支持各种事务性和非事务性语义。 虽然 BTree 的操作复杂度是 O(log N)，但成本也相当高。通常我们认为 O(log N) 基本等同于常数时间，但这条在磁盘操作中不成立。磁盘寻址是每10ms一跳，并且每个磁盘同时只能执行一次寻址，因此并行性受到了限制。 因此即使是少量的磁盘寻址也会很高的开销。由于存储系统将非常快的cache操作和非常慢的物理磁盘操作混合在一起，当数据随着 fixed cache 增加时，可以看到树的性能通常是非线性的——比如数据翻倍时性能下降不只两倍。</p><p>所以直观来看，持久化队列可以建立在简单的读取和向文件后追加两种操作之上，这和日志解决方案相同。这种架构的优点在于所有的操作复杂度都是O(1)，而且读操作不会阻塞写操作，读操作之间也不会互相影响。这有着明显的性能优势，由于性能和数据大小完全分离开来——服务器现在可以充分利用大量廉价、低转速的1+TB SATA硬盘。 虽然这些硬盘的寻址性能很差，但他们在大规模读写方面的性能是可以接受的，而且价格是原来的三分之一、容量是原来的三倍。</p><p>在不产生任何性能损失的情况下能够访问几乎无限的硬盘空间，这意味着我们可以提供一些其它消息系统不常见的特性。例如：在 Kafka 中，我们可以让消息保留相对较长的一段时间(比如一周)，而不是试图在被消费后立即删除。正如我们后面将要提到的，这给消费者带来了很大的灵活性。</p><h2 id="效率（Efficiency）"><a href="#效率（Efficiency）" class="headerlink" title="效率（Efficiency）"></a>效率（Efficiency）</h2><p>我们在性能上已经做了很大的努力。 我们主要的使用场景是处理WEB活动数据，这个数据量非常大，因为每个页面都有可能大量的写入。此外我们假设每个发布 message 至少被一个consumer (通常很多个consumer) 消费， 因此我们尽可能的去降低消费的代价。</p><p>我们还发现，从构建和运行许多相似系统的经验上来看，性能是多租户运营的关键。如果下游的基础设施服务很轻易被应用层冲击形成瓶颈，那么一些小的改变也会造成问题。通过非常快的(缓存)技术，我们能确保应用层冲击基础设施之前，将负载稳定下来。 当尝试去运行支持集中式集群上成百上千个应用程序的集中式服务时，这一点很重要，因为应用层使用方式几乎每天都会发生变化。</p><h3 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h3><p>我们在上一节讨论了磁盘性能。 一旦消除了磁盘访问模式不佳的情况，该类系统性能低下的主要原因就剩下了两个：大量的小型 I/O 操作，以及过多的字节拷贝。</p><p>小型的 I/O 操作发生在客户端和服务端之间以及服务端自身的持久化操作中。</p><p>为了避免这种情况，我们的协议是建立在一个 “消息块” 的抽象基础上，合理将消息分组。 这使得网络请求将多个消息打包成一组，而不是每次发送一条消息，从而使整组消息分担网络中往返的开销。Consumer 每次获取多个大型有序的消息块，并由服务端 依次将消息块一次加载到它的日志中。</p><p>这个简单的优化对速度有着数量级的提升。批处理允许更大的网络数据包，更大的顺序读写磁盘操作，连续的内存块等等，所有这些都使 KafKa 将随机流消息顺序写入到磁盘， 再由 consumers 进行消费。</p><h3 id="zero-copy"><a href="#zero-copy" class="headerlink" title="zero copy"></a>zero copy</h3><p>另一个低效率的操作是字节拷贝，在消息量少时，这不是什么问题。但是在高负载的情况下，影响就不容忽视。为了避免这种情况，我们使用 producer ，broker 和 consumer 都共享的标准化的二进制消息格式，这样数据块不用修改就能在他们之间传递。</p><p>broker 维护的消息日志本身就是一个文件目录，每个文件都由一系列以相同格式写入到磁盘的消息集合组成，这种写入格式被 producer 和 consumer 共用。保持这种通用格式可以对一些很重要的操作进行优化: 持久化日志块的网络传输。 现代的unix 操作系统提供了一个高度优化的编码方式，用于将数据从 pagecache 转移到 socket 网络连接中；在 Linux 中系统调用<a href="http://man7.org/linux/man-pages/man2/sendfile.2.html" target="_blank" rel="noopener"> sendfile </a>做到这一点。</p><p>为了理解 sendfile 的意义，了解数据从文件到套接字的常见数据传输路径就非常重要：</p><ol><li>操作系统从磁盘读取数据到内核空间的 pagecache</li><li>应用程序读取内核空间的数据到用户空间的缓冲区</li><li>应用程序将数据(用户空间的缓冲区)写回内核空间到套接字缓冲区(内核空间)</li><li>操作系统将数据从套接字缓冲区(内核空间)复制到通过网络发送的 NIC 缓冲区</li></ol><p>这显然是低效的，有四次 copy 操作和两次系统调用。使用 sendfile 方法，可以允许操作系统将数据从 pagecache 直接发送到网络，这样避免重新复制数据。所以这种优化方式，只需要最后一步的copy操作，将数据复制到 NIC 缓冲区。</p><p>我们期望一个普遍的应用场景，一个 topic 被多消费者消费。使用上面提交的 zero-copy（零拷贝）优化，数据在使用时只会被复制到 pagecache 中一次，节省了每次拷贝到用户空间内存中，再从用户空间进行读取的消耗。这使得消息能够以接近网络连接速度的 上限进行消费。</p><p>pagecache 和 sendfile 的组合使用意味着，在一个kafka集群中，大多数 consumer 消费时，您将看不到磁盘上的读取活动，因为数据将完全由缓存提供。</p><p>JAVA 中更多有关 sendfile 方法和 zero-copy （零拷贝）相关的资料，可以参考这里的 <a href="http://www.ibm.com/developerworks/linux/library/j-zerocopy" target="_blank" rel="noopener">文章</a>。</p><h3 id="端到端的批量压缩"><a href="#端到端的批量压缩" class="headerlink" title="端到端的批量压缩"></a>端到端的批量压缩</h3><p>在某些情况下，数据传输的瓶颈不是 CPU ，也不是磁盘，而是网络带宽。对于需要通过广域网在数据中心之间发送消息的数据管道尤其如此。当然，用户可以在不需要 Kakfa 支持下一次一个的压缩消息。但是这样会造成非常差的压缩比和消息重复类型的冗余，比如 JSON 中的字段名称或者是或 Web 日志中的用户代理或公共字符串值。高性能的压缩是一次压缩多个消息，而不是压缩单个消息。</p><p>Kafka 以高效的批处理格式支持一批消息可以压缩在一起发送到服务器。这批消息将以压缩格式写入，并且在日志中保持压缩，只会在 consumer 消费时解压缩。</p><p>Kafka 支持 GZIP，Snappy 和 LZ4 压缩协议，更多有关压缩的资料参看 <a href="https://cwiki.apache.org/confluence/display/KAFKA/Compression" target="_blank" rel="noopener">这里</a>。</p><h3 id="总结：Kafka如何实现高性能IO"><a href="#总结：Kafka如何实现高性能IO" class="headerlink" title="总结：Kafka如何实现高性能IO"></a>总结：Kafka如何实现高性能IO</h3><p>使用批量处理的方式来提升系统吞吐能力</p><p>基于磁盘文件高性能顺序读写的特性来设计的存储结构</p><p>利用操作系统的 PageCache 来缓存数据，减少 IO 并提升读性能</p><p>ZeroCopy：使用零拷贝技术加速消费流程</p><h2 id="生产者（Producers）"><a href="#生产者（Producers）" class="headerlink" title="生产者（Producers）"></a>生产者（Producers）</h2><h3 id="Load-balancing"><a href="#Load-balancing" class="headerlink" title="Load balancing"></a>Load balancing</h3><p>生产者直接发送数据到主分区的服务器上，不需要经过任何中间路由。 为了让生产者实现这个功能，所有的 kafka 服务器节点都能响应这样的元数据请求： 哪些服务器是活着的，主题的哪些分区是主分区，分配在哪个服务器上，这样生产者就能适当地直接发送它的请求到服务器上。</p><p>客户端控制消息发送数据到哪个分区，这个可以实现随机的负载均衡方式,或者使用一些特定语义的分区函数。 我们有提供特定分区的接口让用于根据指定的键值进行hash分区(当然也有选项可以重写分区函数)，例如，如果使用用户ID作为key，则用户相关的所有数据都会被分发到同一个分区上。 这允许消费者在消费数据时做一些特定的本地化处理。这样的分区风格经常被设计用于一些本地处理比较敏感的消费者。</p><h3 id="Asynchronous-send"><a href="#Asynchronous-send" class="headerlink" title="Asynchronous send"></a>Asynchronous send</h3><p>批处理是提升性能的一个主要驱动，为了允许批量处理，kafka 生产者会尝试在内存中汇总数据，并用一次请求批次提交信息。 批处理，不仅仅可以配置指定的消息数量，也可以指定等待特定的延迟时间(如64k 或10ms)，这允许汇总更多的数据后再发送，在服务器端也会减少更多的IO操作。 该缓冲是可配置的，并给出了一个机制，通过权衡少量额外的延迟时间获取更好的吞吐量。</p><p>更多的细节可以在 producer 的 <a href="http://kafka.apachecn.org/documentation.html#producerconfigs" target="_blank" rel="noopener">configuration</a> 和 <a href="http://kafka.apache.org/082/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html" target="_blank" rel="noopener">api</a>文档中进行详细的了解。</p><h2 id="消费者（Consumers）"><a href="#消费者（Consumers）" class="headerlink" title="消费者（Consumers）"></a>消费者（Consumers）</h2><p>Kafka consumer通过向 broker 发出一个“fetch”请求来获取它想要消费的 partition。consumer 的每个请求都在 log 中指定了对应的 offset，并接收从该位置开始的一大块数据。因此，consumer 对于该位置的控制就显得极为重要，并且可以在需要的时候通过回退到该位置再次消费对应的数据。</p><h3 id="Push-vs-pull"><a href="#Push-vs-pull" class="headerlink" title="Push vs. pull"></a>Push vs. pull</h3><p>最初我们考虑的问题是：究竟是由 consumer 从 broker 那里 pull 数据，还是由 broker 将数据 push 到 consumer。Kafka 在这方面采取了一种较为传统的设计方式，也是大多数的消息系统所共享的方式：即 producer 把数据 push 到 broker，然后 consumer 从 broker 中 pull 数据。 也有一些 logging-centric 的系统，比如 <a href="http://github.com/facebook/scribe" target="_blank" rel="noopener">Scribe</a> 和 <a href="http://flume.apache.org/" target="_blank" rel="noopener">Apache Flume</a>，沿着一条完全不同的 push-based 的路径，将数据 push 到下游节点。这两种方法都有优缺点。然而，由于 broker 控制着数据传输速率， 所以 push-based 系统很难处理不同的 consumer。让 broker 控制数据传输速率主要是为了让 consumer 能够以可能的最大速率消费；不幸的是，这导致着在 push-based 的系统中，当消费速率低于生产速率时，consumer 往往会不堪重负（本质上类似于拒绝服务攻击）。pull-based 系统有一个很好的特性， 那就是当 consumer 速率落后于 producer 时，可以在适当的时间赶上来。还可以通过使用某种 backoff 协议来减少这种现象：即 consumer 可以通过 backoff 表示它已经不堪重负了，然而通过获得负载情况来充分使用 consumer（但永远不超载）这一方式实现起来比它看起来更棘手。前面以这种方式构建系统的尝试，引导着 Kafka 走向了更传统的 pull 模型。</p><p>另一个 pull-based 系统的优点在于：它可以大批量生产要发送给 consumer 的数据。而 push-based 系统必须选择立即发送请求或者积累更多的数据，然后在不知道下游的 consumer 能否立即处理它的情况下发送这些数据。如果系统调整为低延迟状态，这就会导致一次只发送一条消息，以至于传输的数据不再被缓冲，这种方式是极度浪费的。 而 pull-based 的设计修复了该问题，因为 consumer 总是将所有可用的（或者达到配置的最大长度）消息 pull 到 log 当前位置的后面，从而使得数据能够得到最佳的处理而不会引入不必要的延迟。</p><p>简单的 pull-based 系统的不足之处在于：如果 broker 中没有数据，consumer 可能会在一个紧密的循环中结束轮询，实际上 busy-waiting 直到数据到来。为了避免 busy-waiting，我们在 pull 请求中加入参数，使得 consumer 在一个“long pull”中阻塞等待，直到数据到来（还可以选择等待给定字节长度的数据来确保传输长度）。</p><p>你可以想象其它可能的只基于 pull 的， end-to-end 的设计。例如producer 直接将数据写入一个本地的 log，然后 broker 从 producer 那里 pull 数据，最后 consumer 从 broker 中 pull 数据。通常提到的还有“store-and-forward”式 producer， 这是一种很有趣的设计，但我们觉得它跟我们设定的有数以千计的生产者的应用场景不太相符。我们在运行大规模持久化数据系统方面的经验使我们感觉到，横跨多个应用、涉及数千磁盘的系统事实上并不会让事情更可靠，反而会成为操作时的噩梦。在实践中， 我们发现可以通过大规模运行的带有强大的 SLAs 的 pipeline，而省略 producer 的持久化过程。</p><h3 id="消费者的位置"><a href="#消费者的位置" class="headerlink" title="消费者的位置"></a>消费者的位置</h3><p>令人惊讶的是，持续追踪<em>已经被消费的内容</em>是消息系统的关键性能点之一。</p><p>大多数消息系统都在 broker 上保存被消费消息的元数据。也就是说，当消息被传递给 consumer，broker 要么立即在本地记录该事件，要么等待 consumer 的确认后再记录。这是一种相当直接的选择，而且事实上对于单机服务器来说，也没与其它地方能够存储这些状态信息。 由于大多数消息系统用于存储的数据结构规模都很小，所以这也是一个很实用的选择——因为只要 broker 知道哪些消息被消费了，就可以在本地立即进行删除，一直保持较小的数据量。</p><p>也许不太明显，但要让 broker 和 consumer 就被消费的数据保持一致性也不是一个小问题。如果 broker 在每条消息被发送到网络的时候，立即将其标记为 <strong>consumed</strong>，那么一旦 consumer 无法处理该消息（可能由 consumer 崩溃或者请求超时或者其他原因导致），该消息就会丢失。 为了解决消息丢失的问题，许多消息系统增加了确认机制：即当消息被发送出去的时候，消息仅被标记为<strong>sent</strong> 而不是 <strong>consumed</strong>；然后 broker 会等待一个来自 consumer 的特定确认，再将消息标记为<strong>consumed</strong>。这个策略修复了消息丢失的问题，但也产生了新问题。 首先，如果 consumer 处理了消息但在发送确认之前出错了，那么该消息就会被消费两次。第二个是关于性能的，现在 broker 必须为每条消息保存多个状态（首先对其加锁，确保该消息只被发送一次，然后将其永久的标记为 consumed，以便将其移除）。 还有更棘手的问题要处理，比如如何处理已经发送但一直得不到确认的消息。</p><p>Kafka 使用完全不同的方式解决消息丢失问题。Kafka的 topic 被分割成了一组完全有序的 partition，其中每一个 partition 在任意给定的时间内只能被每个订阅了这个 topic 的 consumer 组中的一个 consumer 消费。这意味着 partition 中 每一个 consumer 的位置仅仅是一个数字，即下一条要消费的消息的offset。这使得被消费的消息的状态信息相当少，每个 partition 只需要一个数字。这个状态信息还可以作为周期性的 checkpoint。这以非常低的代价实现了和消息确认机制等同的效果。</p><p>这种方式还有一个附加的好处。consumer 可以<em>回退</em>到之前的 offset 来再次消费之前的数据，这个操作违反了队列的基本原则，但事实证明对大多数 consumer 来说这是一个必不可少的特性。 例如，如果 consumer 的代码有 bug，并且在 bug 被发现前已经有一部分数据被消费了， 那么 consumer 可以在 bug 修复后通过回退到之前的 offset 来再次消费这些数据。</p><h3 id="离线数据加载"><a href="#离线数据加载" class="headerlink" title="离线数据加载"></a>离线数据加载</h3><p>可伸缩的持久化特性允许 consumer 只进行周期性的消费，例如批量数据加载，周期性将数据加载到诸如 Hadoop 和关系型数据库之类的离线系统中。</p><p>在 Hadoop 的应用场景中，我们通过将数据加载分配到多个独立的 map 任务来实现并行化，每一个 map 任务负责一个 node/topic/partition，从而达到充分并行化。Hadoop 提供了任务管理机制，失败的任务可以重新启动而不会有重复数据的风险，只需要简单的从原来的位置重启即可。</p><h2 id="消息交付语义"><a href="#消息交付语义" class="headerlink" title="消息交付语义"></a>消息交付语义</h2><p>现在我们对于 producer 和 consumer 的工作原理已将有了一点了解，让我们接着讨论 Kafka 在 producer 和 consumer 之间提供的语义保证。显然， 可以提供多种可能的消息传递保证：：</p><ul><li><em>At most once</em>——消息可能会丢失但绝不重传。</li><li><em>At least once</em>——消息可以重传但绝不丢失。</li><li><em>Exactly once</em>——这正是人们想要的, 每一条消息只被传递一次.</li></ul><p>值得注意的是，这个问题被分成了两部分：发布消息的持久性保证和消费消息的保证。</p><p>很多系统声称提供了“Exactly once”的消息交付语义, 然而阅读它们的细则很重要, 因为这些声称大多数都是误导性的 (即它们没有考虑 consumer 或 producer 可能失败的情况，以及存在多个 consumer 进行处理的情况，或者写入磁盘的数据可能丢失的情况。).</p><p>Kafka 的语义是直截了当的。 发布消息时，我们会有一个消息的概念被“committed”到 log 中。 一旦消息被提交，只要有一个 broker 备份了该消息写入的 partition，并且保持“alive”状态，该消息就不会丢失。 有关 committed message 和 alive partition 的定义，以及我们试图解决的故障类型都将在下一节进行细致描述。 现在让我们假设存在完美无缺的 broker，然后来试着理解 Kafka 对 producer 和 consumer 的语义保证。如果一个 producer 在试图发送消息的时候发生了网络故障， 则不确定网络错误发生在消息提交之前还是之后。这与使用自动生成的键插入到数据库表中的语义场景很相似。</p><p>在 0.11.0.0 之前的版本中, 如果 producer 没有收到表明消息已经被提交的响应, 那么 producer 除了将消息重传之外别无选择。 这里提供的是 at-least-once 的消息交付语义，因为如果最初的请求事实上执行成功了，那么重传过程中该消息就会被再次写入到 log 当中。 从 0.11.0.0 版本开始，Kafka producer新增了幂等性的传递选项，该选项保证重传不会在 log 中产生重复条目。 为实现这个目的, broker 给每个 producer 都分配了一个 ID ，并且 producer 给每条被发送的消息分配了一个序列号来避免产生重复的消息。 同样也是从 0.11.0.0 版本开始, producer 新增了使用类似事务性的语义将消息发送到多个 topic partition 的功能： 也就是说，要么所有的消息都被成功的写入到了 log，要么一个都没写进去。这种语义的主要应用场景就是 Kafka topic 之间的 exactly-once 的数据传递(如下所述)。</p><p>并非所有使用场景都需要这么强的保证。对于延迟敏感的应用场景，我们允许生产者指定它需要的持久性级别。如果 producer 指定了它想要等待消息被提交，则可以使用10ms的量级。然而， producer 也可以指定它想要完全异步地执行发送，或者它只想等待直到 leader 节点拥有该消息（follower 节点有没有无所谓）。</p><p>现在让我们从 consumer 的视角来描述语义。 所有的副本都有相同的 log 和相同的 offset。consumer 负责控制它在 log 中的位置。如果 consumer 永远不崩溃，那么它可以将这个位置信息只存储在内存中。但如果 consumer 发生了故障，我们希望这个 topic partition 被另一个进程接管， 那么新进程需要选择一个合适的位置开始进行处理。假设 consumer 要读取一些消息——它有几个处理消息和更新位置的选项。</p><ol><li>Consumer 可以先读取消息，然后将它的位置保存到 log 中，最后再对消息进行处理。在这种情况下，消费者进程可能会在保存其位置之后，但还没有保存消息处理的输出之前发生崩溃。而在这种情况下，即使在此位置之前的一些消息没有被处理，接管处理的进程将从保存的位置开始。在 consumer 发生故障的情况下，这对应于“at-most-once”的语义，可能会有消息得不到处理。</li><li>Consumer 可以先读取消息，然后处理消息，最后再保存它的位置。在这种情况下，消费者进程可能会在处理了消息之后，但还没有保存位置之前发生崩溃。而在这种情况下，当新的进程接管后，它最初收到的一部分消息都已经被处理过了。在 consumer 发生故障的情况下，这对应于“at-least-once”的语义。 在许多应用场景中，消息都设有一个主键，所以更新操作是幂等的（相同的消息接收两次时，第二次写入会覆盖掉第一次写入的记录）。</li></ol><p>那么 exactly once 语义（即你真正想要的东西）呢？当从一个 kafka topic 中消费并输出到另一个 topic 时 (正如在一个<a href="https://kafka.apache.org/documentation/streams" target="_blank" rel="noopener">Kafka Streams</a> 应用中所做的那样)，我们可以使用我们上文提到的 0.11.0.0 版本中的新事务型 producer，并将 consumer 的位置存储为一个 topic 中的消息，所以我们可以在输出 topic 接收已经被处理的数据的时候，在同一个事务中向 Kafka 写入 offset。如果事务被中断，则消费者的位置将恢复到原来的值，而输出 topic 上产生的数据对其他消费者是否可见，取决于事务的“隔离级别”。 在默认的“read_uncommitted”隔离级别中，所有消息对 consumer 都是可见的，即使它们是中止的事务的一部分，但是在“read_committed”的隔离级别中，消费者只能访问已提交的事务中的消息（以及任何不属于事务的消息）。</p><p>在写入外部系统的应用场景中，限制在于需要在 consumer 的 offset 与实际存储为输出的内容间进行协调。解决这一问题的经典方法是在 consumer offset 的存储和 consumer 的输出结果的存储之间引入 two-phase commit。但这可以用更简单的方法处理，而且通常的做法是让 consumer 将其 offset 存储在与其输出相同的位置。 这也是一种更好的方式，因为大多数 consumer 想写入的输出系统都不支持 two-phase commit。举个例子，<a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener">Kafka Connect</a>连接器，它将所读取的数据和数据的 offset 一起写入到 HDFS，以保证数据和 offset 都被更新，或者两者都不被更新。 对于其它很多需要这些较强语义，并且没有主键来避免消息重复的数据系统，我们也遵循类似的模式。</p><p>因此，事实上 Kafka 在<a href="https://kafka.apache.org/documentation/streams" target="_blank" rel="noopener">Kafka Streams</a>中支持了exactly-once 的消息交付功能，并且在 topic 之间进行数据传递和处理时，通常使用事务型 producer/consumer 提供 exactly-once 的消息交付功能。 到其它目标系统的 exactly-once 的消息交付通常需要与该类系统协作，但 Kafka 提供了 offset，使得这种应用场景的实现变得可行。(详见 <a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener">Kafka Connect</a>)。否则，Kafka 默认保证 at-least-once 的消息交付， 并且 Kafka 允许用户通过禁用 producer 的重传功能和让 consumer 在处理一批消息之前提交 offset，来实现 at-most-once 的消息交付。</p><h2 id="复制（-Replication）"><a href="#复制（-Replication）" class="headerlink" title="复制（ Replication）"></a>复制（ Replication）</h2><p>Kafka 允许 topic 的 partition 拥有若干副本，你可以在server端配置partition 的副本数量。当集群中的节点出现故障时，能自动进行故障转移，保证数据的可用性。</p><p>其他的消息系统也提供了副本相关的特性，但是在我们（带有偏见）看来，他们的副本功能不常用，而且有很大缺点：slaves 处于非活动状态，导致吞吐量受到严重影响，并且还要手动配置副本机制。Kafka 默认使用备份机制，事实上，我们将没有设置副本数 的 topic 实现为副本数为1的 topic 。</p><p>创建副本的单位是 topic 的 partition ，正常情况下， 每个分区都有一个 leader 和零或多个 followers 。 总的副本数是包含 leader 的总和。 所有的读写操作都由 leader 处理，一般 partition 的数量都比 broker 的数量多的多，各分区的 leader 均匀的分布在brokers 中。所有的 followers 节点都同步 leader 节点的日志，日志中的消息和偏移量都和 leader 中的一致。（当然, 在任何给定时间, leader 节点的日志末尾时可能有几个消息尚未被备份完成）。</p><p>Followers 节点就像普通的 consumer 那样从 leader 节点那里拉取消息并保存在自己的日志文件中。Followers 节点可以从 leader 节点那里批量拉取消息日志到自己的日志文件中。</p><p>与大多数分布式系统一样，自动处理故障需要精确定义节点 “alive” 的概念。Kafka 判断节点是否存活有两种方式。</p><ol><li>节点必须可以维护和 ZooKeeper 的连接，Zookeeper 通过心跳机制检查每个节点的连接。</li><li>如果节点是个 follower ，它必须能及时的同步 leader 的写操作，并且延时不能太久。</li></ol><p>我们认为满足这两个条件的节点处于 “in sync” 状态，区别于 “alive” 和 “failed” 。 Leader会追踪所有 “in sync” 的节点。如果有节点挂掉了, 或是写超时, 或是心跳超时, leader 就会把它从同步副本列表中移除。 同步超时和写超时的时间由 replica.lag.time.max.ms 配置确定。</p><p>分布式系统中，我们只尝试处理 “fail/recover” 模式的故障，即节点突然停止工作，然后又恢复（节点可能不知道自己曾经挂掉）的状况。Kafka 没有处理所谓的 “Byzantine” 故障，即一个节点出现了随意响应和恶意响应（可能由于 bug 或 非法操作导致）。</p><p>现在, 我们可以更精确地定义, 只有当消息被所有的副本节点加入到日志中时, 才算是提交, 只有提交的消息才会被 consumer 消费, 这样就不用担心一旦 leader 挂掉了消息会丢失。另一方面， producer 也可以选择是否等待消息被提交，这取决他们的设置在延迟时间和持久性之间的权衡，这个选项是由 producer 使用的 acks 设置控制。 请注意，Topic 可以设置同步备份的最小数量， producer 请求确认消息是否被写入到所有的备份时, 可以用最小同步数量判断。如果 producer 对同步的备份数没有严格的要求，即使同步的备份数量低于 最小同步数量（例如，仅仅只有 leader 同步了数据），消息也会被提交，然后被消费。</p><p>在所有时间里，Kafka 保证只要有至少一个同步中的节点存活，提交的消息就不会丢失。</p><p>节点挂掉后，经过短暂的故障转移后，Kafka将仍然保持可用性，但在网络分区（ network partitions ）的情况下可能不能保持可用性。</p><h3 id="备份日志：Quorums-ISRs-和状态机"><a href="#备份日志：Quorums-ISRs-和状态机" class="headerlink" title="备份日志：Quorums, ISRs, 和状态机"></a>备份日志：Quorums, ISRs, 和状态机</h3><p>Kafka的核心是备份日志文件。备份日志文件是分布式数据系统最基础的要素之一，实现方法也有很多种。其他系统也可以用 kafka 的备份日志模块来实现<a href="http://en.wikipedia.org/wiki/State_machine_replication" target="_blank" rel="noopener">状态机风格</a>的分布式系统</p><p>备份日志按照一系列有序的值(通常是编号为0、1、2、…)进行建模。有很多方法可以实现这一点，但最简单和最快的方法是由 leader 节点选择需要提供的有序的值，只要 leader 节点还存活，所有的 follower 只需要拷贝数据并按照 leader 节点的顺序排序。</p><p>当然，如果 leader 永远都不会挂掉，那我们就不需要 follower 了。 但是如果 leader crash，我们就需要从 follower 中选举出一个新的 leader。 但是 followers 自身也有可能落后或者 crash，所以 我们必须确保我们leader的候选者们 是一个数据同步 最新的 follower 节点。</p><p>如果选择写入时候需要保证一定数量的副本写入成功，读取时需要保证读取一定数量的副本，读取和写入之间有重叠。这样的读写机制称为 Quorum。</p><p>这种权衡的一种常见方法是对提交决策和 leader 选举使用多数投票机制。Kafka 没有采取这种方式，但是我们还是要研究一下这种投票机制，来理解其中蕴含的权衡。假设我们有2<em>f</em> + 1个副本，如果在 leader 宣布消息提交之前必须有<em>f</em>+1个副本收到 该消息，并且如果我们从这至少<em>f</em>+1个副本之中，有着最完整的日志记录的 follower 里来选择一个新的 leader，那么在故障次数少于<em>f</em>的情况下，选举出的 leader 保证具有所有提交的消息。这是因为在任意<em>f</em>+1个副本中，至少有一个副本一定包含 了所有提交的消息。该副本的日志将是最完整的，因此将被选为新的 leader。这个算法都必须处理许多其他细节（例如精确定义怎样使日志更加完整，确保在 leader down 掉期间, 保证日志一致性或者副本服务器的副本集的改变），但是现在我们将忽略这些细节。</p><p>这种大多数投票方法有一个非常好的优点：延迟是取决于最快的服务器。也就是说，如果副本数是3，则备份完成的等待时间取决于最快的 Follwer 。</p><p>这里有很多分布式算法，包含 ZooKeeper 的 <a href="http://web.archive.org/web/20140602093727/http://www.stanford.edu/class/cs347/reading/zab.pdf" target="_blank" rel="noopener">Zab</a>, <a href="https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf" target="_blank" rel="noopener">Raft</a>, 和 <a href="http://pmg.csail.mit.edu/papers/vr-revisited.pdf" target="_blank" rel="noopener">Viewstamped Replication</a>. 我们所知道的与 Kafka 实际执行情况最相似的学术刊物是来自微软的 <a href="http://research.microsoft.com/apps/pubs/default.aspx?id=66814" target="_blank" rel="noopener">PacificA</a></p><p>大多数投票的缺点是，多数的节点挂掉让你不能选择 leader。要冗余单点故障需要三份数据，并且要冗余两个故障需要五份的数据。根据我们的经验，在一个系统中，仅仅靠冗余来避免单点故障是不够的，但是每写5次，对磁盘空间需求是5倍， 吞吐量下降到 1/5，这对于处理海量数据问题是不切实际的。这可能是为什么 quorum 算法更常用于共享集群配置（如 ZooKeeper ）， 而不适用于原始数据存储的原因，例如 HDFS 中 namenode 的高可用是建立在 <a href="http://blog.cloudera.com/blog/2012/10/quorum-based-journaling-in-cdh4-1" target="_blank" rel="noopener">基于投票的元数据</a> ，这种代价高昂的存储方式不适用数据本身。</p><p>Kafka 采取了一种稍微不同的方法来选择它的投票集。 Kafka 不是用大多数投票选择 leader 。Kafka 动态维护了一个同步状态的备份的集合 （a set of in-sync replicas）， 简称 ISR ，在这个集合中的节点都是和 leader 保持高度一致的，只有这个集合的成员才 有资格被选举为 leader，一条消息必须被这个集合 <em>所有</em> 节点读取并追加到日志中了，这条消息才能视为提交。这个 ISR 集合发生变化会在 ZooKeeper 持久化，正因为如此，这个集合中的任何一个节点都有资格被选为 leader 。这对于 Kafka 使用模型中， 有很多分区和并确保主从关系是很重要的。因为 ISR 模型和 <em>f+1</em> 副本，一个 Kafka topic 冗余 <em>f</em> 个节点故障而不会丢失任何已经提交的消息。</p><p>我们认为对于希望处理的大多数场景这种策略是合理的。在实际中，为了冗余 <em>f</em> 节点故障，大多数投票和 ISR 都会在提交消息前确认相同数量的备份被收到（例如在一次故障生存之后，大多数的 quorum 需要三个备份节点和一次确认，ISR 只需要两个备份节点和一次确认），多数投票方法的一个优点是提交时能避免最慢的服务器。但是，我们认为通过允许客户端选择是否阻塞消息提交来改善，和所需的备份数较低而产生的额外的吞吐量和磁盘空间是值得的。</p><p>另一个重要的设计区别是，Kafka 不要求崩溃的节点恢复所有的数据，在这种空间中的复制算法经常依赖于存在 “稳定存储”，在没有违反潜在的一致性的情况下，出现任何故障再恢复情况下都不会丢失。 这个假设有两个主要的问题。首先，我们在持久性数据系统的实际操作中观察到的最常见的问题是磁盘错误，并且它们通常不能保证数据的完整性。其次，即使磁盘错误不是问题，我们也不希望在每次写入时都要求使用 fsync 来保证一致性， 因为这会使性能降低两到三个数量级。我们的协议能确保备份节点重新加入ISR 之前，即使它挂时没有新的数据, 它也必须完整再一次同步数据。</p><h3 id="Unclean-leader-选举-如果节点全挂了？"><a href="#Unclean-leader-选举-如果节点全挂了？" class="headerlink" title="Unclean leader 选举: 如果节点全挂了？"></a>Unclean leader 选举: 如果节点全挂了？</h3><p>请注意，Kafka 对于数据不会丢失的保证，是基于至少一个节点在保持同步状态，一旦分区上的所有备份节点都挂了，就无法保证了。</p><p>但是，实际在运行的系统需要去考虑假设一旦所有的备份都挂了，怎么去保证数据不会丢失，这里有两种实现的方法</p><ol><li>等待一个 ISR 的副本重新恢复正常服务，并选择这个副本作为领 leader （它有极大可能拥有全部数据）。</li><li>选择第一个重新恢复正常服务的副本（不一定是 ISR 中的）作为leader。</li></ol><p>这是可用性和一致性之间的简单妥协，如果我只等待 ISR 的备份节点，那么只要 ISR 备份节点都挂了，我们的服务将一直会不可用，如果它们的数据损坏了或者丢失了，那就会是长久的宕机。另一方面，如果不是 ISR 中的节点恢复服务并且我们允许它成为 leader ， 那么它的数据就是可信的来源，即使它不能保证记录了每一个已经提交的消息。 kafka 默认选择第二种策略，当所有的 ISR 副本都挂掉时，会选择一个可能不同步的备份作为 leader ，可以配置属性 unclean.leader.election.enable 禁用此策略，那么就会使用第 一种策略即停机时间优于不同步。</p><p>这种困境不只有 Kafka 遇到，它存在于任何 quorum-based 规则中。例如，在大多数投票算法当中，如果大多数服务器永久性的挂了，那么您要么选择丢失100%的数据，要么违背数据的一致性选择一个存活的服务器作为数据可信的来源。</p><h3 id="可用性和持久性保证"><a href="#可用性和持久性保证" class="headerlink" title="可用性和持久性保证"></a>可用性和持久性保证</h3><p>向 Kafka 写数据时，producers 设置 ack 是否提交完成， 0：不等待broker返回确认消息,1: leader保存成功返回或, -1(all): 所有备份都保存成功返回.请注意. 设置 “ack = all” 并不能保证所有的副本都写入了消息。默认情况下，当 acks = all 时，只要 ISR 副本同步完成，就会返回消息已经写入。例如，一个 topic 仅仅设置了两个副本，那么只有一个 ISR 副本，那么当设置acks = all时返回写入成功时，剩下了的那个副本数据也可能数据没有写入。 尽管这确保了分区的最大可用性，但是对于偏好数据持久性而不是可用性的一些用户，可能不想用这种策略，因此，我们提供了两个topic 配置，可用于优先配置消息数据持久性：</p><ol><li>禁用 unclean leader 选举机制 - 如果所有的备份节点都挂了,分区数据就会不可用，直到最近的 leader 恢复正常。这种策略优先于数据丢失的风险， 参看上一节的 unclean leader 选举机制。</li><li>指定最小的 ISR 集合大小，只有当 ISR 的大小大于最小值，分区才能接受写入操作，以防止仅写入单个备份的消息丢失造成消息不可用的情况，这个设置只有在生产者使用 acks = all 的情况下才会生效，这至少保证消息被 ISR 副本写入。此设置是一致性和可用性 之间的折衷，对于设置更大的最小ISR大小保证了更好的一致性，因为它保证将消息被写入了更多的备份，减少了消息丢失的可能性。但是，这会降低可用性，因为如果 ISR 副本的数量低于最小阈值，那么分区将无法写入。</li></ol><h3 id="备份管理"><a href="#备份管理" class="headerlink" title="备份管理"></a>备份管理</h3><p>以上关于备份日志的讨论只涉及单个日志文件，即一个 topic 分区，事实上，一个Kafka集群管理着成百上千个这样的 partitions。我们尝试以轮询调度的方式将集群内的 partition 负载均衡，避免大量topic拥有的分区集中在 少数几个节点上。同样，我们也试图平衡leadership,以至于每个节点都是部分 partition 的leader节点。</p><p>优化主从关系的选举过程也是重要的，这是数据不可用的关键窗口。原始的实现是当有节点挂了后，进行主从关系选举时，会对挂掉节点的所有partition 的领导权重新选举。相反，我们会选择一个 broker 作为 “controller”节点。controller 节点负责 检测 brokers 级别故障,并负责在 broker 故障的情况下更改这个故障 Broker 中的 partition 的 leadership 。这种方式可以批量的通知主从关系的变化，使得对于拥有大量partition 的broker ,选举过程的代价更低并且速度更快。如果 controller 节点挂了，其他 存活的 broker 都可能成为新的 controller 节点。</p><h2 id="日志整理"><a href="#日志整理" class="headerlink" title="日志整理"></a>日志整理</h2><p>日志整理可确保 Kafka 始终至少为单个 topic partition 的数据日志中的每个 message key 保留最新的已知值。 这样的设计解决了应用程序崩溃、系统故障后恢复或者应用在运行维护过程中重启后重新加载缓存的场景。 接下来让我们深入讨论这些在使用过程中的更多细节，阐述在这个过程中它是如何进行日志压缩的。</p><p>迄今为止，我们只介绍了简单的日志保留方法（当旧的数据保留时间超过指定时间、日志大达到规定大小后就丢弃）。 这样的策略非常适用于处理那些暂存的数据，例如记录每条消息之间相互独立的日志。 然而在实际使用过程中还有一种非常重要的场景——根据key进行数据变更（例如更改数据库表内容），使用以上的方式显然不行。</p><p>让我们来讨论一个关于处理这样的流式数据的具体的例子。 假设我们有一个topic，里面的内容包含用户的email地址；每次用户更新他们的email地址时，我们发送一条消息到这个topic，这里使用用户Id作为消息的key值。 现在，我们在一段时间内为id为123的用户发送一些消息，每个消息对应email地址的改变（其他ID消息省略）:</p><p><code>123 =&gt; bill@microsoft.com</code></p><p><code>...</code></p><p><code>123 =&gt; bill@gatesfoundation.org</code></p><p><code>...</code></p><p><code>123 =&gt; bill@gmail.com</code></p><p>日志压缩为我提供了更精细的保留机制，所以我们至少保留每个key的最后一次更新 （例如：<a href="mailto:bill@gmail.com" target="_blank" rel="noopener">bill@gmail.com</a>）。 这样我们保证日志包含每一个key的最终值而不只是最近变更的完整快照。这意味着下游的消费者可以获得最终的状态而无需拿到所有的变化的消息信息。</p><p>让我们先看几个有用的使用场景，然后再看看如何使用它。</p><ol><li><em>数据库更改订阅</em>。 通常需要在多个数据系统设置拥有一个数据集，这些系统中通常有一个是某种类型的数据库（无论是RDBMS或者新流行的key-value数据库）。 例如，你可能有一个数据库，缓存，搜索引擎集群或者Hadoop集群。每次变更数据库，也同时需要变更缓存、搜索引擎以及hadoop集群。 在只需处理最新日志的实时更新的情况下，你只需要最近的日志。但是，如果你希望能够重新加载缓存或恢复搜索失败的节点，你可能需要一个完整的数据集。</li><li><em>事件源</em>。 这是一种应用程序设计风格，它将查询处理与应用程序设计相结合，并使用变更的日志作为应用程序的主要存储。</li><li><em>日志高可用</em>。 执行本地计算的进程可以通过注销对其本地状态所做的更改来实现容错，以便另一个进程可以重新加载这些更改并在出现故障时继续进行。 一个具体的例子就是在流查询系统中进行计数，聚合和其他类似“group by”的操作。实时流处理框架Samza， <a href="http://samza.apache.org/learn/documentation/0.7.0/container/state-management.html" target="_blank" rel="noopener">使用这个特性</a>正是出于这一原因。</li></ol><p>在这些场景中，主要需要处理变化的实时feed，但是偶尔当机器崩溃或需要重新加载或重新处理数据时，需要处理所有数据。 日志压缩允许在同一topic下同时使用这两个用例。这种日志使用方式更详细的描述请看这篇<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying" target="_blank" rel="noopener">博客</a>。</p><p>想法很简单，我们有无限的日志，以上每种情况记录变更日志，我们从一开始就捕获每一次变更。 使用这个完整的日志，我们可以通过回放日志来恢复到任何一个时间点的状态。 然而这种假设的情况下，完整的日志是不实际的，对于那些每一行记录会变更多次的系统，即使数据集很小，日志也会无限的增长下去。 丢弃旧日志的简单操作可以限制空间的增长，但是无法重建状态——因为旧的日志被丢弃，可能一部分记录的状态会无法重建（这些记录所有的状态变更都在旧日志中）。</p><p>日志压缩机制是更细粒度的、每个记录都保留的机制，而不是基于时间的粗粒度。 这个理念是选择性的删除那些有更新的变更的记录的日志。 这样最终日志至少包含每个key的记录的最后一个状态。</p><p>这个策略可以为每个Topic设置，这样一个集群中，可以一部分Topic通过时间和大小保留日志，另外一些可以通过压缩压缩策略保留。</p><p>这个功能的灵感来自于LinkedIn的最古老且最成功的基础设置——一个称为Databus的数据库变更日志缓存系统。 不像大多数的日志存储系统，Kafka是专门为订阅和快速线性的读和写的组织数据。 和Databus不同，Kafka作为真实的存储，压缩日志是非常有用的，这非常有利于上游数据源不能重放的情况。</p><h3 id="日志整理基础"><a href="#日志整理基础" class="headerlink" title="日志整理基础"></a>日志整理基础</h3><p>这是一个高级别的日志逻辑图，展示了kafka日志的每条消息的offset逻辑结构。</p><img src="/posts/9bcd2d2c/001.png"><p>Log head中包含传统的Kafka日志，它包含了连续的offset和所有的消息。 日志压缩增加了处理tail Log的选项。 上图展示了日志压缩的的Log tail的情况。tail中的消息保存了初次写入时的offset。 即使该offset的消息被压缩，所有offset仍然在日志中是有效的。在这个场景中，无法区分和下一个出现的更高offset的位置。 如上面的例子中，36、37、38是属于相同位置的，从他们开始读取日志都将从38开始。</p><p>压缩也允许删除。通过消息的key和空负载（null payload）来标识该消息可从日志中删除。 这个删除标记将会引起所有之前拥有相同key的消息被移除（包括拥有key相同的新消息）。 但是删除标记比较特殊，它将在一定周期后被从日志中删除来释放空间。这个时间点被称为“delete retention point”，如上图。</p><p>压缩操作通过在后台周期性的拷贝日志段来完成。 清除操作不会阻塞读取，并且可以被配置不超过一定IO吞吐来避免影响Producer和Consumer。实际的日志段压缩过程有点像这样：</p><img src="/posts/9bcd2d2c/002.png"><h3 id="日志整理的保障"><a href="#日志整理的保障" class="headerlink" title="日志整理的保障"></a>日志整理的保障</h3><p>日志整理的保障措施如下：</p><ol><li>任何滞留在日志head中的所有消费者能看到写入的所有消息；这些消息都是有序的offset。 topic使用min.compaction.lag.ms来保障消息写入之前必须经过的最小时间长度，才能被压缩。 这限制了一条消息在Log Head中的最短存在时间。</li><li>始终保持消息的有序性。压缩永远不会重新排序消息，只是删除了一些。</li><li>消息的Offset不会变更。这是消息在日志中的永久标志。</li><li>任何从头开始处理日志的Consumer至少会拿到每个key的最终状态。 另外，只要Consumer在小于Topic的delete.retention.ms设置（默认24小时）的时间段内到达Log head，将会看到所有删除记录的所有删除标记。 换句话说，因为移除删除标记和读取是同时发生的，Consumer可能会因为落后超过delete.retention.ms而导致错过删除标记。</li></ol><h3 id="日志整理细节"><a href="#日志整理细节" class="headerlink" title="日志整理细节"></a>日志整理细节</h3><p>日志整理由Log Cleaner执行，后台线程池重新拷贝日志段，移除那些key存在于Log Head中的记录。每个整理线程如下工作：</p><ol><li>选择log head与log tail比率最高的日志。</li><li>在head log中为每个key的最后offset创建一个的简单概要。</li><li>它从日志的开始到结束，删除那些在日志中最新出现的key的旧的值。新的、干净的日志将会立即被交到到日志中，所以只需要一个额外的日志段空间（不是日志的完整副本）</li><li>日志head的概要本质上是一个空间密集型的哈希表，每个条目使用24个字节。所以如果有8G的整理缓冲区， 则能迭代处理大约366G的日志头部(假设消息大小为1k)。</li></ol><h3 id="配置Log-Cleaner"><a href="#配置Log-Cleaner" class="headerlink" title="配置Log Cleaner"></a>配置Log Cleaner</h3><p>Log Cleaner默认启用。这会启动清理的线程池。如果要开始特定Topic的清理功能，可以开启特定的属性：</p><p><code>log.cleanup.policy=compact</code></p><p>这个可以通过创建Topic时配置或者之后使用Topic命令实现。</p><p>Log Cleaner可以配置保留最小的不压缩的head log。可以通过配置压缩的延迟时间：</p><p><code>log.cleaner.min.compaction.lag.ms</code></p><p>这可以保证消息在配置的时长内不被压缩。 如果没有设置，除了最后一个日志外，所有的日志都会被压缩。 活动的 segment 是不会被压缩的，即使它保存的消息的滞留时长已经超过了配置的最小压缩时间长。</p><p>关于cleaner更详细的配置在 <a href="http://kafka.apachecn.org/documentation.html#brokerconfigs" target="_blank" rel="noopener">这里</a>。</p><h2 id="配额（Quotas-）"><a href="#配额（Quotas-）" class="headerlink" title="配额（Quotas ）"></a>配额（Quotas ）</h2><p>Kafka 集群可以对客户端请求进行配额，控制集群资源的使用。Kafka broker 可以对客户端做两种类型资源的配额限制，同一个group的client 共享配额。</p><ol><li>定义字节率的阈值来限定网络带宽的配额。 (从 0.9 版本开始)</li><li>request 请求率的配额，网络和 I/O线程 cpu利用率的百分比。 (从 0.11 版本开始)</li></ol><h3 id="为什么要对资源进行配额"><a href="#为什么要对资源进行配额" class="headerlink" title="为什么要对资源进行配额?"></a>为什么要对资源进行配额?</h3><p>producers 和 consumers 可能会生产或者消费大量的数据或者产生大量的请求，导致对 broker 资源的垄断，引起网络的饱和，对其他clients和brokers本身造成DOS攻击。 资源的配额保护可以有效防止这些问题，在大型多租户集群中，因为一小部分表现不佳的客户端降低了良好的用户体验，这种情况下非常需要资源的配额保护。 实际情况中，当把Kafka当做一种服务提供的时候，可以根据客户端和服务端的契约对 API 调用做限制。</p><h3 id="Client-groups"><a href="#Client-groups" class="headerlink" title="Client groups"></a>Client groups</h3><p>Kafka client 是一个用户的概念， 是在一个安全的集群中经过身份验证的用户。在一个支持非授权客户端的集群中，用户是一组非授权的 users，broker使用一个可配置的PrincipalBuilder 类来配置 group 规则。 Client-id 是客户端的逻辑分组，客户端应用使用一个有意义的名称进行标识。(user, client-id)元组定义了一个安全的客户端逻辑分组，使用相同的user 和 client-id 标识。</p><p>资源配额可以针对 （user,client-id），users 或者client-id groups 三种规则进行配置。对于一个请求连接，连接会匹配最细化的配额规则的限制。同一个 group 的所有连接共享这个 group 的资源配额。 举个例子，如果 (user=”test-user”, client-id=”test-client”) 客户端producer 有10MB/sec 的生产资源配置，这10MB/sec 的资源在所有 “test-user” 用户，client-id是 “test-client” 的producer实例中是共享的。</p><h3 id="配额配置"><a href="#配额配置" class="headerlink" title="配额配置"></a>配额配置</h3><p>资源配额的配置可以根据 (user, client-id)，user 和 client-id 三种规则进行定义。在配额级别需要更高（或者更低）的配额的时候，是可以覆盖默认的配额配置。 这种机制和每个 topic 可以自定义日志配置属性类似。 覆盖 User 和 (user, client-id) 规则的配额配置会写到zookeeper的 <strong><em>/config/users</em></strong>路径下，client-id 配额的配置会写到 <strong><em>/config/clients</em></strong> 路径下。 这些配置的覆盖会被所有的 brokers 实时的监听到并生效。所以这使得我们修改配额配置不需要重启整个集群。更多细节参考 <a href="http://kafka.apachecn.org/documentation.html#quotas" target="_blank" rel="noopener">here</a>。 每个 group 的默认配额可以使用相同的机制进行动态更新。</p><p>配额配置的优先级顺序是:</p><ol><li>/config/users/<user>/clients/<client-id></client-id></user></li><li>/config/users/<user>/clients/<default></default></user></li><li>/config/users/<user></user></li><li>/config/users/<default>/clients/<client-id></client-id></default></li><li>/config/users/<default>/clients/<default></default></default></li><li>/config/users/<default></default></li><li>/config/clients/<client-id></client-id></li><li>/config/clients/<default></default></li></ol><p>Broker 的配置属性 (quota.producer.default, quota.consumer.default) 也可以用来设置 client-id groups 默认的网络带宽配置。这些配置属性在未来的 release 版本会被 deprecated。 client-id 的默认配额也是用zookeeper配置，和其他配额配置的覆盖和默认方式是相似的。</p><h3 id="网络带宽配额"><a href="#网络带宽配额" class="headerlink" title="网络带宽配额"></a>网络带宽配额</h3><p>网络带宽配额使用字节速率阈值来定义每个 group 的客户端的共享配额。 默认情况下，每个不同的客户端 group 是集群配置的固定配额，单位是 bytes/sec。 这个配额会以broker 为基础进行定义。在 clients 被限制之前，每个 group 的clients可以发布和拉取单个broker 的最大速率，单位是 bytes/sec。</p><h3 id="请求速率配额"><a href="#请求速率配额" class="headerlink" title="请求速率配额"></a>请求速率配额</h3><p>请求速率的配额定义了一个客户端可以使用 broker request handler I/O 线程和网络线程在一个配额窗口时间内使用的百分比。 n% 的配置代表一个线程的 n%的使用率，所以这种配额是建立在总容量 ((num.io.threads + num.network.threads) * 100)%之上的. 每个 group 的client 的资源在被限制之前可以使用单位配额时间窗口内I/O线程和网络线程利用率的 n%。 由于分配给 I/O和网络线程的数量是基于 broker 的核数，所以请求量的配额代表每个group 的client 使用cpu的百分比。</p><h3 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h3><p>默认情况下，集群给每个不同的客户端group 配置固定的配额。 这个配额是以broker为基础定义的。每个 client 在受到限制之前可以利用每个broker配置的配额资源。 我们觉得给每个broker配置资源配额比为每个客户端配置一个固定的集群带宽资源要好，为每个客户端配置一个固定的集群带宽资源需要一个机制来共享client 在brokers上的配额使用情况。这可能比配额本身实现更难。</p><p>broker在检测到有配额资源使用违反规则会怎么办？在我们计划中，broker不会返回error，而是会尝试减速 client 超出的配额设置。 broker 会计算出将客户端限制到配额之下的延迟时间，并且延迟response响应。这种方法对于客户端来说也是透明的（客户端指标除外）。这也使得client不需要执行任何特殊的 backoff 和 retry 行为。而且不友好的客户端行为（没有 backoff 的重试）会加剧正在解决的资源配额问题。</p><p>网络字节速率和线程利用率可以用多个小窗口来衡量（例如 1秒30个窗口），以便快速的检测和修正配额规则的违反行为。实际情况中 较大的测量窗口（例如，30秒10个窗口）会导致大量的突发流量，随后长时间的延迟，会使得用户体验不是很好。</p>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【整理】饶军-Apache Kafka的过去、现在和未来</title>
      <link href="/posts/2eee52f0/"/>
      <url>/posts/2eee52f0/</url>
      
        <content type="html"><![CDATA[<p>大家好，我大概简单的介绍一下，我叫饶军，我是硅谷的初创公司Conﬂuent的联合创始人之一，我们公司的三个创始人最开始都是在领英这个公司做kafka开发出身的。我们公司是2014年成立的，成立的宗旨想把公司做成一个帮助各种各样企业做基于kafka之上的数据流的事情。</p><!-- more --><h2 id="2010的领英"><a href="#2010的领英" class="headerlink" title="2010的领英"></a>2010的领英</h2><img src="/posts/2eee52f0/002.png"><p>在开始之前，我想大概做一个简单的调查，在座的有谁用过Kafka。大概有80%的人都用了。好，谢谢。今天跟大家分享，想分享一下我们的项目，Kafka的发展，它是怎么创建起来的，然后他的一些经历。说起Kafka的话，那就要回朔到2010年，在这个领域，我是在2010年加入领英，可能很多人都熟悉，这是一个提供人才和机会的社交平台。在2010年的时候，领英初具了一点规模，这也是领英高速成长的一个阶段。在2010年我加入领英的时候大概是600号员工，我是2014年离开领英，离开的时候已经发展到6000号员工，在短短的四年的过程这个组织高速发展。</p><h2 id="数据驱动"><a href="#数据驱动" class="headerlink" title="数据驱动"></a>数据驱动</h2><img src="/posts/2eee52f0/003.png"><p>在领英的高速发展的过程中，之所以它能有这么高速发展与数据有不可分割的关系，领英像很多互联网公司一样，数据是它的核心，领英有自己的用户，通过自己提供的服务和用户交流，用户把自己的数据直接的或者间接地提供给领英，领英通过做各样的科研或者分析，可以提取出很多新的见识和认知，这些信息会被反馈到我们的产品上，这个产品就会做得更有效，可以吸引更多的用户到我们平台，所以如果数据做得好的话，可以形成一个非常好的良性循环，用户可以得到更多的数据，可以做更好的分析，可以生产更好的产品，又可以吸引到更多的用户。</p><h2 id="数据源的多样性"><a href="#数据源的多样性" class="headerlink" title="数据源的多样性"></a>数据源的多样性</h2><img src="/posts/2eee52f0/004.png"><p>从数据上角度来讲，领英的数据是非常多元化的，最常见的数据，可能大家都知道这是一种——交易数据，这些数据一般是存在数据库里的，从领英的角度来讲，这种这种交易数据就很简单，你提供工作简历，或者你上学的一些简历，包括你和里面成员的连接关系，都是一种交易性的数据，但是还存在大量的非交易数据，一些很多用户的行为数据，比如说作为一个用户，你点了哪个连接，你输入哪些搜索的关键词，这些其实都是非常有价值的信息。从我们内部的运营来讲，有很多的运营服务指标，一些应用程序的日志，然后到最后我们很多智能手机上的一些信息，这也非常有价值。所以从价值来讲，这些非交易性的数据的价值不亚于这些交易性数据的价值。但是从流量上来讲，这些非交易型的数据的流量可能是这种交易性数据的一百倍，甚至一千万倍的数据源。下面就举一个小的例子，看看领英怎么用这些数据的理念提供这个服务。</p><h2 id="people-you-may-know"><a href="#people-you-may-know" class="headerlink" title="people you may know"></a>people you may know</h2><img src="/posts/2eee52f0/005.png"><p>英文叫做people you may know，简称PYMK，这个机构做的事情就是提供给领英的用户一些推荐，他想推荐一些其他的领英的用户，目前还没有在你的连接里，他这个推荐是怎么做的呢？它内部里要用到30到40种信息，把这些信息加起来，使得给你最后一个推荐。举一些简单的例子，比如说我们两个人去过同一个学校，或者在一个公司工作过，这是一个很强的信息，也许我们就是需要连接在一起，但是有很这种非直接的信息，比如说甲和乙两个人，他们没有直接这种共同的一些明显的关系，但是如果在某一个很短的时间，有很多人同时看到这两个人的简历，那说明他们可能还有一些这种隐藏的信息，使得他们值得连在一起。所以在早期的领英，大家使用这个服务的话，就会发现很多的推荐非常神奇。你乍一看的话，可能觉得他怎么会推荐这么一个人给我，但是你如果细想一下，就会发现它有很多很强的理由，确实是有些道理，类似的在里面还有很多的服务，使得他可以用到各种各样的实时数据。但当时在2010年的时候，我们领英有很大的一块问题，就是在数据的集成上，其实是一个非常不完善的过程。这个图大概就介绍了一下当时的状态，所以在上面我看到这是各种各样的数据源，领英最开始是一个老牌的互联网公司，所有的数据都是存在数据库里头，随着理念的发展，我有一个系统是收集所有的用户行为的数据，很多的数据都是存在本地的文件里，还有一些其他的信息是存在运行上的日志里，运行一些识别监测数据。</p><p>在下游我们可以看到这是各种各样的消费端，领英最开始有这种数据仓库，随着时间的推移，我们有越来越多实时的微服务，它和这些批处理差不多，也要夺取或多或少同样的从这些不同数据源而来的信息。像我们刚才讲到的这种推荐引擎，它是其中的一个微服务，我们有很多这样的，还有一些社交的图形处理，他可以分析两个节点之间，比如两个领英的成员，他们之间是怎么连起来的，哪个连接是最强，还有一些实时的搜索，所以这些数量逐渐增多，而且他们很多的用法是更加的实时，从数据产生到它更新的系统，很多时候是几秒，甚至更短的一些延时。</p><h2 id="点到点的数据集成"><a href="#点到点的数据集成" class="headerlink" title="点到点的数据集成"></a>点到点的数据集成</h2><img src="/posts/2eee52f0/006.png"><p>所以当时我们的做法是，如果想把这些数据送到从数据源送到消费端的话，做法就是我们说的点到点的数据集成，我们知道有些数据，我们想要的是把这些数据送到数据仓库里，我们的做法是写下脚本或者写一些程序。过了几天，我们发现很多系统里也需要读数据，我们又会做一些类似的工作，又在写下脚本一些程序，所以我们一直在很长一段时间都在做这种类似的东西，但是我写了五六个类似的数据流之后，发现这是一个非常低效的做法。主要的问题是什么？第一个我们要解决的问题是一个叉乘问题，是和数据员和数据消费端叉乘的问题。所以每增加一个数据源，就要把这个数据源和所有的消费端都连起来，同样增加一个消费端的话，消费端需要和所有的数据源直接连接。第二个问题就是我们在做这种点到点的流数据流的时候，每做一个数据流，我们都要重复很多相同的工作，另外每一个数据源，我们都没有足够的时间把它做到百分之百的尽善尽美，所以我们觉得这个体系结构不是非常理想。</p><h2 id="理想架构"><a href="#理想架构" class="headerlink" title="理想架构"></a>理想架构</h2><img src="/posts/2eee52f0/007.png"><p>那么如果要改进的话，应该改进成什么样？我们当时想如果有这么一个体系结构，假设中间这个地方我们有一个集中式的日志系统，能够把所有数据源的信息先缓存住，如果能做到这一点，我们就会把这个框架大大的简化。所以你如果是数据源的话，你不需要知道所有的消费端，你唯一要做的事，就是把你的数据发送给中央的日志系统。同样你如果是一个消费端的话，你也不需要知道所有的数据源，你做的事情只是要像这种中央的日志系统去订阅你所要的消息，所以我们就把刚才叉乘问题简化成一个现实问题，关键的就是在体系结构里头，什么样的系统可以做这个中央的日志系统，所以这是我们当时在讨论的事情，我们最开始的话也没有想重新造一个新的系统，这个好像是一个非常常见的企业级的问题，那这个企业里应该有一个类似的解决方法。</p><h2 id="首先尝试：不想重复造轮子"><a href="#首先尝试：不想重复造轮子" class="headerlink" title="首先尝试：不想重复造轮子"></a>首先尝试：不想重复造轮子</h2><img src="/posts/2eee52f0/009.png"><p>如果你仔细看一看，想一想，中央日志系统从界面角度来讲，类似于传统的消息系统。我们的消息系统一般把这种生产端和消费端分开，然后又是一个非常实时的系统，所以我们想为什么不尝试一些现有的消息系统，当时又有一些开源的消息系统，还有一些企业级的消息系统，但我们发现效果非常的不好。具体的原因有很多，但是最重要的一个原因就是这些传统的消息系统，从它的设计上来讲，不是给我们这个用法来设计的，尤其最大的问题就是它的吞吐量。</p><h2 id="Kafka第一版：高吞吐发布订阅消息系统"><a href="#Kafka第一版：高吞吐发布订阅消息系统" class="headerlink" title="Kafka第一版：高吞吐发布订阅消息系统"></a>Kafka第一版：高吞吐发布订阅消息系统</h2><img src="/posts/2eee52f0/010.png"><p>很多的这种早期的消息，他的设计师给这些数据库上的数据，这种消费交易型数据来设计，但是你可能很难想象把很多非交易性数据，比如说用户行为日志，还有一些这种监测数据，都通过这种传统的消息来流通。所以在这种情况下，我们觉得我们没有办法解决这个问题，但又没有一个现成的结果，那么就说我们自己来做一个事情。在2010年左右，我们做了开始做kafka的第一个版本。第一个版本我们的定位也很简单，就想把它做成一个高吞吐的消息系统，高存储是我们最重要的目的。</p><h2 id="分布式架构"><a href="#分布式架构" class="headerlink" title="分布式架构"></a>分布式架构</h2><img src="/posts/2eee52f0/011.png"><p>下面的话，我们大概讲讲我们怎么实现高吞吐。第一件事我们做了高吞吐，就是把在咖啡的第一个版本里，我们就把它做成一个分布式的框架。很多熟悉kafka的人都知道，kafka里面有三层，中间这层加服务层下面是生产端，然后下面是消费端。服务端的话一般有一个或者多个节点，基本的概念叫做消息拓片，这个消息源是可以分区的，每一个分区可以放到某一个节点的某一个硬盘上，所以如果想增加吞吐的话，最简单的方法就是增加集群里的机器，可以有更多的资源，不管是从存储还是带宽的角度，你都可以有更多的资源来接受很多的数据，同样我们生产端和消费端做的也是一个这种多线程的设计。在任何一个情况下，你可以有成千上万的这种生产端的线程和消费端的线程，从喀斯特机群上写或者读取数据。所以这个设计就是说在我们第一个班级有的东西很多，这种老牌的一些消息系统。</p><h2 id="简单实用的日志存储"><a href="#简单实用的日志存储" class="headerlink" title="简单实用的日志存储"></a>简单实用的日志存储</h2><img src="/posts/2eee52f0/012.png"><p>第二点我们做的是使用了一个日志的存储结构，这个也非常简单，但是它是一个非常有效的存储结构，所以大概是它的一些结构的话是每一个消息源的分区，都会有一个相对应的这么一个日志结构，而且日志结构式和硬盘挂在一起的所有会是通过硬盘来存储的。这个结构里面就是每一个小方块都对应了一个消息，每一个消息有一个代号，代号是连续增加的，如果你是一个生产端的话，你做的事情就是你把你要写的消息写到日志的最后面，你会得到一个新的更大的消息代号日志，再送到给消费端的话是按顺序送的，你按什么顺序写进去，他就按什么顺序送给消费端，这样的好处是，从消费端来讲你的开销非常的小，因为不需要记住所有消费端的消息，只需要记住它最后消费过的一个消息的代号。然后记住这个的话，它就可以从这个地方往后继续消费，因为我们知道所有的消息都是按顺序去做发送的，所以在这个消息之前的所有消息应该已经全都被消费过了。</p><h2 id="两个优化"><a href="#两个优化" class="headerlink" title="两个优化"></a>两个优化</h2><img src="/posts/2eee52f0/013.png"><p>这个设计有几个好处，第一个好处就是他的访问的模式非常利于优化，因为不光是从写的角度还是从读的角度来讲，这个都是线性的写，读也是从某一个位置开始线性的读。所以从这个角度出发，利于操作系统和文件系统来优化它的性能。第二点，我们这个系统设置上可以支持同时多消费，在任何时候你可以有一个或者多个消费者，消费者他可以说从这个地方开始消费，另一个消费者可以从一个不同的地方再消费，但不管你有多少个消费者，这个数据只是存一次，所以从存储的角度来讲，它的性能和你消费的次数是没有关系的。另外一点并不是很明显的，由于我们日志是存在硬盘上的，使得我们可以同时接收实时的消费者，也可以接受一些不实时的批处理的消费者。但是因为所有的数据都在硬盘上，我们可以有一个非常大的缓存，所以不管你是实时还是不实时的，从消费者端的服务方法都是一套的，他不需要做不同的优化，唯一的就是我们依赖这种操作系统来决定哪些数据是可以从内存里提供给消费者，哪些需要从硬盘里来读。但是从这个框架的设计上都是一样的。最后一点我们做成这种高吞吐，我们又做了两个小的优化，这两个优化是有关联的，第一个优化就是批处理，所有三个层面在服务端，刚才我们说到这些消息是要存在一个基于硬盘的日志里，但是写到硬盘的话它是有一定的开销，所以我们不是每一个消息就马上写了这个硬盘，而是一般会等一段时间，当我们积攒了一些足够的消息之后，才把他一批写到硬盘，所以虽然你还有同样的开销，但你这个开销是分摊到很多消息上，同样在生产端也是这样，如果你想发送一个消息，我们一般也不是马上就把这个消息作为一个远程的请求发送给这个服务端，而是我们也会等一等，希望能够等到一些更多的消息，把他们一起打包送到这个服务端。和批处理相关的就是数据压缩，我们压缩也是在一批数据上进行压缩，而且是从端到端的压缩，如果你开启压缩的功能的话，再生产端我们先会等一批数据等到一批数据完成之后，我们会把这一批数据一起做一个压缩，击压缩一批数据，往往会得到比这个压缩每一个消息得到更好的压缩比例也是。不同的消息往往会有一些这种重复，然后压缩的数据会被从生产端送到这个服务端，那服务端会把数据压缩的格式存在日志里头在以压缩的格式送到消费端，直到消费端在消费一个消息的时候，我们才会把这个消息解压。所以如果你启动了压缩的话，我们不光节省了网络的开销，还节省了这个寄存开销，所以这两个都是非常有效的实现这种高吞吐的方法。所以我们第一个版本kafka做了大概有半年左右的时间，但是我们又花了更多的一点时间把它用到领英的数据线上，因为领英内部有很多的微服务，大概在我们2011年底的时候做完了这件事，这是当时的一些基本的数量。</p><h2 id="kafka在领英2011"><a href="#kafka在领英2011" class="headerlink" title="kafka在领英2011"></a>kafka在领英2011</h2><img src="/posts/2eee52f0/014.png"><p>生产端我们当时有几十万的消息被生产出来，然后有上百万的消息被消费，这个数据在当时还是非常的可观，而且领英当时有几百个微服务，上万个微服务的线程，更重要的是我们在做了这个事情之后，实现了这个领域内部的一个数据的民主化。在没有kafka之前，你如果是领英的一个工程师或者是一个产品经理，或者是一个数据分析家，你想做一些新的设计或者新的这种应用程序，最困难的问题是你不知道应该用什么样的界面去读取，也不知道这个数据是不是完整。做了kafka这件事情，我们就把这一块的问题大大的简化了，大大解放了工程师创新的能力。所以有了成功的经历，并且感觉kafka的这个系统非常有用，我们又往后做了一些更多的开发，第二部分的开发主要是做一些这种高可用性上的支持。</p><h2 id="Kafka第二版：高可用性"><a href="#Kafka第二版：高可用性" class="headerlink" title="Kafka第二版：高可用性"></a>Kafka第二版：高可用性</h2><img src="/posts/2eee52f0/015.png"><p>第一个版本里的话，每一个消息只是被存在一个节点上，如果那个节点下机的话，那这个数据就没法获取了。如果这个机器是永久性的损坏的话，你的数据还会丢失。所以第二版我们做的时候，就是增加了一些这种高可用性，实现方法就是增加的这种多副本的机制。如果群里有多个节点的话，那我们可以把一个消息冗余的存在多个副本上，同一个小的颜色是多个不同的副本。在同样的情况，如果你一个机器下线的话，另外一个迹象，如果还有同样的副本，他还可以继续持续提供同样数据的服务。所以有了第二个版本之后，我们就可以把它可能够包括的数据的面能够拓展得更广一点，不光是这种非交易性的时候，一些包括交易性的数据，也可以通过我们的系统来被收集。</p><h2 id="Kafka于2011年加入Apache"><a href="#Kafka于2011年加入Apache" class="headerlink" title="Kafka于2011年加入Apache"></a>Kafka于2011年加入Apache</h2><img src="/posts/2eee52f0/016.png"><p>在2000年我们还做了一件事，那一年是kafka这个项目被捐赠给了阿帕奇基金会，当时我们做这个事也是觉得我们做的系统至少对领域内部非常有用，那我们就看看是不是对其他的公司也有一些用处，其他的互联网公司也许也觉得有用，但是我没有意识到开源了之后，他用途是非常广泛，所以往往是布局网，不只是局限于这种互联网的公司而是整个工业界。只要你的公司有些这种实时数据，你需要收集的话都可以用得上。很大的原因是一些各种各样的传统的企业，它也在经历这种软件化数字化的过程。有一些传统行业，以前强的地方可能是在那些传统的制造业，或者说有一些零售点，但现在必须在软件上或在数据上也能够比较强才可以。那kafka就从实时这种数据的集成上，给很多企业都提供了非常有效的渠道。</p><h2 id="Confluent于2014年成立"><a href="#Confluent于2014年成立" class="headerlink" title="Confluent于2014年成立"></a>Confluent于2014年成立</h2><img src="/posts/2eee52f0/017.png"><p>在下一步我们经历了几年kafka的开发，知道它用途越来越广，所以我们就想做一件致力于kafka上的事，因为这个事是全职性的工作，所以我们在2014年就离开了领英成立了Conﬂuent公司。这个公司我们是想为各种各样的企业提供方便，用的可以更广一点，现在我们的公司大概是有超过两百人。</p><h2 id="Kafka的发展"><a href="#Kafka的发展" class="headerlink" title="Kafka的发展"></a>Kafka的发展</h2><img src="/posts/2eee52f0/018.png"><p>下面大概讲一讲从14年之后我们做了哪些发展。在这之后，kafka我们主要做了两块的东西，第一块和企业级的功能有关的东西，这块主要是和数据集成有关的。第二块是和数据流处理有关的。那么两方面都会稍微讲一讲。这个就跳过去了，在企业界上我们做了很大的一块，和刚才我们最开始讲的数据集成的事情有关。很多的这种公司，如果你的公司时间比较长的话，你会发现你数据源是分散在很多的系统里，刚才我们讲的就是有kafka之后很方便，你可以把这些数据提取出来，但是不同的公司的话，你不希望每个公司都读东西来做他们自己的一套东西，所以我们设计的初衷中有两块，第一块是它有一个平台部分，里面它把很多常见的东西提取出来，做成一个模块，比如说你需要做一些数据的分布，你需要做一些并行处理，你还需要做一些这种失败检测，检测之后你能再做一些数据平衡，所以这些常见的东西都做到这个模块里面，这个模块里面又有一个开放式的接口，这个接口可以用来设计实现各种各样的不同的数据源的连接。在数据的发送端的地方，如果想搜索一些副本，我们也可以做一些类似的事情，所以这是我们做的第一块。</p><p>第二块做的就是和数据流有关的一个方面。如果你有一个系统，像kafka里能实时把很多的数据收集起来，最开始的用途是当成一个数据传输的平台。但是我们觉得加以时间的话，kafka可能并不只局限一个传输平台，而且还可以做这种分享合作的平台，有实时数据之后，你常做的一些事情比如说你要做一些这种数据流的出来，比如说把一个数据从一个格式转到另外一个格式，你可能还想做一些数据的扩充，比如说你有一个数据流，里面有一些数据的信息，但只有用户的代号，没有数据的用户的具体信息，但是你有一个可能数据库里有很多更详细的用户的信息，如果您能够把这两个信息并在一起，这个数据流就更丰富，可以使你做一些更多的更有效的处理。另外你可能也想做一些实时的数据的聚合，应用程序里，我们想把这一块再简化。</p><h2 id="数据安全"><a href="#数据安全" class="headerlink" title="数据安全"></a>数据安全</h2><img src="/posts/2eee52f0/019.png"><h2 id="Kafka-connect"><a href="#Kafka-connect" class="headerlink" title="Kafka connect"></a>Kafka connect</h2><img src="/posts/2eee52f0/020.png"><h2 id="数据流处理"><a href="#数据流处理" class="headerlink" title="数据流处理"></a>数据流处理</h2><img src="/posts/2eee52f0/021.png"><h2 id="Kafka-Streams"><a href="#Kafka-Streams" class="headerlink" title="Kafka Streams"></a>Kafka Streams</h2><img src="/posts/2eee52f0/022.png"><h2 id="KSQL"><a href="#KSQL" class="headerlink" title="KSQL"></a>KSQL</h2><img src="/posts/2eee52f0/023.png"><h2 id="Kafka的未来"><a href="#Kafka的未来" class="headerlink" title="Kafka的未来"></a>Kafka的未来</h2><img src="/posts/2eee52f0/024.png"><p>未来的话，我觉得kafka系统不光是一个实时的数据收集和传输的平台，更多的可能随着时间发展的话，它可能还是更多的数据流的处理，交换和共享的一个平台，所以我们会在这个方向上做更多的东西。未来随着很多的应用更广，我们觉得很多的应用程序会越来越变成这种实时的应用程序。所以在这个基础上，我们在kafka上可能会有很强的一套生态系统。</p><h2 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h2><img src="/posts/2eee52f0/025.png"><p>最后给大家分享一个小的故事。这个故事是是我们北美的一个用户，这个银行是一个比较传统的老牌银行，已经是几十年的一个历史银行，很长时间存在的一个问题是它的数据是非常的分分散的。所以你如果是这个银行的客户，你可能有一个银行的账户，你可能有一个贷款，你可能还有一个保险，你可能还有一张信用卡，以前所有这个客户的信息，因为它都是不同的商业部门，它都是完全分开的。你如果作为一个银行的销售人员，你苦恼的事就是没法知道这个客户的所有的信息。这个公司它做了一个和Kafka有关的项目，一个项目就是把所有的客户的不同的数据源信息都实时地收集起来，然后把这个信息推提供给他们上万个销售人员，这样的话销售人员在做销售的时候，就会有更有效的一些实时信息可以给客户做一些更有针对的推荐，所以这个项目就非常成功。</p>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka演讲 </tag>
            
            <tag> 饶军 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka介绍</title>
      <link href="/posts/4161d32c/"/>
      <url>/posts/4161d32c/</url>
      
        <content type="html"><![CDATA[<p>Kafka最初是由Linkedin公司开发的消息引擎，后来捐赠给了Apache软件基金会。起初，人们说Kafka是一个可扩展的，支持发布-订阅模式的消息系统；后来，人们说Kafka是一个实时数据管道；再后来，人们又说Kafka是一个分布式复制日志系统，可构建统一的企业数据集成堆栈；现在，Apache Kafka的官网说Kafka是一个分布式流平台。其实，这些说法都对，Kafka是一个不断迭代演进的系统，从最初的发布-订阅模式的消息系统逐步发展成为一个分布式流数据处理平台。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>Kafka作为一个分布式流处理平台。需要提供以下三种特性:</p><ol><li>发布和订阅流式的记录。这一方面与消息队列或者企业消息系统类似。</li><li>储存流式记录，并且提供较好的容错性。</li><li>在流式记录产生时就进行处理。</li></ol><p>Kafka可以用于构建两大类别的应用:</p><ol><li>构造实时流数据管道，它可以在系统或应用之间可靠地获取数据。 (相当于message queue)</li><li>构建实时流式应用程序，对这些流数据进行转换或者影响。 (就是流处理，通过kafka stream topic和topic之间内部进行变化)</li></ol><p>Kafka作为一个集群，运行在一台或者多台服务器上。Kafka通过 <em>Topic</em> 对存储的流数据进行分类。每条记录中包含一个key，一个value和一个timestamp（时间戳）</p><p>Kafka提供四个核心的API:</p><ul><li><a href="http://kafka.apachecn.org/documentation.html#producerapi" target="_blank" rel="noopener">Producer API</a> 允许一个应用程序发布一串流式的数据到一个或者多个topic。</li><li><a href="http://kafka.apachecn.org/documentation.html#consumerapi" target="_blank" rel="noopener">Consumer API</a> 允许一个应用程序订阅一个或多个 topic ，并且对发布给他们的流式数据进行处理。</li><li><a href="http://kafka.apachecn.org/documentation/streams" target="_blank" rel="noopener">Streams API</a> 允许一个应用程序作为一个<em>流处理器</em>，消费一个或者多个topic产生的输入流，然后生产一个输出流到一个或多个topic中去，在输入输出流中进行有效的转换。</li><li><a href="http://kafka.apachecn.org/documentation.html#connect" target="_blank" rel="noopener">Connector API</a> 允许构建并运行可重用的生产者或者消费者，将topics连接到已存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表（table）的所有变更内容。</li></ul><p>下面这张图对kafka的整体功能做了一个形象化的描述。</p><img src="/posts/4161d32c/001.png"><p>在Kafka中，客户端和服务器使用一个简单、高性能、支持多语言的 <a href="https://kafka.apache.org/protocol.html" target="_blank" rel="noopener">TCP 协议</a>.此协议版本化并且向下兼容老版本， 我们为Kafka提供了Java客户端，也支持许多<a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients" target="_blank" rel="noopener">其他语言的客户端</a>。</p><h1 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h1><p>Kafka 属于分布式的消息引擎系统，它的主要功能是提供一套完备的消息发布与订阅解决方案。</p><h2 id="整体架构图"><a href="#整体架构图" class="headerlink" title="整体架构图"></a>整体架构图</h2><img src="/posts/4161d32c/002.png"><h2 id="主题和分区"><a href="#主题和分区" class="headerlink" title="主题和分区"></a>主题和分区</h2><p>Topic 就是数据主题，是数据记录发布的地方,可以用来区分业务系统。Kafka中的Topics总是多订阅者模式，一个topic可以拥有一个或者多个消费者来订阅它的数据。</p><p>对于每一个topic， Kafka集群都会维持一个分区日志，如下所示：</p><img src="/posts/4161d32c/003.png"><p>每个分区都是有序且顺序不可变的记录集，并且不断地追加到结构化的commit log文件。分区中的每一个记录都会分配一个id号来表示顺序，我们称之为offset，<em>offset</em>用来唯一的标识分区中每一条记录。</p><p>Kafka 集群保留所有发布的记录—无论他们是否已被消费—并通过一个可配置的参数——保留期限来控制. 举个例子， 如果保留策略设置为2天，一条记录发布后两天内，可以随时被消费，两天过后这条记录会被抛弃并释放磁盘空间。Kafka的性能和数据大小无关，所以长时间存储数据没有什么问题.</p><img src="/posts/4161d32c/004.png"><p>事实上，在每一个消费者中唯一保存的元数据是offset（偏移量）即消费在log中的位置.偏移量由消费者所控制:通常在读取记录后，消费者会以线性的方式增加偏移量，但是实际上，由于这个位置由消费者控制，所以消费者可以采用任何顺序来消费记录。例如，一个消费者可以重置到一个旧的偏移量，从而重新处理过去的数据；也可以跳过最近的记录，从”现在”开始消费。</p><p>这些细节说明Kafka 消费者是非常廉价的—-消费者的增加和减少，对集群或者其他消费者没有多大的影响。比如，你可以使用命令行工具，对一些topic内容执行 tail操作，并不会影响已存在的消费者消费数据。</p><p>日志中的 partition（分区）有以下几个用途。第一，当日志大小超过了单台服务器的限制，允许日志进行扩展。每个单独的分区都必须受限于主机的文件限制，不过一个主题可能有多个分区，因此可以处理无限量的数据。第二，可以作为并行的单元集，分布在多个服务器上，确保容错性，实现集群的负载均衡。</p><h2 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h2><p>日志的分区partition （分布）在Kafka集群的服务器上。每个服务器在处理数据和请求时，共享这些分区。每一个分区都会在已配置的服务器上进行备份，确保容错性.</p><p>每个分区都有一台 server 作为 “leader”，零台或者多台server作为 follwers 。leader server 处理一切对 partition （分区）的读写请求，而follwers只需被动的同步leader上的数据。当leader宕机了，followers 中的一台服务器会自动成为新的 leader。每台 server 都会成为某些分区的 leader 和某些分区的 follower，因此集群的负载是平衡的。</p><h2 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h2><p>生产者可以将数据发布到所选择的topic（主题）中。生产者负责将记录分配到topic的哪一个 partition（分区）中。可以使用循环的方式来简单地实现负载均衡，也可以根据某些语义分区函数(例如：记录中的key)来完成。下面会介绍更多关于分区的使用。</p><h2 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h2><p>消费者使用一个 <em>消费组</em> 名称来进行标识，发布到topic中的每条记录被分配给订阅消费组中的一个消费者实例.消费者实例可以分布在多个进程中或者多个机器上。</p><p>如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例.</p><p>如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程.</p><img src="/posts/4161d32c/005.png"><p>如图，这个 Kafka 集群有两台 server 的，四个分区(p0-p3)和两个消费者组。消费组A有两个消费者，消费组B有四个消费者。</p><p>通常情况下，每个 topic 都会有一些消费组，一个消费组对应一个”逻辑订阅者”。一个消费组由许多消费者实例组成，便于扩展和容错。这就是发布和订阅的概念，只不过订阅者是一组消费者而不是单个的进程。</p><p>在Kafka中实现消费的方式是将日志中的分区划分到每一个消费者实例上，以便在任何时间，每个实例都是分区唯一的消费者。维护消费组中的消费关系由Kafka协议动态处理。如果新的实例加入组，他们将从组中其他成员处接管一些 partition 分区;如果一个实例消失，拥有的分区将被分发到剩余的实例。</p><p>Kafka 只保证分区内的记录是有序的，而不保证主题中不同分区的顺序。每个 partition 分区按照key值排序足以满足大多数应用程序的需求。但如果你需要总记录在所有记录的上面，可使用仅有一个分区的主题来实现，这意味着每个消费者组只有一个消费者进程。</p><h2 id="可靠性保证"><a href="#可靠性保证" class="headerlink" title="可靠性保证"></a>可靠性保证</h2><p>high-level Kafka给予以下保证:</p><ul><li>生产者发送到特定topic partition 的消息将按照发送的顺序处理。 也就是说，如果记录M1和记录M2由相同的生产者发送，并先发送M1记录，那么M1的偏移比M2小，并在日志中较早出现</li><li>一个消费者实例按照日志中的顺序查看记录.</li><li>对于具有N个副本的主题，我们最多容忍N-1个服务器故障，从而保证不会丢失任何提交到日志中的记录.</li></ul><h2 id="Kafka作为消息系统"><a href="#Kafka作为消息系统" class="headerlink" title="Kafka作为消息系统"></a>Kafka作为消息系统</h2><p>Kafka streams的概念与传统的企业消息系统相比如何？</p><p>传统的消息系统有两个模块: <a href="http://en.wikipedia.org/wiki/Message_queue" target="_blank" rel="noopener">队列</a> 和 <a href="http://en.wikipedia.org/wiki/Publish–subscribe_pattern" target="_blank" rel="noopener">发布-订阅</a>。 在队列中，消费者池从server读取数据，每条记录被池子中的一个消费者消费; 在发布订阅中，记录被广播到所有的消费者。两者均有优缺点。 队列的优点在于它允许你将处理数据的过程分给多个消费者实例，使你可以扩展处理过程。 不好的是，队列不是多订阅者模式的—一旦一个进程读取了数据，数据就会被丢弃。 而发布-订阅系统允许你广播数据到多个进程，但是无法进行扩展处理，因为每条消息都会发送给所有的订阅者。</p><p>消费组在Kafka有两层概念。在队列中，消费组允许你将处理过程分发给一系列进程(消费组中的成员)。 在发布订阅中，Kafka允许你将消息广播给多个消费组。</p><p>Kafka的优势在于每个topic都有以下特性—可以扩展处理并且允许多订阅者模式—不需要只选择其中一个.</p><p>Kafka相比于传统消息队列还具有更严格的顺序保证</p><p>传统队列在服务器上保存有序的记录，如果多个消费者消费队列中的数据， 服务器将按照存储顺序输出记录。 虽然服务器按顺序输出记录，但是记录被异步传递给消费者， 因此记录可能会无序的到达不同的消费者。这意味着在并行消耗的情况下， 记录的顺序是丢失的。因此消息系统通常使用“唯一消费者”的概念，即只让一个进程从队列中消费， 但这就意味着不能够并行地处理数据。</p><p>Kafka 设计的更好。topic中的partition是一个并行的概念。 Kafka能够为一个消费者池提供顺序保证和负载平衡，是通过将topic中的partition分配给消费者组中的消费者来实现的， 以便每个分区由消费组中的一个消费者消耗。通过这样，我们能够确保消费者是该分区的唯一读者，并按顺序消费数据。 众多分区保证了多个消费者实例间的负载均衡。但请注意，消费者组中的消费者实例个数不能超过分区的数量。</p><h2 id="Kafka-作为存储系统"><a href="#Kafka-作为存储系统" class="headerlink" title="Kafka 作为存储系统"></a>Kafka 作为存储系统</h2><p>许多消息队列可以发布消息，除了消费消息之外还可以充当中间数据的存储系统。那么Kafka作为一个优秀的存储系统有什么不同呢?</p><p>数据写入Kafka后被写到磁盘，并且进行备份以便容错。直到完全备份，Kafka才让生产者认为完成写入，即使写入失败Kafka也会确保继续写入</p><p>Kafka使用磁盘结构，具有很好的扩展性—50kb和50TB的数据在server上表现一致。</p><p>可以存储大量数据，并且可通过客户端控制它读取数据的位置，您可认为Kafka是一种高性能、低延迟、具备日志存储、备份和传播功能的分布式文件系统。</p><h2 id="Kafka用做流处理"><a href="#Kafka用做流处理" class="headerlink" title="Kafka用做流处理"></a>Kafka用做流处理</h2><p>Kafka 流处理不仅仅用来读写和存储流式数据，它最终的目的是为了能够进行实时的流处理。</p><p>在Kafka中，流处理器不断地从输入的topic获取流数据，处理数据后，再不断生产流数据到输出的topic中去。例如，零售应用程序可能会接收销售和出货的输入流，经过价格调整计算后，再输出一串流式数据。</p><p>简单的数据处理可以直接用生产者和消费者的API。对于复杂的数据变换，Kafka提供了Streams API。 Stream API 允许应用做一些复杂的处理，比如将流数据聚合或者join。</p><p>这一功能有助于解决以下这种应用程序所面临的问题：处理无序数据，当消费端代码变更后重新处理输入，执行有状态计算等。</p><p>Streams API建立在Kafka的核心之上：它使用Producer和Consumer API作为输入，使用Kafka进行有状态的存储， 并在流处理器实例之间使用相同的消费组机制来实现容错。</p><h2 id="Kafka就是一个分布式流数据平台"><a href="#Kafka就是一个分布式流数据平台" class="headerlink" title="Kafka就是一个分布式流数据平台"></a>Kafka就是一个分布式流数据平台</h2><p>将消息、存储和流处理结合起来，使得Kafka看上去不一般，但这是它作为流平台所备的.</p><p>像HDFS这样的分布式文件系统可以存储用于批处理的静态文件。 一个系统如果可以存储和处理历史数据是非常不错的。</p><p>传统的企业消息系统允许处理订阅后到达的数据。以这种方式来构建应用程序，并用它来处理即将到达的数据。</p><p>Kafka结合了上面所说的两种特性。作为一个流应用程序平台或者流数据管道，这两个特性，对于Kafka 来说是至关重要的。</p><p>通过组合存储和低延迟订阅，流式应用程序可以以同样的方式处理过去和未来的数据。 一个单一的应用程序可以处理历史记录的数据，并且可以持续不断地处理以后到达的数据，而不是在到达最后一条记录时结束进程。 这是一个广泛的流处理概念，其中包含批处理以及消息驱动应用程序</p><p>同样，作为流数据管道，能够订阅实时事件使得Kafk具有非常低的延迟; 同时Kafka还具有可靠存储数据的特性，可用来存储重要的支付数据， 或者与离线系统进行交互，系统可间歇性地加载数据，也可在停机维护后再次加载数据。流处理功能使得数据可以在到达时转换数据。</p><img src="/posts/4161d32c/006.png"><h2 id="核心概念总结"><a href="#核心概念总结" class="headerlink" title="核心概念总结"></a>核心概念总结</h2><ol><li><strong>Record/Message</strong>：消息，Kafka 是消息引擎嘛，这里的消息就是指 Kafka 处理的主要对象。</li><li><strong>Offset</strong>：消息位移，表示分区中每条消息的位置信息，是一个单调递增且不变的值。</li><li><strong>Topic</strong>：主题，主题是承载消息的逻辑容器，在实际使用中多用来区分具体的业务。</li><li><strong>Partition</strong>：分区，一个有序不变的消息序列。每个主题下可以有多个分区。</li><li><strong>Broker</strong>：代理，消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。</li><li><strong>Leader</strong>：领导者</li><li><strong>Follower</strong>：追随者</li><li><strong>Replica</strong>：副本，Kafka 中同一条消息能够被拷贝到多个地方以提供数据冗余，这些地方就是所谓的副本。副本还分为领导者副本和追随者副本，各自有不同的角色划分。副本是在分区层级下的，即每个分区可配置多个副本实现高可用。</li><li><strong>Producer</strong>：生产者，向主题发布消息的应用程序。</li><li><strong>Consumer</strong>：消费者，从主题订阅消息的应用程序。</li><li><strong>Consumer Group</strong>：消费者组，多个消费者实例共同组成的一个组，同时消费多个分区以实现高吞吐。</li><li><strong>Consumer Offset</strong>：消费者位移，表征消费者消费进度，每个消费者都有自己的消费者位移。</li><li><strong>Rebalance</strong>：重平衡，消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。Rebalance 是 Kafka 消费者端实现高可用的重要手段。</li></ol><h1 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h1><p>以下描述了一些 ApacheKafka ®的流行用例。有关这些领域的概述，请参阅 <a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/" target="_blank" rel="noopener">此博客中的文章</a>。</p><p><strong>消息</strong></p><p>Kafka 很好地替代了传统的message broker（消息代理）。 Message brokers 可用于各种场合（如将数据生成器与数据处理解耦，缓冲未处理的消息等）。 与大多数消息系统相比，Kafka拥有更好的吞吐量、内置分区、具有复制和容错的功能，这使它成为一个非常理想的大型消息处理应用。</p><p>根据我们的经验，通常消息传递使用较低的吞吐量，但可能要求较低的端到端延迟，Kafka提供强大的持久性来满足这一要求。</p><p>在这方面，Kafka 可以与传统的消息传递系统（<a href="http://activemq.apache.org/" target="_blank" rel="noopener">ActiveMQ</a> 和 <a href="https://www.rabbitmq.com/" target="_blank" rel="noopener">RabbitMQ</a>）相媲美。</p><p><strong>跟踪网站活动</strong></p><p>Kafka 的初始用例是将用户活动跟踪管道重建为一组实时发布-订阅源。 这意味着网站活动（浏览网页、搜索或其他的用户操作）将被发布到中心topic，其中每个活动类型有一个topic。 这些订阅源提供一系列用例，包括实时处理、实时监视、对加载到Hadoop或离线数据仓库系统的数据进行离线处理和报告等。</p><p>每个用户浏览网页时都生成了许多活动信息，因此活动跟踪的数据量通常非常大</p><p><strong>度量</strong></p><p>Kafka 通常用于监控数据。这涉及到从分布式应用程序中汇总数据，然后生成可操作的集中数据源。</p><p><strong>日志聚合</strong></p><p>许多人使用Kafka来替代日志聚合解决方案。 日志聚合系统通常从服务器收集物理日志文件，并将其置于一个中心系统（可能是文件服务器或HDFS）进行处理。 Kafka 从这些日志文件中提取信息，并将其抽象为一个更加清晰的消息流。 这样可以实现更低的延迟处理且易于支持多个数据源及分布式数据的消耗。 与Scribe或Flume等以日志为中心的系统相比，Kafka具备同样出色的性能、更强的耐用性（因为复制功能）和更低的端到端延迟。</p><p><strong>流处理</strong></p><p>许多Kafka用户通过管道来处理数据，有多个阶段： 从Kafka topic中消费原始输入数据，然后聚合，修饰或通过其他方式转化为新的topic， 以供进一步消费或处理。 例如，一个推荐新闻文章的处理管道可以从RSS订阅源抓取文章内容并将其发布到“文章”topic; 然后对这个内容进行标准化或者重复的内容， 并将处理完的文章内容发布到新的topic; 最终它会尝试将这些内容推荐给用户。 这种处理管道基于各个topic创建实时数据流图。从0.10.0.0开始，在Apache Kafka中，<a href="http://kafka.apachecn.org/documentation/streams" target="_blank" rel="noopener">Kafka Streams</a> 可以用来执行上述的数据处理，它是一个轻量但功能强大的流处理库。除Kafka Streams外，可供替代的开源流处理工具还包括<a href="https://storm.apache.org/" target="_blank" rel="noopener">Apache Storm</a> 和<a href="http://samza.apache.org/" target="_blank" rel="noopener">Apache Samza</a>.</p><p><strong>采集日志</strong></p><p><a href="http://martinfowler.com/eaaDev/EventSourcing.html" target="_blank" rel="noopener">Event sourcing</a>是一种应用程序设计风格，按时间来记录状态的更改。 Kafka 可以存储非常多的日志数据，为基于 event sourcing 的应用程序提供强有力的支持。</p><p><strong>提交日志</strong></p><p>Kafka 可以从外部为分布式系统提供日志提交功能。 日志有助于记录节点和行为间的数据，采用重新同步机制可以从失败节点恢复数据。 Kafka的<a href="http://kafka.apachecn.org/documentation.html#compaction" target="_blank" rel="noopener">日志压缩</a> 功能支持这一用法。 这一点与<a href="http://zookeeper.apache.org/bookkeeper/" target="_blank" rel="noopener">Apache BookKeeper</a> 项目类似。</p>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【整理】日志-每个软件工程师都应该知道的有关实时数据的统一抽象</title>
      <link href="/posts/c85ca198/"/>
      <url>/posts/c85ca198/</url>
      
        <content type="html"><![CDATA[<p>原文：<a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying" target="_blank" rel="noopener">The Log: What every software engineer should know about real-time data’s unifying abstraction</a></p><p>我在六年前的一个令人兴奋的时刻加入了LinkedIn公司。从那个时候开始，我们为了解决单一的集中式数据库的限制，启动了专用的分布式系统套件的转换工作。这是一个令人兴奋的经历：我们构建部署了分布式图形数据库、分布式搜索后端、Hadoop套件以及第一代和第二代key-value存储，而且这些系统直到今天仍然在运行。</p><p>从这一切里我们体会到的最有益的事情是我们构建的许多东西的核心里都包含一个简单的理念：日志。有时候也称作预先写入日志（write-ahead logs）或者提交日志（commit logs）或者事务日志（transaction logs），日志几乎在计算机产生的时候就存在，同时它还是许多分布式数据系统和实时应用架构的核心。</p><p>不懂得日志，你就不可能完全懂得数据库、NoSQL存储、key-value存储、复制、paxos、Hadoop、版本控制以及几乎所有的软件系统；然而大多数软件工程师对它不是很熟悉。我愿意改变这种现状。在这篇博客文章里，我将带你浏览你必须了解的有关日志的所有的东西，包括日志是什么，如何在数据集成、实时处理和系统构建中使用日志等等。</p><!-- more --><h1 id="第一部分：日志是什么？"><a href="#第一部分：日志是什么？" class="headerlink" title="第一部分：日志是什么？"></a>第一部分：日志是什么？</h1><h2 id="什么是日志"><a href="#什么是日志" class="headerlink" title="什么是日志"></a>什么是日志</h2><p>日志可能是一种简单的不能再简单的存储抽象。它只能追加、按照时间排序的完全有序的记录序列。日志看起来象下面这样：</p><img src="/posts/c85ca198/001.jpeg" title="the log image"><p>记录被添加到日志的末尾，读取记录时，从左到右读取。每一条记录都指定了一个唯一的顺序的日志记录编号。</p><p>日志记录的次序（ordering）定义了『时间』概念，因为位于左边的日志记录表示比右边的要早。 日志记录编号可以看作是这条日志记录的『时间戳』。 把次序直接看成是时间概念，刚开始你会觉得有点怪异，但是这样的做法有个便利的性质：解耦了时间和任一特定的物理时钟。 引入分布式系统后，这会成为一个必不可少的性质。</p><p>日志记录的内容和格式是什么对于本文要讨论的内容并不重要。另外，不可能一直给日志添加记录，因为总会耗尽存储空间。稍后我们会再回来讨论这个问题。</p><p>所以，日志和文件、数据表并没有什么大的不同。文件是一系列字节，表是由一系列记录组成，而日志实际上只是一种按照时间顺序存储记录的数据表或文件。</p><p>讨论到现在，你可能奇怪为什么要讨论这么简单的概念？只能追加的有序的日志记录究竟又是怎样与数据系统产生关系的？ 答案是日志有其特定的目标：它记录了什么时间发生了什么事情。而对分布式数据系统，在许多方面，这是要解决的问题的真正核心。</p><p>不过，在我们进行更加深入的讨论之前，让我先澄清让人混淆的一些概念。每个程序员都熟悉另一种日志记录的定义 —— 应用使用syslog或者log4j写入到本地文件里的无结构的错误信息或者追踪信息。为了区分，这种情形的称为『应用日志记录』。 应用日志记录是我说的日志概念的一种退化。两者最大的区别是：应用日志意味着主要用来方便人去阅读，而构建我所说的『日志（journal）』或者『数据日志（data logs）』是用于程序的访问。（实际上，如果你深入地思考一下，会觉得人去阅读某个机器上的日志这样的想法有些落伍过时了。 当涉及很多服务和服务器时，这样的做法很快就变得难于管理， 我们的目的很快就变成输入查询和输出用于理解多台机器的行为的图表， 因此，文件中的字句文本几乎肯定不如本文所描述的结构化日志更合适。）</p><h2 id="数据库中的日志"><a href="#数据库中的日志" class="headerlink" title="数据库中的日志"></a>数据库中的日志</h2><p>我不知道日志概念的起源 —— 可能就像二分查找（binary search）一样，发明者觉得太简单了而不是一项发明。早在IBM的<a href="http://www.cs.berkeley.edu/~brewer/cs262/SystemR.pdf" target="_blank" rel="noopener">系统R</a>出现的时候日志就出现了。 在数据库里的用法是在崩溃的时候用它来保持各种数据结构和索引的同步。为了保证操作的原子性（atomic）和持久性（durable)， 在对数据库维护的所有各种数据结构做更改之前，数据库会把要做的更改操作的信息写入日志。 日志记录了发生了什么，而每个表或者索引都是更改历史中的一个投影。由于日志是立即持久化的，发生崩溃时，可以作为恢复其他所有持久化结构的可靠来源。</p><p>随着时间的推移，日志的用途从ACID的实现细节成长为数据库间复制数据的一种方法。 结果证明，发生在数据库上的更改序列即是与远程副本数据库（replica database）保持同步所需的操作。 Oracle、MySQL 和PostgreSQL都包括一个日志传送协议，传输日志给作为备库的副本数据库。 Oracle还把日志产品化为一个通用的数据订阅机制，为非Oracle数据订阅用户提供了<a href="http://docs.oracle.com/cd/E11882_01/server.112/e16545/xstrm_intro.htm" target="_blank" rel="noopener">XStreams</a>和<a href="http://www.oracle.com/technetwork/middleware/goldengate/overview/index.html" target="_blank" rel="noopener">GoldenGate</a>，在MySQL和PostgreSQL中类似设施是许多数据架构的关键组件。</p><p>正是由于这样的起源，机器可识别的日志的概念主要都被局限在数据库的内部。日志作为做数据订阅机制的用法似乎是偶然出现的。 但这正是支持各种的消息传输、数据流和实时数据处理的理想抽象。</p><h2 id="分布式系统中的日志"><a href="#分布式系统中的日志" class="headerlink" title="分布式系统中的日志"></a>分布式系统中的日志</h2><p>日志解决了两个问题：更改动作的排序和数据的分发，这两个问题在分布式数据系统中尤为重要。 协商达成一致的更改动作的顺序（或是协商达成不一致做法并去做有副作用的数据拷贝）是分布式系统设计的核心问题之一。</p><p>分布式系统以日志为中心的方案是来自于一个简单的观察，我称之为<strong>状态机复制原理（State Machine Replication Principle）：如果两个相同的、确定性的进程从同一状态开始，并且以相同的顺序获得相同的输入，那么这两个进程将会生成相同的输出，并且结束在相同的状态。</strong>听起来有点难以晦涩，让我们更加深入的探讨，弄懂它的真正含义。</p><p><a href="http://en.wikipedia.org/wiki/Deterministic_algorithm" target="_blank" rel="noopener">确定性</a>（deterministic）意味着处理过程是与时间无关的，而且不让任何其他『带外数据（out of band）』的输入影响处理结果。 例如，如果一个程序的输出会受到线程执行的具体顺序影响，或者受到getTimeOfDay调用或者其他一些非重复性事件的影响，那么这样的程序一般被认为是非确定性的。</p><p>进程状态是进程保存在机器上的任何数据，在进程处理结束的时候，这些数据要么保存在内存里，要么保存在磁盘上。</p><p>当碰到以相同的顺序输入相同的内容的情况时，应该触发你的条件反射：这个地方要引入日志。 下面是个很直觉的意识：如果给两段确定性代码相同的日志输入，那么它们就会生产相同的输出。</p><p>应用到分布式计算中就相当明显了。你可以把用多台机器都执行相同事情的问题化简为实现用分布式一致性日志作为这些处理的输入的问题。 这里日志的目的是把所有非确定性的东西排除在输入流之外，以确保处理这些输入的各个副本保持同步。</p><p>当你理解了这个以后，状态机复制原理就不再复杂或深奥了：这个原理差不多就等于说的是『确定性的处理过程就是确定性的』。不管怎样，我认为它是分布式系统设计中一个更通用的工具。</p><p>这个方案的一个美妙之处就在于：用于索引日志的时间戳就像用于保持副本状态的时钟 —— 你可以只用一个数字来描述每一个副本，即这个副本已处理的最大日志记录的时间戳。 日志中的时间戳 一一对应了 副本的完整状态。</p><p>根据写进日志的内容，这个原理可以有不同的应用方式。举个例子，我们可以记录一个服务的输入请求日志，或者从请求到响应服务的状态变化日志，或者服务所执行的状态转换命令的日志。 理论上来说，我们甚至可以记录各个副本执行的机器指令序列的日志或是所调用的方法名和参数序列的日志。 只要两个进程用相同的方式处理这些输入，这些副本进程就会保持一致的状态。</p><p>对日志用法不同群体有不同的说法。数据库工作者通常说成物理日志（physical logging）和逻辑日志（logical logging）。物理日志是指记录每一行被改变的内容。逻辑日志记录的不是改变的行而是那些引起行的内容改变的SQL语句（insert、update和delete语句等等）。</p><p>分布式系统文献通常把处理和复制方案宽泛地分成两种。『状态机器模型』常常被称为主-主模型（active-active model）， 记录输入请求的日志，各个复本处理每个请求。 对这个模型做了细微的调整称为『主备模型』（primary-backup model），即选出一个副本做为leader，让leader按请求到达的顺序处理请求，并输出它请求处理的状态变化日志。 其他的副本按照顺序应用leader的状态变化日志，保持和leader同步，并能够在leader失败的时候接替它成为leader。</p><img src="/posts/c85ca198/002.png"><p>为了理解两种方式的差异，我们来看一个不太严谨的例子。假定有一个要复制的『算法服务』，维护一个独立的数字作为它的状态（初始值为0），可以对这个值进行加法和乘法运算。 主-主方式所做的可能的是输出所进行的变换的日志，比如『+1』、『*2』等。各个副本都会应用这些变换，从而经过一系列相同的值。 而主备方式会有一个独立的Master执行这些变换并输出结果日志，比如『1』、『3』、『6』等。 这个例子也清楚的展示了为什么说顺序是保证各副本间一致性的关键：加法和乘法的顺序的改变将会导致不同的结果。</p><img src="/posts/c85ca198/003.jpg"><p>分布式日志可以看作是建模<a href="https://en.wikipedia.org/wiki/Consensus_(computer_science)" target="_blank" rel="noopener">一致性</a>（consensus）问题的数据结构。 因为日志代表了『下一个』追加值的一系列决策。 你需要眯起眼睛才能从<a href="http://en.wikipedia.org/wiki/Paxos_(computer_science)" target="_blank" rel="noopener">Paxos</a>算法簇中找到日志的身影，尽管构建日志是它们最常见的实际应用。 Paxos通过称为multi-paxos的一个扩展协议来构建日志，把日志建模为一系列一致性值的问题，日志的每个记录对应一个一致性值。 日志的身影在<a href="http://www.stanford.edu/class/cs347/reading/zab.pdf" target="_blank" rel="noopener">ZAB</a>、<a href="https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf" target="_blank" rel="noopener">RAFT</a>和<a href="http://pmg.csail.mit.edu/papers/vr-revisited.pdf" target="_blank" rel="noopener">Viewstamped Replication</a>等其它的协议中明显得多，这些协议建模的问题直接就是维护分布式一致的日志。</p><p>个人有一点感觉，在这个问题上，我们的思路被历史发展有些带偏了，可能是由于过去的几十年中，分布式计算的理论远超过了其实际应用。 在现实中，一致性问题是有点被过于简单化了。计算机系统几乎不需要决定单个的值，要的是处理一序列的请求。 所以，日志而不是一个简单的单值寄存器是更自然的抽象。</p><p>此外，对算法的专注掩盖了系统底层所需的日志抽象。 个人觉得，我们最终会更关注把日志作为一个商品化的基石而不是考虑它的实现，就像我们经常讨论哈希表而不纠结于它的细节， 比如使用线性探测的杂音哈希（the murmur hash with linear probing）还是某个变种。 日志将成为一种大众化的接口，可以有多种竞争的算法和实现，以提供最好的保证和最佳的性能。</p><h2 id="变更日志101：表与事件的二象性"><a href="#变更日志101：表与事件的二象性" class="headerlink" title="变更日志101：表与事件的二象性"></a>变更日志101：表与事件的二象性</h2><p>让我们继续聊一下数据库。变更日志和表之间有着迷人的二象性。 日志类似借贷清单和银行处理流水，而数据库表则是当前账户的余额。如果有变更日志，你就可以应用这些变更生成数据表并得到当前状态。 表记录的是每条数据的最后状态（日志的一个特定时间点）。可以认识到日志是更基本的数据结构：日志除了可用来创建原表，也可以用来创建各类衍生表。 （是的，表也可以是非关系型用户用的键值数据存储）</p><img src="/posts/c85ca198/004.jpg"><p>这个过程也是可逆的：如果你对一张表进行更新，你可以记录这些变更，并把所有更新的『变更日志』发布到表的状态信息中。 这些变更日志正是你所需要的支持准实时的复制。 基于此，你就可以清楚的理解表与事件的二象性： 表支持了静态数据，而日志记录了变更。日志的魅力就在于它是变更的<em>完整</em>记录，它不仅仅包含了表的最终版本的内容， 而且可以用于重建任何存在过的其它版本。事实上，日志可以看作是表每个历史状态的一系列备份。</p><p>这可能会让你想到源代码的版本控制。源码控制和数据库之间有着密切的关系。 版本管理解决了一个和分布式数据系统要解决的很类似的问题 —— 管理分布式的并发的状态变更。 版本管理系统建模的是补丁序列，实际上这就是日志。 你可以检出当前的代码的一个『快照』并直接操作，这个代码快照可以类比成表。 你会注意到，正如有状态的分布式系统一样，版本控制系统通过日志来完成复制：更新代码即是拉下补丁并应用到你的当前快照中。</p><p>从销售日志数据库的公司<a href="http://www.datomic.com/" target="_blank" rel="noopener">Datomic</a>那里，大家可以看到一些这样的想法。 <a href="https://www.youtube.com/watch?v=Cym4TZwTCNU" target="_blank" rel="noopener">这个视频</a>比较好地介绍了他们如何在系统中应用这些想法。 当然这些想法不是只专属于这个系统，这十多年他们贡献了很多分布式系统和数据库方面的文献。</p><p>这节的内容可能有点理论化了。但别沮丧！后面马上就是实操的干货。</p><h2 id="接下来的内容"><a href="#接下来的内容" class="headerlink" title="接下来的内容"></a>接下来的内容</h2><p>本文剩下的内容，我会试着重点讲述除了作为分布式计算内部实现或模型抽象日志还有什么优点。包含：</p><ul><li><em>数据集成</em>—— 让组织中所有存储和处理系统可以容易地访问组织所有的数据。</li><li><em>实时数据处理</em> —— 计算生成的数据流。</li><li><em>分布式系统设计</em> —— 如何通过集中式日志的设计来简化实际应用系统。</li></ul><p>所有这些用法都是通过把日志用做单独服务来实现的。</p><p>上面这些场景中，日志的好处都来自日志所能提供的简单功能：生成持久化的可重放的历史记录。 令人意外的是，能让多台机器以确定性的方式按各自的速度重放历史记录的能力是这些问题的核心。</p><h1 id="第二部分：数据集成"><a href="#第二部分：数据集成" class="headerlink" title="第二部分：数据集成"></a>第二部分：数据集成</h1><h2 id="什么是数据集成"><a href="#什么是数据集成" class="headerlink" title="什么是数据集成"></a>什么是数据集成</h2><p>我先解释一下我说的是『数据集成』（data integration）是什么，还有为什么我觉得它很重要，然后我们再来看看它是如何和日志建立关系的。</p><p>数据集成是指使一个组织的所有数据对这个组织的所有的服务和系统可用。</p><p>『数据集成』还不是一个常见的用语，但是我找不到一个更好的。大家更熟知的术语<a href="http://en.wikipedia.org/wiki/Extract,_transform,_load" target="_blank" rel="noopener">ETL</a>通常只是覆盖了数据集成的一个有限子集 —— 主要在关系型数据仓库的场景。但我描述的东西很大程度上可以理解为，将ETL推广至覆盖实时系统和处理流程。</p><img src="/posts/c85ca198/005.jpg"><p>你一定不会听到数据集成就兴趣盎然地屏住呼吸，并且天花乱坠的想到<strong><em>大数据</em></strong>的概念， 但尽管如此，我相信这个陈词滥调的『让数据可用』的问题是组织可以关注的更有价值的事情之一。</p><p>对数据的高效使用遵循一种马斯洛的需要层次理论。 金字塔的基础部分包含捕获所有相关数据，能够将它们全部放到适当的处理环境中（可以是一个华丽的实时查询系统，或仅仅是文本文件和python脚本构成的环境）。 这些数据需要以统一的方式建模，以方便读取和处理。 一旦这些以统一的方式捕获数据的基本需求得到满足，那么在基础设施上以不同方法处理这些数据就变得理所当然 ——MapReduce、实时查询系统等等。</p><p>显而易见但值得注意的一点：如果没有可靠的、完整的数据流，Hadoop集群只不过是个非常昂贵而且安装麻烦的供暖器。 一旦有了数据和处理，人们的关注点就会转移到良好的数据模型和一致且易于理解的语义这些更精致的问题上来。 最后，关注点会转移到更高级处理上 —— 更好的可视化、生成报表以及处理和预测算法。</p><p>以我的经验，大多数组织在这个数据金字塔的底部存在巨大的漏洞 —— 缺乏可靠的完整的数据流 —— 却想直接跳到高级数据模型技术上。这样做完全是本未倒置。</p><p>所以问题是，我们如何在组织中构建贯穿所有数据系统的可靠数据流？</p><h2 id="数据集成：两个难题"><a href="#数据集成：两个难题" class="headerlink" title="数据集成：两个难题"></a>数据集成：两个难题</h2><p>有两个趋势使数据集成变得更加困难。</p><p><em>趋势一：事件数据管道</em></p><p>第一个趋势是增长的事件数据。事件数据记录的是发生的事情，而不是已存在的事情。 在Web系统中，这就意味着用户活动日志，还有为了可靠地操作和监控数据中心机器的价值而记录的机器级别的事件和统计数字。 人们倾向于称它们为『日志数据（log data）』，因为它们经常被写到应用日志中，但这样的说法混淆了形式与功能。 这些数据是现代Web的核心：归根结底，Google的财富来自于建立在点击和展示上的相关性管道，而这些点击和展示正是事件。</p><p>这样的事情并不是仅限于Web公司，只是Web公司已经完全数字化，所以更容易完成。财务数据长久以来一直是以事件为中心的。 <a href="http://en.wikipedia.org/wiki/RFID" target="_blank" rel="noopener">RFID</a>（无线射频识别）使得能对物理设备做这样的跟踪。 随着传统的业务和活动的<a href="http://online.wsj.com/article/SB10001424053111903480904576512250915629460.html" target="_blank" rel="noopener">数字化</a>， 我认为这个趋势仍将继续。</p><p>这种类型的事件数据记录了发生的事情，往往比传统数据库应用要大好几个数量级。这对于处理提出了重大的挑战。</p><p><em>趋势二：专用的数据系统的爆发</em></p><p>第二个趋势来自于专用的数据系统的<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.9136" target="_blank" rel="noopener">爆发</a>，这些数据系统在最近五年开始流行并且可以免费获得。 专门用于<a href="https://github.com/metamx/druid/wiki" target="_blank" rel="noopener">OLAP</a>、<a href="http://www.elasticsearch.org/" target="_blank" rel="noopener">搜索</a>、<a href="http://www.rethinkdb.com/" target="_blank" rel="noopener">简单</a><a href="http://www.slideshare.net/amywtang/espresso-20952131" target="_blank" rel="noopener">在线</a><a href="http://hadoop.apache.org/" target="_blank" rel="noopener">存储</a>、 <a href="http://hadoop.apache.org/" target="_blank" rel="noopener">批处理</a>、<a href="http://graphlab.org/" target="_blank" rel="noopener">图分析（graph analysis）</a><a href="http://redis.io/" target="_blank" rel="noopener">等</a><a href="http://spark.incubator.apache.org/" target="_blank" rel="noopener">等</a> 的数据系统已经出现。</p><p>更加多样化的数据同时变成更加大量，而且这些数据期望放到更多的系统中，这些需求同时要解决，导致了一个巨大的数据集成问题。</p><h2 id="log-structured数据流"><a href="#log-structured数据流" class="headerlink" title="log-structured数据流"></a>log-structured数据流</h2><p>处理系统之间的数据流，日志是最自然的数据结构。解决方法很简单：<strong>提取所有组织的数据，并放到一个用于实时订阅的中心日志中。</strong></p><p>每个逻辑数据源都可以建模为它自己的日志。 一个数据源可以看作一个输出事件日志的应用（如点击或页面的浏览），或是 一个接受修改的数据库表。 每个订阅消息的系统都尽可能快的从日志读取信息，将每条新的记录应用到自己的存储中，同时向前滚动日志文件中的自己的位置。 订阅方可以是任意一种数据系统 —— 缓存、Hadoop、另一个网站中的另一个数据库、一个搜索系统等等。</p><img src="/posts/c85ca198/006.png"><p>举个例子，日志概念为每个变更提供了逻辑时钟，所有的订阅方都可以比较这个逻辑时钟。 这极大简化了如何去推断不同的订阅系统的状态彼此是否一致的，因为每个系统都持有了一个读到哪儿的『时间点』。</p><p>为了让讨论更具体些，我们考虑一个简单的案例，有一个数据库和一组缓存服务器集群。 日志提供了一个方法可以同步更新到所有这些系统，并推断出每个系统的所处在的时间点。 我们假设做了一个写操作，对应日志记录X，然后要从缓存做一次读操作。 如果我们想保证看到的不是过时的数据，我们只需保证，不要去读取那些复制操作还没有跟上X的缓存即可。</p><p>日志也起到缓冲的作用，使数据的生产异步于数据的消费。有许多原因使得这一点很重要，特别是在多个订阅方消费数据的速度各不相同的时候。 这意味着一个数据订阅系统可以宕机或是下线维护，在重新上线后再赶上来：订阅方可以按照自己的节奏来消费数据。 批处理系统如Hadoop或者是一个数据仓库，或许只能每小时或者每天消费一次数据，而实时查询系统可能需要及时到秒。 无论是起始的数据源还是日志都感知感知各种各样的目标数据系统，所以消费方系统的添加和删除无需去改变传输管道。</p><p>特别重要的是：目标系统只知道日志，而不知道来源系统的任何细节。 无论是数据来自于一个RDBMS、一种新型的键值存储，还是由一个不包含任何类型实时查询的系统所生成的，消费方系统都无需关心。 这似乎是一个小问题，但实际上却是至关重要的。</p><img src="/posts/c85ca198/007.jpg"><p><a href="http://en.wikipedia.org/wiki/Anna_Karenina_principle" target="_blank" rel="noopener">『每个工作的数据管道要设计得像是一个日志；每个损坏的数据管道都以其自己的方式损坏。』</a></p><p><a href="http://en.wikipedia.org/wiki/Anna_Karenina_principle" target="_blank" rel="noopener">—— <em>Count Leo Tolstoy</em> ——</a></p><p>这里我使用术语『日志』取代了『消息系统』或者『发布-订阅』，因为在语义上明确得多，并且准确得多地描述了在实际实现中支持数据复制时你所要做的事。 我发现『发布订阅』只是表达出了消息的间接寻址—— 如果你去比较两个发布-订阅的消息系统的话，会发现他们承诺的是完全不同的东西，而且大多数模型在这一领域没什么用。 你可以认为日志是一种有持久性保证和强有序语义的消息系统。 在分布式系统中，这个通信模型有时有个（有些可怕的）名字叫做<a href="http://en.wikipedia.org/wiki/Atomic_broadcast" target="_blank" rel="noopener">原子广播（atomic broadcast）</a>。</p><p>值得强调的是，日志仍然只是基础设施，并不是精通数据流这个故事的结束： 故事的剩余部分围绕着metadata、schemas、兼容性以及处理数据结构及其演化的所有细节来展开。 但是，除非有一种可靠的通用的方式来处理数据流的机制，否则语义细节总是次要的。</p><h2 id="在LinkedIn"><a href="#在LinkedIn" class="headerlink" title="在LinkedIn"></a>在LinkedIn</h2><p>随着LinkedIn从集中式关系数据库过渡到一套分布式系统，我注意到数据集成的问题在迅速地演变。</p><p>目前我们主要的数据系统包括：</p><ul><li><a href="http://data.linkedin.com/projects/search" target="_blank" rel="noopener">搜索</a></li><li><a href="http://engineering.linkedin.com/real-time-distributed-graph/using-set-cover-algorithm-optimize-query-latency-large-scale-distributed" target="_blank" rel="noopener">Social Graph</a></li><li><a href="http://project-voldemort.com/" target="_blank" rel="noopener">Voldemort</a> （键值存储）</li><li><a href="http://data.linkedin.com/projects/espresso" target="_blank" rel="noopener">Espresso</a> （文档存储）</li><li><a href="http://www.quora.com/LinkedIn-Recommendations/How-does-LinkedIns-recommendation-system-work" target="_blank" rel="noopener">推荐引擎</a></li><li>OLAP查询引擎</li><li><a href="http://hadoop.apache.org/" target="_blank" rel="noopener">Hadoop</a></li><li><a href="http://www.teradata.com/" target="_blank" rel="noopener">Terradata</a></li><li><a href="http://engineering.linkedin.com/52/autometrics-self-service-metrics-collection" target="_blank" rel="noopener">Ingraphs</a> （监控图表和指标服务）</li></ul><p>每一个都是专用的分布式系统，在各自的专门领域提供高级的功能。</p><p>使用日志作为数据流的这个想法，甚至在我到这里之前，就已经在LinkedIn的各个地方开始浮现了。 我们开发的最早的一个基础设施是一个称为<a href="https://github.com/linkedin/databus" target="_blank" rel="noopener">databus</a>的服务， 它在我们早期的Oracle表上提供了一种日志缓存的抽象，用于可伸缩地订阅数据库修改，给我们的social graph和搜索索引输入数据。</p><p>我先简单介绍一些历史以提供讨论的上下文。在发布我们自己键值存储之后，大约是2008年我开始参与这个项目。 我接着的一个项目是把一个运行的Hadoop部署用起来，迁移我们的一些推荐处理上来。 由于缺乏这方面的经验，我们只计划了几周时间完成数据的导入导出，剩下的时间则用来实现复杂的预测算法。 就这样我们开始了长途跋涉。</p><p>我们本来计划是仅仅将数据从现存的Oracle数据仓库中剖离。 但是我们首先发现将数据从Oracle中迅速取出简直是一个黑魔法。 更糟的是，数据仓库的处理过程并不适合于我们为Hadoop设计的生产批处理过程 —— 大部分处理都是不可逆的，并且与要生成的具体报表相关。 最终我们采取的办法是，避免使用数据仓库，直接访问源数据库和日志文件。 最后，我们实现了一个管道，用于完成<a href="http://data.linkedin.com/blog/2009/06/building-a-terabyte-scale-data-cycle-at-linkedin-with-hadoop-and-project-voldemort" target="_blank" rel="noopener">加载数据到我们的键值存储</a>并生成结果。</p><p>这种普通常见的数据拷贝最终成为原来开发项目的主要内容之一。 糟糕的是，只要在任何时间任意管道有一个问题，Hadoop系统基本上就是废的 —— 在错误的数据基础上运行复杂的算法只会产生更多的错误数据。</p><p>虽然我们已经使用了一种很通用的构建方式，但是每个数据源都需要自定义的安装配置。这也被证明是大量错误与失败的根源。 我们用Hadoop实现的网站功能开始流行起来，而我们发现自己有一大把需要协作的工程师。 每个用户都有他们想要集成的一大把的系统，并且想要导入的一大把新数据源。</p><p>有些东西在我面前开始渐渐清晰起来。</p><p>首先，我们已建成的通道虽然有一些杂乱，但实际上是极有价值的。 仅在一个新的处理系统（Hadoop）中让数据可用于处理就开启了大量的可能性。 基于这些数据过去很难实现的计算如今已变为可能。 许多新的产品和分析技术都来源于把多个数据片块放在一起，这些数据过去被锁定在特定的系统中。</p><img src="/posts/c85ca198/009.jpg"><p>古希腊时代的ETL。并没有太多变化。</p><p>第二，可靠的数据加载需要数据通道的深度支持，这点已经变得很清晰了。 如果我们可以捕获所有我们需要的结构，就可以使得Hadoop数据全自动地加载， 这样不需要额外的手动操作就可以添加新的数据源或者处理schema变更 —— 数据就会自动的出现在HDFS，并且Hive表就会自动的为新数据源生成恰当的列。</p><p>第三，我们的数据覆盖率仍然很低。 如果看一下LinkedIn所有数据在Hadoop中可用的比率，仍然很不完整。 相比接入并运转一个新数据源所要做的努力，完整接入一个数据源更不容易。</p><p>我们曾经推行的方式是为每个数据源和目标构建自定义的数据加载，很显然这是不可行的。 我们有几十个数据系统和数据仓库。把这些系统和仓库联系起来，就会导致任意两两系统间构建自定义的管道，如下所示：</p><img src="/posts/c85ca198/010.png"><p>需要注意的是数据是双向流动的：例如许多系统（数据库、Hadoop）同时是数据传输的来源和目的。 这就意味着我们我们最后要为每个系统建立两个通道：一个用于数据输入，一个用于数据输出。</p><p>这显然需要一大群人，而且也不具有可操作性。随着我们接近完全连接，最终我们会有差不多O(N^2)条管道。</p><p>要避免上面的问题，我们需要像这样的通用方式：</p><img src="/posts/c85ca198/011.png"><p>我们需要尽可能的将每个消费者与数据源隔离。理想情形下，它们应该只与一个单独的数据源集成，就能访问到所有数据。</p><p>这个思想是增加一个新的数据系统 —— 它可以作为数据来源或者数据目的地 —— 集成工作只需连接这个新系统到一个单独的管道，而无需连接到每个数据消费方。</p><p>这样的经历使得我专注于创建<a href="http://kafka.apache.org/" target="_blank" rel="noopener">Kafka</a>，把我们所知的消息系统的特点与在数据库和分布式系统内核常用的日志概念 结合起来。 我们首先需要一个实体作为所有的活动数据的中心管道，并逐步的扩展到其他很多的使用方式，包括Hadoop之外的数据、数据监控等等。</p><p>在相当长的时间内，Kafka是独一无二的（有人会说是怪异） —— 作为一个底层设施，它既不是数据库，也不是日志文件收集系统，更不是传统的消息系统。 但是最近Amazon提供了非常非常类似Kafka的服务，称之为<a href="http://aws.amazon.com/kinesis" target="_blank" rel="noopener">Kinesis</a>。 相似度包括了分片（partition）处理的方式，数据的持有方式，甚至包括有点特别的Kafka API分类（分成高端和低端消费者）。 我很开心看到这些，这表明了你已经创建了很好的底层设施抽象，AWS已经把它作为服务提供！ 他们对此的想法看起来与我所描述的完全吻合： 管道联通了所有的分布式系统，诸如DynamoDB,RedShift,S3等，同时作为使用EC2进行分布式流处理的基础。</p><h2 id="ETL与数据仓库的关系"><a href="#ETL与数据仓库的关系" class="headerlink" title="ETL与数据仓库的关系"></a>ETL与数据仓库的关系</h2><p>我们再来聊聊数据仓库。数据仓库旨在包含支撑数据分析的规整的集成的结构化数据。 这是一个非常好的理念。对不了解数据仓库概念的人来说，数据仓库的用法是： 周期性的从源数据库抽取数据，把它们转化为可理解的形式，然后把它导入中心数据仓库。 对于数据集中分析和处理，拥有高度集中的位置存放全部数据的规整副本对于数据密集的分析和处理是非常宝贵的资产。 在更高层面上，无论你使用传统的数据仓库Oracle还是Teradata或Hadoop， 这个做法不会有太多变化，可能<a href="http://searchdatamanagement.techtarget.com/definition/Extract-Load-Transform-ELT" target="_blank" rel="noopener">调整</a>一下抽取和加载数据的顺序。</p><p>数据仓库是极其重要的资产，它包含了的和规整的数据，但是实现此目标的机制有点过时了。</p><img src="/posts/c85ca198/012.jpg"><p>对于以数据为中心的组织，关键问题是把规整的集成的数据联结到数据仓库。 数据仓库是个批处理查询基础设施：它们适用于各类报表和临时性分析，特别是当查询包含了简单的计数、聚合和过滤。 但是如果批处理系统是唯一一个包含规整的完整的数据的仓库， 这就意味着，如果一个系统需要实时数据输入的实时系统（如实时处理、实时搜索索引、实时监控等系统），这些数据是不可用的。</p><p>依我之见，ETL包括两件事。 首先，它是数据抽取和清理的处理 —— 本质上就是释放被锁在组织的各类系统中的数据，去除特定于系统的约束。 第二，依照数据仓库的查询重构数据，例如使其符合关系数据库类型系统， 强制使用star schema、snowflake schema，可能会打散数据成高性能的<a href="http://parquet.io/" target="_blank" rel="noopener">列</a><a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/ds_Hive/orcfile.html" target="_blank" rel="noopener">格式</a>（column format）等等。同时做好这两件事是有困难的。 这些集成仓库的规整的数据除了要索引到实时存储系统中，也应当可用于实时或是低时延处理中。</p><p>在我看来，正是因为这个原因有了额外好处：使得数据仓库ETL大大提升了组织级的可伸缩性（scalable）。 典型的问题是数据仓库团队要负责收集和整理组织中各个团队所生成的全部数据。 两边的收益是不对称的：数据的生产者常常并不知晓在数据仓库中数据的使用情况， 结果产生的数据，为了转成为可用的形式，抽取过程很难或是很繁重，转换过程很难统一规模化。 当然，中心团队的规模不可能跟上组织中其它团队增长， 结果数据的覆盖率总是参差不齐的，数据流是脆弱的，跟进变更是缓慢的。</p><p>较好的做法是有一个中央管道即日志，用定义良好的API来添加数据。 集成这个管道和提供良好的结构化的输入数据所需的职责由提供数据的生产者承担。 这意味着作为系统设计和实现一部分的生产者，在交付到中心通道时， 必须考虑其输出和输入的数据要有良好结构形式的问题。 新的存储系统的加入对于数据仓库团队是无关紧要的，因为他们现在只有一个中心结点去集成。 （<strong><em>译注</em></strong>：原来要集成的是其它各个相关的系统，工作是被简化了的） 数据仓库团队需只处理更简单的问题，从中心日志中加载结构化的输入数据、完成特定于他们系统的数据转换。</p><img src="/posts/c85ca198/013.png"><p>从上面讨论可以看出，当考虑采纳传统的数据仓库之外额外的数据系统时，组织级的伸缩性显得尤为重要。 例如，想为组织的所有的数据集提供搜索能力或者想为数据流的监控的次级监控（sub-second monitoring）添加实时数据趋势和告警。 无论是哪个情况，传统的数据仓库的基础设施，甚至是Hadoop集群都将不再适合。 更糟的是，用于支持数据加载的ETL处理管道可能输入不了数据到其它系统， 和带动不了要动用数据仓库这样的大企业下的那些基础设备。 这样的做法应该是不可行的，可能可以解释为什么多数组织对他们的所有数据很难轻松具备这样的能力。 反之，如果组织能导出标准的结构良好的数据， 那么任何新的系统要使用所有数据仅仅需要提供一个用于集成的管道接到中央管道上即可。</p><p>关于数据规整化和转换在哪里进行，这种架构也引出了的不同观点：</p><ul><li>在添加数据到公司全局日志之前，由数据的生产者完成。</li><li>由在日志上的一个实时转换器完成，转换器生成一个新的转换过的日志。</li><li>作为加载过程的一部分，由目标系统完成。</li></ul><p>最好的模型是数据发布到日志之前由数据生产者完成数据规整化。 这样可以确保数据是处于规范形式的， 并且不需要保留数据从原来生产系统的特定代码或是原来存储系统的维护方式所带来的任何遗留属性。 这些细节最好由产成数据的团队来处理，因为他们最了解他们自己的数据。 这个阶段所使用的任何逻辑都应该是无损的和可逆的。</p><p>可以实时完成的任何类型有附加值的转换操作都应该作为原始日志数据的后处理环节完成。 这类操作包括了事件数据的会话管理，或者附加上大家感兴趣的派生字段。 原始日志仍然是可用的，但这样的实时处理生产了包含增强数据的派生日志。</p><p>最后，只有针对目标系统的聚合操作才应该加到加载过程中。 比如可能包括在数据仓库中为分析和报表而做的把数据转化成特定的星型或者雪花状schema。 因为在这个阶段（一般比较自然地对应到传统的ETL处理阶段），现在处理的是一组规整得多和统一得多的流， 处理过程已经大大简化了。</p><h2 id="日志文件与事件"><a href="#日志文件与事件" class="headerlink" title="日志文件与事件"></a>日志文件与事件</h2><p>我们再来聊聊这种架构的附带的优势：支持解耦的事件驱动的系统。</p><p>在Web行业取得活动数据的典型方法是写日志到文本文件中， 然后这些文本文件分解进入数据仓库或者Hadoop用于聚合和查询。 这做的问题和所有批处理的ETL做法一样：数据流耦合了数据仓库系统的能力和处理计划。</p><p>在LinkedIn，是以在中心日志完成处理的方式构建事件数据。 Kafka做为中心的有多个订阅方的事件日志，定义数百种事件类型， 每种类型都会捕获一个特定动作类型的独特属性。 这样的方式覆盖从页面浏览、广告展示、搜索到服务调用、应用异常的方方面面。</p><p>为了进一步理解这一优势，设想一个简单的场景 —— 显示在工作职位页面提交的职位信息。 职位页面应当只包括显示职位所需的逻辑。 然而，在足够动态站点中，这很容易就变成与职位显示无关的额外逻辑的点缀。 例如，我们将对如下的系统进行集成：</p><ul><li>发送数据到Hadoop和数据仓库中，以做离线数据处理</li><li>浏览计数，确保查看者不是一个内容爬虫</li><li>聚合浏览信息，在职位提交者的分析页面显示</li><li>记录浏览信息，确保合适地设置了用户的推荐职位的展示上限（不想重复地展示同样内容给用户）</li><li>推荐系统可能需要记录浏览，以正确的跟踪职位的流行程度</li><li>等等</li></ul><p>用不了多久，简单的职位显示变得相当的复杂。 与此同时，还要增加职位显示的其它终端 —— 移动终端应用等等 —— 这样的逻辑必须继续实现，复杂程度被不断地提升。 更糟的是，我们需要交互的系统是多方需求交织缠绕在一起的 —— 负责显示职位的工程师需要知道多个其它系统和功能，才可以确保集成的正确。 这里仅是简单描述了问题，实际应用系统只会更加复杂。</p><p>『事件驱动』风格提供了简化这类问题的方案。 职位显示页面现在只负责显示职位并记录显示职位的信息，如职位相关属性、页面浏览者及其它有价值的信息。 其它所有关心这个信息的系统诸如推荐系统、安全系统、职位提交分析系统和数据仓库，只需订阅上面的输出数据进行各自的处理。 显示代码并不需要关注其它的系统，也不需要因为增加了数据的消费者而做改变。</p><h2 id="构建可伸缩的日志"><a href="#构建可伸缩的日志" class="headerlink" title="构建可伸缩的日志"></a>构建可伸缩的日志</h2><p>当然，把发布者与订阅者分离不再是什么新鲜事了。 但是如果要给一个需要按用户扩展的（consumer-scale）网站提供多个订阅者的实时提交日志， 那么可伸缩性就会成为你所面临的首要挑战。 如果我们不能创建快速、低成本和可伸缩的日志以满足实际大规模的使用，把日志用作统一集成机制只不过是个美好的幻想。</p><p>人们普遍认为分布式日志是缓慢的、重量级的抽象（并且通常只把它与『元数据』类的使用方式联系在一起，可能用Zookeeper才合适）。 但有了一个专注于大数据流的深思熟虑的实现可以打破上面的想法。 在LinkedIn，目前每天通过Kafka写入超过600亿条不同的消息。 （如果算上<a href="http://kafka.apache.org/documentation.html#datacenters" target="_blank" rel="noopener">数据中心之间镜像</a>的消息，那么这个数字会是数千亿。）</p><p>为了支持这样的规模，我们在Kafka中使用了一些技巧：</p><ul><li>日志分片</li><li>通过批量读出和写入来优化吞吐量</li><li>规避无用的数据拷贝</li></ul><p>为了确保水平可扩展性，我们把日志进行切片：</p><img src="/posts/c85ca198/014.png"><p>每个分片的日志是有序的，但是分片之间没有全局的次序（这个有别于在你的消息中可能包含的挂钟时间）。 由写入者决定消息发送到特定的日志分片，大部分使用者以某种键值（如用户id）来进行分片。 追加日志时，分片方式在片段之间可以不需要协调，并且可以使系统的吞吐量与Kafka集群大小线性增长。</p><p>每个分片通过可配置数字指定数据复制的副本个数，每个副本都有一个分片日志完全一致的一份拷贝。 任何时候都有一个副本作为leader，如果leader出错了，会有一个副本接替成为leader。</p><p>缺少跨分片的全局顺序是个局限，但是我们没有发现它成为大问题。 事实上，与日志的交互一般来源于成百上千个不同的处理流程，所以为所有处理提供全局顺序没有意义。 转而需要的是，我们提供的每个分片有序的保证，和Kafka提供的同一发送者发送给同一分区的消息以相同的顺序交付到接收者的保证。</p><p>日志，和文件系统一样，对于顺序读写可以方便地优化。日志可以把小的读写合成大的高吞吐量的操作。 Kafka非常积极做这方面的优化。客户端向服务器端的数据发送、磁盘写入、服务器之间复制、到消费者数据传递和数据提交确认都会做批处理。</p><p>最后，Kafka使用简单的二进制格式维护内存日志、磁盘日志和传送网络数据。这使得我们可以使用包括『<a href="https://www.ibm.com/developerworks/library/j-zerocopy" target="_blank" rel="noopener">0拷贝的数据传输</a>』在内的大量的优化机制。</p><p>这些优化的积累效应是往往以磁盘和网络的速度在读写数据，即使维护的数据集大大超出内存大小。</p><p>这些自卖自夸的介绍不意味着是关于Kafka的主要内容，我就不再深入细节了。 LinkedIn方案的更细节说明在<a href="http://sites.computer.org/debull/A12june/pipeline.pdf" target="_blank" rel="noopener">这儿</a>，Kafka设计的详细说明在<a href="http://kafka.apache.org/documentation.html#design" target="_blank" rel="noopener">这儿</a>，你可以读一下。</p><h1 id="第三部分：日志与实时流处理"><a href="#第三部分：日志与实时流处理" class="headerlink" title="第三部分：日志与实时流处理"></a>第三部分：日志与实时流处理</h1><p>到目前为止，我只讲述了系统之间拷贝数据的理想机制。但是在存储系统之间搬运字节不是所要讲述内容的全部。 最终会发现，『日志』是流的另一种说法， 并且日志是<a href="http://highlyscalable.wordpress.com/2013/08/20/in-stream-big-data-processing/" target="_blank" rel="noopener">流处理</a>的核心。</p><h2 id="什么是流处理"><a href="#什么是流处理" class="headerlink" title="什么是流处理"></a>什么是流处理</h2><p>但是，等会儿，流处理到底是什么呢？</p><p>如果你是上世纪90年代晚期或者21世纪初<a href="http://cs.brown.edu/research/aurora/vldb03_journal.pdf" target="_blank" rel="noopener">数据库</a><a href="http://db.cs.berkeley.edu/papers/cidr03-tcq.pdf" target="_blank" rel="noopener">文化</a>或者成功了一半的<a href="http://www-03.ibm.com/software/products/us/en/infosphere-streams" target="_blank" rel="noopener">数据</a><a href="http://en.wikipedia.org/wiki/StreamBase_Systems" target="_blank" rel="noopener">基础设施</a><a href="http://en.wikipedia.org/wiki/Truviso" target="_blank" rel="noopener">产品</a>的爱好者，那么你就可能会把流处理与建创SQL引擎或者『箱子和箭头』（boxes and arrows）接口用于事件驱动的处理联系起来。</p><p>如果你关注大量出现的开源数据库系统，你就可能把流处理和一些这领域的系统关联起来， 比如<a href="http://storm-project.net/" target="_blank" rel="noopener">Storm</a>、<a href="http://akka.io/" target="_blank" rel="noopener">Akka</a>、<a href="http://incubator.apache.org/s4" target="_blank" rel="noopener">S4</a>和<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying" target="_blank" rel="noopener">Samza</a>。 但是大部分人会把这些系统看为异步消息处理系统，与支持群集的远程过程调用层没什么差别 （而事实上这一领域一些系统确实是如此）。</p><p>这些观点都有一些局限性。流处理即与SQL无关，也不局限于实时流处理。 还没有根本的原因限制你不能使用多种不同的语言来表达计算，处理昨天的或者一个月之前的流数据。</p><img src="/posts/c85ca198/015.jpg"><p>我把流处理视为更广泛的概念：持续数据流处理的基础设施。 我认为计算模型可以像MapReduce或者分布式处理框架一样通用，但是有能力生成低时延的结果。</p><p>处理模型的真正驱动力是数据收集方法。成批收集数据自然是批处理。当数据是持续收集的，自然也应该持续不断地处理。</p><p>美国的统计调查是一个成批收集数据的经典例子。 统计调查周期性的开展，用的是蛮力调查，通过挨门挨户的走访统计美国公民的信息。 在1790年统计调查刚刚开始，这样做是很合理的。 那时的数据收集本质就是面向批处理的，包括了骑马到周边人家，用纸笔记录，然后把成批的记录运输到人们统计数据的中心站点。 现在，在描述这个统计过程时，人们立即会想到为什么我们不保留出生和死亡的记录，这样就可以算出人口统计信息，这些信息或是持续即时计算出来或者按需要时间隔计算。</p><p>这是一个极端的例子，但是现在大量的数据传输处理仍然依赖于周期性的转录和批量的传输和集成。 处理批量转录数据的唯一方法就是批量的处理。 但是随着这些批处理被持续的数据输入所取代，人们自然而然的开始向持续处理转变，以平滑地使用所需的处理资源并且减少延迟。</p><p>例如在LinkedIn几乎完全没有批量数据收集。我们大部分的数据要么是活动数据或者要么是数据库变更，两者都是不间断地发生的。 事实上，你想到的任何商业业务，底层的机制几乎都是不间断的处理，正如<em>Jack Bauer</em>所说的，事件的发生是实时的。 当数据以成批的方式收集，几乎总是由这些原因所致：有一些人为的步骤；缺少数字化；或是非数字化流程的历史古董不能自动化。 当使用邮件或者人工方式，传输和处理数据是非常缓慢的。刚开始转成自动化时，总是保持着原来流程的形式，所以这样的情况会持续相当长的时间。</p><p>每天运行的『批量』处理作业常常在模拟一种窗口大小是一天的持续计算。 当然，底层的数据其实总是在变化着的。 在LinkedIn，这样的做法如此之常见（并且在Hadoop做到这些的实现机制如此之复杂）， 以至于我们实现了一整套<a href="http://engineering.linkedin.com/datafu/datafus-hourglass-incremental-data-processing-hadoop" target="_blank" rel="noopener">框架</a>来管理增量的Hadoop工作流。</p><p>由此看来，对于流处理我很容易得出不同观点： 它处理的是包含时间概念的底层数据并且不需要静态的数据快照， 所以可以以用户可控频率生产输出而不是等待数据集都到达后再生产输出（译注：数据是会持续的，所以实际上不会有都达到的时间点）。 从这个角度上讲，流处理是广义上的批处理，随着实时数据的流行，流处理会是很重要处理方式。</p><p>那么，为什么流处理的传统观点大家之前会认为更合适呢？ 我个人认为最大的原因是缺少实时数据收集，使得持续处理之前是学术性的概念。</p><p>我觉得，是否缺少实时数据的收集决定了商用流处理系统的命运。 当他们的客户还是用面向文件的每日批量处理完成ETL和数据集成时， 建设流处理系统的公司专注于提供处理引擎来连接实时数据流，而结果是当时几乎没有人真地有实时数据流。 其实我在LinkedIn工作的初期，有一家公司想把一个非常棒的流处理系统卖给我们， 但是因为当时我们的所有数据都按小时收集在的文件里， 所以用上这个系统我们能做到的最好效果就是在每小时的最后把这些文件输入到流处理系统中。 他们意识到这是个普遍问题。 下面的这个异常案例实际上是证明上面规律： 流处理获得一些成功的一个领域 —— 金融领域，这个领域在过去，实时数据流就已经标准化，并且流处理已经成为了瓶颈。</p><p>甚至于在一个健康的批处理的生态中，我认为作为一种基础设施风格，流处理的实际应用能力是相当广阔的。 我认为它填补了实时数据请求/响应服务和离线批量处理之间的缺口。现代的互联网公司，我觉得大约25%的代码可以划分到这个情况。</p><p>事实证明，日志解决了流处理中最关键的一些技术问题，后面我会进一步讲述， 但解决的最大的问题是日志使得多个订阅者可以获得实时的数据输入。 对技术细节感兴趣的朋友，我们已经开源了<a href="http://samza.apache.org/" target="_blank" rel="noopener">Samza</a>， 它正是基于这些理念建设的一个流处理系统。 很多这方面的应用的更多技术细节我们在<a href="http://samza.apache.org/learn/documentation/latest/" target="_blank" rel="noopener">此文档</a>中有详细的描述。</p><h2 id="数据流图"><a href="#数据流图" class="headerlink" title="数据流图"></a>数据流图</h2><img src="/posts/c85ca198/016.png"><p>流处理最有趣的特点是它与流处理系统的内部组织无关， 但是与之密切相关的是，流处理是怎么扩展了之前在数据集成讨论中提到的认识：输入数据（data feed）是什么。 我们主要讨论了原始数据（primary data）的feeds 或说日志——各种系统执行所产生的事件和数据行。 但是流处理允许我们包括了由其它feeds计算出的feeds。 在消费者看来，这些派生的feeds和 用于生成他们的原始数据的feeds 看下来没什么差别。 这些派生的feeds可以按任意的复杂方式封装组合。</p><p>让我们再深入一点这个问题。 对于我们的目标，流处理作业是指从日志读取数据和将输出写入到日志或其它系统的任何系统。 用于输入和输出的日志把这些处理系统连接成一个处理阶段的图。 事实上，以这样的风格使用中心化的日志，你可以把组织全部的数据抓取、转化和工作流仅仅看成是一系列的写入它们的日志和处理过程。</p><p>流处理器根本不需要高大上的框架： 可以是读写日志的一个处理或者一组处理过程，但是为了便于管理处理所用的代码，可以提供一些额外的基础设施和支持。</p><p>在集成中日志的目标是双重的：</p><p>首先，日志让各个数据集可以有多个订阅者并使之有序。 让我们回顾一下『状态复制』原理来记住顺序的重要性。 为了更具体地说明，设想一下从数据库中更新数据流 —— 如果在处理过程中把对同一记录的两次更新重新排序，可能会产生错误的输出。 这里的有序的持久性要强于TCP之类所提供的有序，因为不局限于单一的点对点链接，并且在流程处理失败和重连时仍然要保持有序。</p><p>其次，日志提供了处理流程的缓冲。 这是非常基础重要的。如果多个处理之间是非同步的，那么生成上行流数据的作业生成数据可能比另一个下行流数据作业所能消费的更快。 这种情况下，要么使处理进程阻塞，要么引入缓冲区，要么丢弃数据。 丢弃数据似乎不是个好的选择，而阻塞处理进程，会使得整个的数据流的图被迫中止处理。 日志是一个非常非常大的缓冲，允许处理进程的重启或是失败，而不影响流处理图中的其它部分的处理速度。 要扩展数据流到一个更庞大的组织，这种隔离性极其重要，整个处理是由组织中不同的团队提供的处理作业完成的。 不能因为某个作业发生错误导致影响前面作业，结果整个处理流程都被卡住。</p><p><a href="http://storm-project.net/" target="_blank" rel="noopener">Storm</a>和<a href="http://samza.apache.org/" target="_blank" rel="noopener">Sama</a>都是按这种风格构建，能用kafka或其它类似的系统作为它们的日志。</p><h2 id="有状态的实时流处理"><a href="#有状态的实时流处理" class="headerlink" title="有状态的实时流处理"></a>有状态的实时流处理</h2><p>一些实时流处理做的只是无状态的单次记录的转换，但有很多使用方式需要在流处理的某个大小的时间窗口内进行更复杂的计数、聚合和关联（join）操作。 比如，给一个的事件流（如用户点击的流）附加上做点击操作用户的信息—— 实际上即是关联点击流到用户的账户数据库。 这类流程最终总是要处理者维护一些状态信息： 比如在计算一个计数时，需要维护到目前为止的计数器。 在处理者可能挂掉的情况下，如何维护正确的状态？</p><p>最简单的方案是把状态保存在内存中。但是如果处理流程崩溃，会丢失中间状态。 如果状态是按窗口维护的，处理流程只能会回退到日志中窗口开始的时间点上。 但是，如果计数的时间窗口是1个小时这么长，那么这种方式可能不可行。</p><p>另一个方案是简单地存储所有的状态到远程的存储系统，通过网络与这些存储关联起来。 但问题是没了数据的局部性并产生很多的网络间数据往返（network round-trip）。</p><p>如何才能即支持像处理流程一样分片又支持像『表』一样的存储呢？</p><p>回顾一下关于表和日志二象性的讨论。它正好提供了把流转换成与这里我们处理中所需的表的工具，同时也是一个解决表的容错的处理机制。</p><p>流处理器可以把它的状态保存在本地的『表』或『索引』中 —— <a href="http://www.oracle.com/technetwork/products/berkeleydb" target="_blank" rel="noopener">bdb</a>、<a href="https://code.google.com/p/leveldb" target="_blank" rel="noopener">leveldb</a> 甚至是些更不常见的组件，如<a href="http://lucene.apache.org/" target="_blank" rel="noopener">Lucene</a>或<a href="https://sdm.lbl.gov/fastbit" target="_blank" rel="noopener">fastbit</a>索引。 这样一些存储的内容可以从它的输入流生成（可能做过了各种转换后的输入流）。 通过记录关于本地索引的变更日志，在发生崩溃重启时也可以恢复它的状态。 这是个通用的机制，用于保持任意索引类型的分片之间相互协作（co-partitioned）的本地状态与输入流数据一致。</p><p>当处理流程失败时，可以从变更日志中恢复它的索引。 每次备份时，即是日志把本地状态转化成一种增量记录。</p><p>这种状态管理方案的优雅之处在于处理器的状态也是做为日志来维护。 我们可以把这个日志看成是数据库表变更的日志。 事实上，这些处理器本身就很像是自维护的分片之间相互协作的表。 因为这些状态本身就是日志，所以其它处理器可以订阅它。 如果处理流程的目标是更新结点的最后状态并且这个状态又是流程的一个自然的输出，那么这种方式就显得尤为重要。</p><p>再组合使用上用于解决数据集成的数据库输出日志，日志和表的二象性的威力就更加明显了。 从数据库中抽取出来的变更日志可以按不同的形式索引到各种流处理器中，以关联到事件流上。</p><p>在Samza和这些大量<a href="http://samza.apache.org/learn/documentation/0.7.0/container/state-management.html" target="_blank" rel="noopener">实际例子</a>中， 我们说明了这种风格的有状态流处理管理的更多细节。</p><h2 id="日志合并（log-compaction）"><a href="#日志合并（log-compaction）" class="headerlink" title="日志合并（log compaction）"></a>日志合并（log compaction）</h2><img src="/posts/c85ca198/017.png"><p>当然，我们不能奢望一直保存着全部变更的完整日志。 除非想要使用无限空间，日志总是要清理。 为了让讨论更具体些，我会介绍一些Kafka这方面的实现。 在Kafka中，清理有两种方式，取决于数据包括的是键值存储的更新还是事件数据。 对于事件数据，Kafka支持仅维护一个窗口的数据。通常，窗口配置成几天，但窗口也可以按空间大小来定。 对于键值存储的更新，尽管完整日志的一个优点是可以回放以重建源系统的状态（一般是另一个系统中重建）。</p><p>但是，随着时间的推移，保持完整的日志会使用越来越多的空间，并且回放的耗时也会越来越长。 因此在Kafka中，我们支持不同类型的保留方式。 我们删除过时的记录（如这些记录的主键最近更新过）而不是简单的丢弃旧日志。 这样做我们仍然保证日志包含了源系统的完整备份，但是现在我们不再重现原系统曾经的所有状态，仅是最近的哪些状态。 这一功能我们称之为<a href="https://cwiki.apache.org/confluence/display/KAFKA/Log+Compaction" target="_blank" rel="noopener">日志合并</a>。</p><h1 id="第四部分：系统构建"><a href="#第四部分：系统构建" class="headerlink" title="第四部分：系统构建"></a>第四部分：系统构建</h1><p>最后我要讨论的是在线数据系统设计中日志的角色。</p><p>日志服务在分布式数据库中服务于数据流可以类比日志服务在大型组织机构中服务于数据集成。 在这两个应用场景中，日志要解决的问题都是数据流、一致性和可恢复性。 如果组织不是一个很复杂的分布式数据系统呢，它究竟是什么？</p><h2 id="分解单品方式而不是打包套餐方式？"><a href="#分解单品方式而不是打包套餐方式？" class="headerlink" title="分解单品方式而不是打包套餐方式？"></a>分解单品方式而不是打包套餐方式？</h2><p>如果你换个角度，可以把组织的系统和数据流整体看做整个一个分布式数据： 把所有独立的面向查询的系统（如Redis、SOLR、Hive等等）看做只是你的数据的特定的索引； 把流处理系统（如Storm、Samza）看做只是一种很成熟的触发器和视图的具体机制。 我注意到，传统的数据库人员非常喜欢这样的观点，因为他们终于能解释通，这些不同的数据系统到底是做什么用的 —— 它们只是不同的索引类型而已！</p><p>不可否认这段时间涌现了大量类型的数据系统，但实际上，这方面的复杂性早就存在。 即使是在关系数据库的鼎盛时期，组织里就有种类繁多的关系数据库！ 因为大型机，所有的数据都存储在相同的位置，所以可能并没有真正的数据集成。 有很多推动力要把数据分离到多个系统：数据伸缩性、地理地域、安全性和性能隔离是最常见的。 这些问题可以通过一个好的系统来解决： 比如组织使用单个Hadoop保存所有数据来服务大量各式各样的客户，这样做是可能的。</p><p>所以处理的数据向分布式系统变迁的过程中，已经有了个可能的简单方法： 把大量的不同系统的小实例合到少数的大集群中。</p><p>许多的系统还不足好到支持这个方法：它们没有安全，或者不能保证性能隔离性，或者伸缩性不够好。 不过这些问题都是可以解决的。</p><p>依我之见，不同系统大量涌现的原因是构建分布式数据系统的难度。 把关注点消减到单个查询类型或是用例，各个系统可以把关注范围控制到一组能构建出来的东西上。 但是把全部这些系统运行起来，这件事有非常多的复杂性。</p><p>我觉得解决这类问题将来有三个可能的方向：</p><p>第一种可能性是延续现状，各个分离的系统在往后很长的一段时间里基本保持不变。 发生这种可能要么是因为建设分布式系统的困难很难克服， 要么系统的specialization能让各个系统的convenience和power达到一个新的高度。 只要现状不变，为了能够使用数据，数据集成问题将仍会最核心事情之一。 如果是这样，用于集成数据的外部日志将会非常的重要。</p><p>第二种可能性是一个统一合并的系统，这个系统具备足够的通用性，逐步把所有不同的功能合并成单个超极系统。 这个超级系统表面看起来类似关系数据库，但在组织中使用方式会非常不一样，因为只能用一个大系统而不是无数个小系统。 在这样的世界里，除了系统自身的内部，不存在真正的数据集成问题。 我觉得，因为建设这样的系统的实际困难，使这个情况不太可能发生。</p><p>还有另一种可能的结果，呃，其实我觉得这个结果对工程师很有吸引力。 新一代数据系统的一个让人感兴趣的特征是，这个系统几乎是完全开源的。 开源提供了另一个可能性：数据基础架构不用是打包套餐式的而是分解单品成一组服务及面向应用的API。 在Java栈中，你可以看到这种状况在一定程度上已经发生了：</p><ul><li><a href="http://zookeeper.apache.org/" target="_blank" rel="noopener">Zookeeper</a>处理系统之间的协调的很多问题。 （或许诸如<a href="http://helix.incubator.apache.org/" target="_blank" rel="noopener">Helix</a> 或<a href="http://curator.incubator.apache.org/" target="_blank" rel="noopener">Curator</a>等高级别抽象可以有些帮助）。</li><li><a href="http://mesos.apache.org/" target="_blank" rel="noopener">Mesos</a>和<a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">YARN</a> 处理虚拟化（virtualization）和资源管理。</li><li><a href="http://lucene.apache.org/" target="_blank" rel="noopener">Lucene</a>和<a href="https://code.google.com/p/leveldb" target="_blank" rel="noopener">LevelDB</a>等嵌入式类库做为索引。</li><li><a href="http://netty.io/" target="_blank" rel="noopener">Netty</a>、<a href="http://www.eclipse.org/jetty" target="_blank" rel="noopener">Jetty</a> 和 更高层封装如<a href="http://twitter.github.io/finagle" target="_blank" rel="noopener">Finagle</a>、<a href="http://rest.li/" target="_blank" rel="noopener">rest.li</a>处理远程通信。</li><li><a href="http://avro.apache.org/" target="_blank" rel="noopener">Avro</a>、<a href="https://code.google.com/p/protobuf" target="_blank" rel="noopener">Protocol Buffers</a>、<a href="http://thrift.apache.org/" target="_blank" rel="noopener">Thrift</a>和<a href="https://github.com/eishay/jvm-serializers/wiki" target="_blank" rel="noopener">umpteen zillion</a>等其它类库处理序列化。</li><li><a href="http://kafka.apache.org/" target="_blank" rel="noopener">Kafka</a>和<a href="http://zookeeper.apache.org/bookkeeper" target="_blank" rel="noopener">Bookeeper</a>提供后端支持的日志。</li></ul><p>如果你把上面这些叠成一堆，换个角度去看，它会有点像是乐高版的分布式数据系统工程。 你可以把这些零件拼装在一起，创建大量的可能的系统。 显然，上面说的不是面向主要关心API及API实现的最终用户， 但在一个更多样化和模块化且持续演变的世界中，这可能一条途径可以通往简洁的单个系统。 因为随着可靠的、灵活的构建模块的出现，实现分布式系统的时间由年缩减为周，聚合形成大型整体系统的压力将会消失。</p><h2 id="日志在系统架构中的地位"><a href="#日志在系统架构中的地位" class="headerlink" title="日志在系统架构中的地位"></a>日志在系统架构中的地位</h2><p>提供外部日志的系统允许各个系统抛弃很多各自的复杂性，依靠共享的日志。在我看来，日志可以做到以下事情：</p><ul><li>通过对节点的并发更新的排序处理，处理了数据一致性（无论即时的还是最终的一致）</li><li>提供节点之间的数据复制</li><li>为写入者提供『提交（commit）』语义（仅当写入数据确保不会丢失时才会收到完成确认（acknowledge））</li><li>为系统提供外部的数据订阅</li><li>对于丢失数据的失败了的副本，提供恢复或是启动一个新副本的能力</li><li>调整节点间的数据平衡</li></ul><p>这就是一个数据分布式系统所要做的主要部分，实际上，剩下的大部分内容是最终用户要面对的查询API和索引策略相关的。 这正是不同系统间的应该变化的部分，例如：一个全文搜索查询语句可能需要查询所有分区， 而一个主键查询只需要查询负责这个主键数据的单个节点就可以了。</p><img src="/posts/c85ca198/018.png"><p>下面我们来看下系统是如何工作的。 系统被分为两个逻辑部分：日志和服务层。日志按顺序捕获状态变化。 服务节点存储索引提供查询服务需要的所有信息（比如键值存储的索引可能会类似BTree或SSTable，搜索系统可能用的是倒排索引（inverted index））。 写入操作可以直接进入日志，尽管可能经过服务层的代理。 在写入日志的时候会生成逻辑时间戳（称为日志中的索引），如果系统是分区的，我也假定是分区的， 那么日志和服务节点会包含相同分区个数，尽管两者的机器台数可能相差很多。</p><p>服务节点订阅日志，并按照日志存储的顺序尽快把日志写到它的本地索引中。</p><p>客户端只要在查询语句中提供某次写入操作的时间戳，就可以有从任何节点『读到该次写入』的语义（read-your-write semantics）—— 服务节点收到该查询语句后，会将其中的时间戳与自身索引的位置比较， 如果必要，服务节点会延迟请求直到它的索引至少已经跟上那个时间戳，以避免提供的是旧数据。</p><p>服务节点可能会或可能不会需要感知master身份或是当选leader。 对很多简单的使用场景，服务节点集群可以完全无需leader，因为日志是正确真实的信息源。</p><p>分布式系统所需要处理的一件比较复杂的事是恢复失败节点和在结点之间移动分片（partition）。 典型的做法是仅保留一个固定窗口的数据，并把这个数据和分片中存储数据的一个快照关联。 另一个相同效果的做法是，让日志保留数据的完整拷贝，并<a href="https://cwiki.apache.org/confluence/display/KAFKA/Log+Compaction" target="_blank" rel="noopener">对日志做垃圾回收</a>。 这把大量的复杂性从特定于系统的系统服务层移到了通用的日志中。</p><p>有了这个日志系统，你得到一个成熟完整的订阅API，这个API可以订阅数据存储的内容，驱动到其它系统的ETL操作。 事实上，许多系统都可以共享相同的日志以提供不同的索引，如下所示：</p><img src="/posts/c85ca198/019.png"><p>注意，这样的以日志为中心的系统是如何做到本身即是在其它系统中要处理和加载的数据流的提供者的呢？ 同样，流处理器既可以消费多个输入流，然后通过这个流处理器输出把这些输入流的数据索引到其它系统中。</p><p>我觉得把系统分解成日志和查询API的观点很有启迪性，因为使得查询相关的因素与系统的可用性和一致性方面解耦。 我其实觉得这更是个好用的思路，可以对于没按这种方式构建的系统做概念上的分解。</p><p>值得一提的是，尽管Kafka和Bookeeper都是一致性日志，但并不是必须的。 你可以轻松把<a href="http://www.allthingsdistributed.com/2007/10/amazons_dynamo.html" target="_blank" rel="noopener">Dynamo</a>之类的数据库作为你的系统的 最终一致的<a href="http://en.wikipedia.org/wiki/CAP_theorem" target="_blank" rel="noopener">AP</a>日志和键值对服务层。 这样的日志使用起来很灵活，因为它会重传了旧消息并依赖订阅者的信息处理（很像Dynamo所做的）。</p><p>很多人认为在日志中维护数据的单独拷贝（特别是做全量数据拷贝）太浪费。 然而事实上，有几个因素可以让这个不成为问题。 首先，日志可以是一种特别高效的存储机制。在我们Kafka生产环境的服务器上，每个数据中心都存储了超过75TB的数据。 同时其它的许多服务系统需要的是多得多的内存来提供高效的服务（例如文本搜索，它通常是全在内存里）。 其次，服务系统会用优化过的硬件。例如，我们的在线数据系统或者基于内存提供服务或者使用固态硬盘。 相反，日志系统只需要线性读写，因此很合适用TB级的大硬盘。 最后，如上图所示，多个系统使用日志数据提供服务，日志的成本是分摊到多个索引上。 上面几点合起来使得外部日志的开销相当小。</p><p>LinkedIn正是使用这个模式构建了它很多的实时查询系统。 这些系统的数据来自数据库（使用作为日志概念的数据总线，或是来自Kafka的真正日志），提供了在这个数据流上特定的分片、索引和查询能力。 这也是我们实现搜索、social graph和OLAP查询系统的方式。 事实上，把单个数据源（无论来自Hadoop的在线数据源还是派生数据源）复制到多个在线服务系统中，这个做法很常见。 这种方式经过了验证可以大大简化系统的设计。 系统根本不需要给外部提供写入API，Kafka和数据库通过日志给查询系统提供记录和变更流。 各个分片的结点在本地完成写操作。 这些结点只要机械地把日志中的数据转录到自己的存储中。失败的结点通过回放上游的日志就可以恢复。</p><p>系统的强度取决于日志的使用方式。一个完全可靠的系统把日志用作数据分片、结点的存储、负载均衡，以及所有和数据一致性和数据传播（propagation）有关的方面。 在这样的架构中，服务层实际上只不过是一种『缓存』，可以通过直接写日志就能完成某种处理。</p><h1 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h1><p>如果你从头一直做读到了这，那么我对日志的理解你大部分都知道了。</p><p>这里再给一些有意思参考资料，你可以再去看看。</p><p>人们会用不同的术语描述同一事物，当你从数据库系统到分布式系统、从各类企业级应用软件到广阔的开源世界查看资料时， 这会让人有一些困惑。无论如何，在大方向上还是有一些共同之处。</p><p>学术论文、系统、讨论和博客：</p><ul><li>关于<a href="http://www.cs.cornell.edu/fbs/publications/smsurvey.pdf‎" target="_blank" rel="noopener">状态机</a>和<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.5896" target="_blank" rel="noopener">主备</a>复制的概述。</li><li><a href="http://research.microsoft.com/apps/pubs/default.aspx?id=66814" target="_blank" rel="noopener">PacificA</a>是实施微软基于日志的分布式存储系统的通用架构。</li><li><a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/spanner-osdi2012.pdf" target="_blank" rel="noopener">Spanner</a> —— 并不是每个人都支持把逻辑时间用于他们的日志，Google最新的数据库就尝试使用物理时间，并通过把时间戳直接做为区间来直接建时钟迁移的不确定性。</li><li><a href="http://www.datomic.com/" target="_blank" rel="noopener">Datanomic</a>：<a href="https://www.youtube.com/watch?v=Cym4TZwTCNU" target="_blank" rel="noopener">解构数据库</a>是<em>Rich Hickey</em>（Clojure的创建者）在它的首个数据库产品中的重要陈述之一。</li><li><a href="http://www.cs.utexas.edu/~lorenzo/papers/SurveyFinal.pdf" target="_blank" rel="noopener">在消息传递系统中回滚恢复协议的调查</a>。 我发现这是有关容错处理和通过日志在数据库之外完成恢复的实际应用的很不错的介绍。</li><li><a href="http://www.reactivemanifesto.org/" target="_blank" rel="noopener">反应式宣言（Reactive Manifesto）</a> —— 我其实并不清楚反应式编程（reactive programming）的确切涵义，但是我想它和『事件驱动』指的是同一件事。 这个链接并没有太多的讯息，但<em>Martin Odersky</em>（Scala名家）讲授的<a href="https://www.coursera.org/course/reactive" target="_blank" rel="noopener">这个课程</a>是很有吸引力的。</li><li>Paxos!<ul><li>原论文在<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/lamport-paxos.pdf" target="_blank" rel="noopener">这里</a>。 <em>Leslie Lamport</em> 有一个有趣的历史：在80年代算法是如何发现的，但是直到1998年才发表了，因为评审组不喜欢论文中的希腊寓言，而作者又不愿修改。</li><li>甚至于论文发布以后，它还是不被人们理解。<em>Lamport</em> 再次尝试，这次它包含了一些并不有趣的小细节，这些细节是关于如何使用这些新式的自动化的计算机的。 它仍然没有得到广泛的认可。</li><li>Fred Schneider<em>](<a href="http://www.cs.cornell.edu/fbs/publications/SMSurvey.pdf)和[" target="_blank" rel="noopener">http://www.cs.cornell.edu/fbs/publications/SMSurvey.pdf)和[</a></em>Butler Lampson*](<a href="http://research.microsoft.com/en-us/um/people/blampson/58-consensus/Abstract.html)分别给出了更多细节关于在实时系统中如何应用Paxos。" target="_blank" rel="noopener">http://research.microsoft.com/en-us/um/people/blampson/58-consensus/Abstract.html)分别给出了更多细节关于在实时系统中如何应用Paxos。</a></li><li>一些Google的工程师总结了他们在Chubby中实施Paxos的<a href="http://www.cs.utexas.edu/users/lorenzo/corsi/cs380d/papers/paper2-1.pdf" target="_blank" rel="noopener">经验</a>。</li><li>我发现所有关于Paxos的论文理解起来很痛苦，但是值得我们费大力气弄懂。你不必忍受这样的痛苦了，因为日志结构的文件系统的大师<a href="http://www.stanford.edu/~ouster/cgi-bin/papers/lfs.pdf" target="_blank" rel="noopener"><em>John Ousterhout</em></a>的<a href="https://www.youtube.com/watch?v=JEpsBg0AO6o" target="_blank" rel="noopener">这个视频</a> 让这一切变得相当的容易。这些一致性算法用展开的通信图表述的更好，而不是在论文中通过静态的描述来说明。颇为讽刺的是，这个视频录制的初衷是告诉人们Paxos很难理解。</li><li><a href="http://arxiv.org/pdf/1103.2408.pdf" target="_blank" rel="noopener">使用Paxos来构造规模一致的数据存储</a>。这是一篇很棒的介绍使用日志来构造数据存储的文章，<em>Jun</em> 是文章的共同作者之一，他也是Kafka最早期的工程师之一。</li></ul></li><li>Paxos有很多的竞争者。如下诸项可以更进一步的映射到日志的实施，更适合于实用性的实施。<ul><li>由<em>Barbara Liskov</em>提出的<a href="http://pmg.csail.mit.edu/papers/vr-revisited.pdf" target="_blank" rel="noopener">视图戳复现</a>是直接进行日志复现建模的较早的算法。</li><li><a href="http://www.stanford.edu/class/cs347/reading/zab.pdf" target="_blank" rel="noopener">Zab</a>是Zookeeper所使用的算法。</li><li><a href="https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf" target="_blank" rel="noopener">RAFT</a>是易于理解的一致性算法之一。由<em>John Ousterhout</em>讲授的这个<a href="https://www.youtube.com/watch?v=YbZ3zDzDnrw" target="_blank" rel="noopener">视频</a>非常的棒。</li></ul></li><li>你可以的看到在不同的实时分布式数据库中动作日志角色：<ul><li><a href="https://www.youtube.com/watch?v=YbZ3zDzDnrw" target="_blank" rel="noopener">PNUTS</a>是探索在大规模的传统的分布式数据库系统中实施以日志为中心设计理念的系统。</li><li><a href="http://hbase.apache.org/" target="_blank" rel="noopener">Hbase</a>和Bigtable都是在目前的数据库系统中使用日志的样例。</li><li>LinkedIn自己的分布式数据库<a href="http://www.slideshare.net/amywtang/espresso-20952131" target="_blank" rel="noopener">Espresso</a>和PNUTs一样，使用日志来复现，但有一个小的差异是它使用自己底层的表做为日志的来源。</li></ul></li><li>如果你正在做一致性算法选型，<a href="http://arxiv.org/abs/1309.5671" target="_blank" rel="noopener">这篇论文</a>会对你所有帮助。</li><li><a href="http://www.amazon.com/Replication-Practice-Lecture-Computer-Theoretical/dp/3642112935" target="_blank" rel="noopener">复现：理论与实践</a>，这是收录了分布式系统一致性的大量论文的一本巨著。网上有该书的诸多章节（<a href="http://disi.unitn.it/~montreso/ds/papers/replication.pdf" target="_blank" rel="noopener">1</a>，<a href="http://research.microsoft.com/en-us/people/aguilera/stumbling-chapter.pdf" target="_blank" rel="noopener">4</a>，<a href="http://www.distributed-systems.net/papers/2010.verita.pdf" target="_blank" rel="noopener">5</a>，<a href="http://www.cs.cornell.edu/ken/history.pdf" target="_blank" rel="noopener">6</a>，<a href="http://www.pmg.csail.mit.edu/papers/vr-to-bft.pdf" target="_blank" rel="noopener">7</a>，<a href="http://engineering.linkedin.com/distributed-systems/www.cs.cornell.edu/fbs/publications/TrustSurveyTR.pdf" target="_blank" rel="noopener">8</a>）。</li><li>流处理：这个话题要总结的内容过于宽泛，但还是有几件我所关注的要提一下：<ul><li><a href="http://infolab.usc.edu/csci599/Fall2002/paper/DML2_streams-issues.pdf" target="_blank" rel="noopener">在数据流系统中建模和相关事件</a>：它可能是研究这一领域的最佳概述之一。</li><li><a href="http://cs.brown.edu/research/aurora/hwang.icde05.ha.pdf" target="_blank" rel="noopener">分布处式流处理的高可用性算法</a>。</li><li>随机系统的一些论文：<ul><li><a href="http://db.cs.berkeley.edu/papers/cidr03-tcq.pdf" target="_blank" rel="noopener">TelegraphCQ</a></li><li><a href="http://cs.brown.edu/research/aurora/vldb03_journal.pdf" target="_blank" rel="noopener">Aurora</a></li><li><a href="http://research.cs.wisc.edu/niagara/papers/NiagaraCQ.pdf" target="_blank" rel="noopener">NiagaraCQ</a></li><li><a href="http://www.cs.berkeley.edu/~matei/papers/2012/hotcloud_spark_streaming.pdf" target="_blank" rel="noopener">Discretized Streams</a>: This paper discusses Spark’s streaming system.</li><li><a href="http://research.google.com/pubs/pub41378.html" target="_blank" rel="noopener">MillWheel</a> is one of Google’s stream processing systems.</li><li><a href="http://research.microsoft.com/apps/pubs/?id=201100" target="_blank" rel="noopener">Naiad: A Timely Dataflow System</a></li></ul></li></ul></li></ul><p>企业级软件存在着同样的问题，只是名称不同，或者规模较小，或者是XML格式的。哈哈，开个玩笑。</p><ul><li><a href="http://cs.brown.edu/research/aurora/hwang.icde05.ha.pdf" target="_blank" rel="noopener">事件驱动</a> —— 据我所知：它就是企业级应用的工程师们常说的『状态机的复现』。有趣的是同样的理念会用在如此迥异的场景中。事件驱动关注的是小的、内存中的使用场景。 这种机制在应用开发中看起来是把发生在日志事件中的『流处理』和应用关联起来。因此变得不那么琐碎： 当处理的规模大到需要数据分片时，我关注的是流处理作为独立的首要的基础设施。</li><li><a href="http://en.wikipedia.org/wiki/Change_data_capture" target="_blank" rel="noopener">变更数据捕获</a> —— 在数据库之外会有些对于数据的舍入处理，这些处理绝大多数都是日志友好的数据扩展。</li><li><a href="http://en.wikipedia.org/wiki/Enterprise_application_integration" target="_blank" rel="noopener">企业级应用集成</a>，当你有一些现成的类似客户类系管理CRM和供应链管理SCM的软件时，它似乎可以解决数据集成的问题。</li><li><a href="http://en.wikipedia.org/wiki/Complex_event_processing" target="_blank" rel="noopener">复杂事件处理（CEP）</a>没有人知道它的确切涵义或者它与流处理有什么不同。这些差异看起来集中在无序流和事件过滤、发现或者聚合上，但是依我之见，差别并不明显。我认为每个系统都有自己的优势。</li><li><a href="http://en.wikipedia.org/wiki/Enterprise_service_bus" target="_blank" rel="noopener">企业服务总线（ESB）</a> —— 我认为企业服务总线的概念类似于我所描述的数据集成。在企业级软件社区中这个理念取得了一定程度的成功，对于从事网络和分布式基础架构的工程师们这个概念还是很陌生的。</li></ul><p>一些相关的开源软件：</p><ul><li><a href="http://kafka.apache.org/" target="_blank" rel="noopener">Kafka</a>是把日志作为服务的一个项目，它是后边所列各项的基础。</li><li><a href="http://zookeeper.apache.org/bookkeeper/" target="_blank" rel="noopener">Bookeeper</a> 和<a href="http://zookeeper.apache.org/bookkeeper/" target="_blank" rel="noopener">Hedwig</a> 另外的两个开源的『把日志作为服务』的项目。它们更关注的是数据库系统内部构件而不是事件数据。</li><li><a href="https://github.com/linkedin/databus" target="_blank" rel="noopener">Databus</a>是提供类似日志的数据库表的覆盖层的系统。</li><li><a href="http://akka.io/" target="_blank" rel="noopener">Akka</a> 是用于Scala的Actor框架。它有一个<a href="https://github.com/eligosource/eventsourced" target="_blank" rel="noopener">事件驱动</a>的插件，它提供持久化和记录。</li><li><a href="http://storm-project.net/" target="_blank" rel="noopener">Samza</a>是我们在LinkedIn中用到的流处理框架，它用到了本文论述的诸多理念，同时与Kafka集成来作为底层的日志。</li><li><a href="http://storm-project.net/" target="_blank" rel="noopener">Storm</a>是广泛使用的可以很好的与Kafka集成的流处理框架之一。</li><li><a href="http://spark.incubator.apache.org/docs/0.7.3/streaming-programming-guide.html" target="_blank" rel="noopener">Spark Streaming</a>一个流处理框架，它是<a href="http://spark.incubator.apache.org/" target="_blank" rel="noopener">Spark</a>的一部分。</li><li><a href="https://blog.twitter.com/2013/streaming-mapreduce-with-summingbird" target="_blank" rel="noopener">Summingbird</a>是在Storm或Hadoop之上的一层，它提供了便洁的计算摘要。</li></ul><p>对于这一领域，我将持续的关注，如何您知道一些我遗漏的内容，请您告知。</p><p>最后一起来听听这首歌放松一下吧：</p><p><a href="https://youtu.be/2C7mNr5WMjA" target="_blank" rel="noopener">The Log Song - Ren &amp; Stimpy (Deadwood HoN)</a></p>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
            <tag> 日志 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
