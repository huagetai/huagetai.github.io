<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.1"><link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222"><link rel="stylesheet" href="/css/main.css?v=7.4.1"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.4.1",exturl:!1,sidebar:{position:"right",display:"post",offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!0,color:"#222",save:"manual"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><meta name="description" content="我们在使用 Apache Kafka 生产和消费消息的时候，肯定是希望能够将数据均匀地分配到所有服务器上。比如很多公司使用 Kafka 收集应用服务器的日志数据，这种数据都是很多的，特别是对于那种大批量机器组成的集群环境，每分钟产生的日志量都能以 GB 数，因此如何将这么大的数据量均匀地分配到 Kafka 的各个 Broker 上，就成为一个非常重要的问题。那么 Kafka 生产者如何实现这个需求"><meta name="keywords" content="kafka,kafka生产者,分区策略"><meta property="og:type" content="article"><meta property="og:title" content="kafka生产者消息分区机制"><meta property="og:url" content="https:&#x2F;&#x2F;huagetai.github.io&#x2F;posts&#x2F;fabbb24d&#x2F;index.html"><meta property="og:site_name" content="有翼"><meta property="og:description" content="我们在使用 Apache Kafka 生产和消费消息的时候，肯定是希望能够将数据均匀地分配到所有服务器上。比如很多公司使用 Kafka 收集应用服务器的日志数据，这种数据都是很多的，特别是对于那种大批量机器组成的集群环境，每分钟产生的日志量都能以 GB 数，因此如何将这么大的数据量均匀地分配到 Kafka 的各个 Broker 上，就成为一个非常重要的问题。那么 Kafka 生产者如何实现这个需求"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;huagetai.github.io&#x2F;posts&#x2F;fabbb24d&#x2F;001.jpeg"><meta property="og:image" content="https:&#x2F;&#x2F;huagetai.github.io&#x2F;posts&#x2F;fabbb24d&#x2F;002.png"><meta property="og:image" content="https:&#x2F;&#x2F;huagetai.github.io&#x2F;posts&#x2F;fabbb24d&#x2F;003.png"><meta property="og:image" content="https:&#x2F;&#x2F;huagetai.github.io&#x2F;posts&#x2F;fabbb24d&#x2F;004.png"><meta property="og:image" content="https:&#x2F;&#x2F;huagetai.github.io&#x2F;posts&#x2F;fabbb24d&#x2F;005.png"><meta property="og:image" content="https:&#x2F;&#x2F;huagetai.github.io&#x2F;posts&#x2F;fabbb24d&#x2F;006.jpeg"><meta property="og:image" content="https:&#x2F;&#x2F;huagetai.github.io&#x2F;posts&#x2F;fabbb24d&#x2F;007.jpeg"><meta property="og:image" content="https:&#x2F;&#x2F;huagetai.github.io&#x2F;posts&#x2F;fabbb24d&#x2F;008.jpeg"><meta property="og:image" content="https:&#x2F;&#x2F;huagetai.github.io&#x2F;posts&#x2F;fabbb24d&#x2F;009.jpeg"><meta property="og:updated_time" content="2019-10-19T13:27:21.923Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;huagetai.github.io&#x2F;posts&#x2F;fabbb24d&#x2F;001.jpeg"><link rel="canonical" href="https://huagetai.github.io/posts/fabbb24d/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>kafka生产者消息分区机制 | 有翼</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">有翼</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">心若有翼，我自飞翔</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a></li><li class="menu-item menu-item-guestbook"><a href="/guestbook" rel="section"><i class="fa fa-fw fa-comment"></i>留言</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://huagetai.github.io/posts/fabbb24d/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="worry"><meta itemprop="description" content="认真做事，踏实做人"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="有翼"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">kafka生产者消息分区机制</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-10-09 18:10:46" itemprop="dateCreated datePublished" datetime="2019-10-09T18:10:46+08:00">2019-10-09</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-10-19 21:27:21" itemprop="dateModified" datetime="2019-10-19T21:27:21+08:00">2019-10-19</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span> </a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/fabbb24d/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/fabbb24d/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>6.9k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>17 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>我们在使用 Apache Kafka 生产和消费消息的时候，肯定是希望能够将数据均匀地分配到所有服务器上。比如很多公司使用 Kafka 收集应用服务器的日志数据，这种数据都是很多的，特别是对于那种大批量机器组成的集群环境，每分钟产生的日志量都能以 GB 数，因此如何将这么大的数据量均匀地分配到 Kafka 的各个 Broker 上，就成为一个非常重要的问题。那么 Kafka 生产者如何实现这个需求呢。</p><h1 id="为什么分区（partition）？"><a href="#为什么分区（partition）？" class="headerlink" title="为什么分区（partition）？"></a>为什么分区（partition）？</h1><p>对于每一个主题（topic）， kafka集群都会维持一个分区日志，如下所示：</p><img src="/posts/fabbb24d/001.jpeg"><p>每个分区都是有序且顺序不可变的消息记录集，消息被不断地追加到commit log文件末尾。主题下的每条消息只会保存在某一个分区中。kafka为什么要分区呢？分区主要有以下几个原因：</p><p>第一，当日志大小超过了单台服务器的限制，允许日志进行扩展。每个单独的分区都必须受限于主机的文件限制，不过一个主题可能有多个分区，因此可以处理无限量的数据。</p><p>第二，提供负载均衡的能力。不同的分区分布在Kafka集群不同的节点服务器上。每个服务器在处理数据和请求时，共享这些分区。每一个分区都会在已配置的服务器上进行备份，确保容错性。每个分区都有一台 server 作为leader，零台或者多台server作为follwers 。leader server 处理一切对分区的读写请求，而follwers只需被动的同步leader上的数据。当leader宕机了，followers 中的一台服务器会自动成为新的 leader。每台 server 都会成为某些分区的 leader 和某些分区的 follower，因此集群的负载是平衡的。并且，我们还可以通过添加新的节点机器来增加整体系统的吞吐量。</p><p>第三，除了提供负载均衡这种最核心的功能之外，利用分区也可以实现其他一些业务级别的需求，例如，如果使用用户ID作为key，则用户相关的所有数据都会被分发到同一个分区上。 这允许消费者在消费数据时做一些特定的本地化处理。</p><h1 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h1><img src="/posts/fabbb24d/002.png"><p>所谓分区策略是决定生产者将消息发送到哪个分区的算法。Kafka 为我们提供了默认的分区策略，同时它也支持你自定义分区策略。</p><h2 id="Partitioner接口"><a href="#Partitioner接口" class="headerlink" title="Partitioner接口"></a>Partitioner接口</h2><p>这个接口很简单，只定义了两个方法:partition()和close()，通常你只需要实现最重要的 partition 方法。我们来看看这个方法的方法签名：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span></span>;</span><br></pre></td></tr></table></figure><p>这里的topic、key、keyBytes、value和valueBytes都属于消息数据，cluster则是集群信息（可以获得一个主题的所有分区信息列表和available分区信息列表等等）。我们能够充分地利用这些信息对消息进行分区，计算出它要被发送到哪个分区中。只要你自己的实现类定义好了 partition 方法，同时设partitioner.class参数为你自己实现类的 Full Qualified Name，那么生产者程序就会按照你的代码逻辑对消息进行分区。</p><h2 id="随机策略"><a href="#随机策略" class="headerlink" title="随机策略"></a>随机策略</h2><p>也称 Randomness 策略。所谓随机就是我们随意地将消息放置到任意一个分区上，如下面这张图所示。</p><img src="/posts/fabbb24d/003.png"><p>如果要实现随机策略版的 partition 方法，很简单，只需要两行代码即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line"><span class="keyword">return</span> ThreadLocalRandom.current().nextInt(partitions.size());`</span><br></pre></td></tr></table></figure><p>先计算出该主题总的分区数，然后随机地返回一个小于它的正整数。</p><p>本质上看随机策略是力求将数据均匀地打散到各个分区，但从实际表现来看，它要逊于下面介绍的轮询策略，目前已经弃用。</p><h2 id="轮询策略"><a href="#轮询策略" class="headerlink" title="轮询策略"></a>轮询策略</h2><p>RoundRobinPartitioner，也称 Round-robin 策略，即顺序分配。比如一个主题下有 3 个分区，那么第一条消息被发送到分区 0，第二条被发送到分区 1，第三条被发送到分区 2，以此类推。当生产第 4 条消息时又会重新开始，即将其分配到分区 0，就像下面这张图展示的那样。</p><img src="/posts/fabbb24d/004.png"><p>这就是所谓的轮询策略。轮询策略有非常优秀的负载均衡表现，它总是能保证消息最大限度地被平均分配到所有分区上。但是在吞吐量较小的情况下，也会导致更多批次，而这些批次的大小更小，其实，将所有记录转到指定的分区（或几个分区）并以更大的批记录一起发送似乎会更好。在key=null时，轮询策略是2.3版本默认使用的分区策略，2.4版本（KIP-480）引入了更好的黏性分区策略。</p><h2 id="黏性分区策略"><a href="#黏性分区策略" class="headerlink" title="黏性分区策略"></a>黏性分区策略</h2><p>UniformStickyPartitioner，黏性分区策略。</p><p>消息记录会成批的从生产者发送到Broker。生产者触发发送请求的时机由批记录的大小参数和linger.ms参数决定，批记录的大小达到设定的值或linger.ms参数时间到，都会触发批记录的发送。因此，批记录的大小对生产者到Broker发送延迟是有影响的。较小的批记录会导致更多的请求和排队从而导致更高的延迟。这意味着，在关闭linger.ms的情况下（即将linger.ms参数设置为零），较大的批记录也会减少延迟。在启用linger.ms的情况下，低吞吐量通常会向系统中注入延迟，因为如果没有足够的记录来填充批记录，则需要等到linger.ms设定的参数值才会发送该批记录。在linger.ms值之前找到增加批记录大小以触发发送的方法将进一步减少延迟。</p><p>黏性分区策略通过“黏贴”到分区直到批记录已满（或在linger.ms启动时发送），与轮询策略相比，我们可以创建更大的批记录并减少系统中的延迟。即使在linger.ms为零立即发送的情况下，也可以看到改进的批处理和减少的延迟。在创建新批处理时更改粘性分区，随着时间的流逝，记录应该在所有分区之间是平均分配的。2.4版本key=null时默认使用黏性分区策略</p><h2 id="按消息key分区策略"><a href="#按消息key分区策略" class="headerlink" title="按消息key分区策略"></a>按消息key分区策略</h2><p>Kafka 允许为每条消息定义消息键，简称为 Key。这个 Key 的作用非常大，它可以是一个有着明确业务含义的字符串，比如客户代码、部门编号或是业务 ID 等；也可以用来表征消息元数据。特别是在 Kafka 不支持时间戳的年代，在一些场景中，工程师们都是直接将消息创建时间封装进 Key 里面的。一旦消息被定义了 Key，那么你就可以保证同一个 Key 的所有消息都进入到相同的分区里面，如下图所示。</p><img src="/posts/fabbb24d/005.png"><h2 id="自定义分区策略"><a href="#自定义分区策略" class="headerlink" title="自定义分区策略"></a>自定义分区策略</h2><p>一种比较常见的，即所谓的基于地理位置的分区策略。当然这种策略一般只针对那些大规模的 Kafka 集群，特别是跨城市、跨国家甚至是跨大洲的集群。</p><p>我就拿“美团app”举个例子吧，假设美团app的所有服务都部署在北京的一个机房（这里我假设它是自建机房，不考虑公有云方案。其实即使是公有云，实现逻辑也差不多），现在美团app考虑在南方找个城市（比如广州）再创建一个机房；另外从两个机房中选取一部分机器共同组成一个大的 Kafka 集群。显然，这个集群中必然有一部分机器在北京，另外一部分机器在广州。</p><p>假设美团app计划为每个新注册用户提供一份注册礼品，比如南方的用户注册极客时间可以免费得到一碗“甜豆腐脑”，而北方的新注册用户可以得到一碗“咸豆腐脑”。如果用 Kafka 来实现则很简单，只需要创建一个双分区的主题，然后再创建两个消费者程序分别处理南北方注册用户逻辑即可。</p><p>但问题是你需要把南北方注册用户的注册消息正确地发送到位于南北方的不同机房中，因为处理这些消息的消费者程序只可能在某一个机房中启动着。换句话说，送甜豆腐脑的消费者程序只在广州机房启动着，而送咸豆腐脑的程序只在北京的机房中，如果你向广州机房中的 Broker 发送北方注册用户的消息，那么这个用户将无法得到礼品！</p><p>此时我们就可以根据 Broker 所在的 IP 地址实现定制化的分区策略。比如下面这段代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); </span><br><span class="line"><span class="keyword">return</span> partitions.stream().filter(p -&gt; isSouth(p.leader().host())).map(PartitionInfo::partition).findAny().get();</span><br></pre></td></tr></table></figure><p>我们可以从所有分区中找出那些 Leader 副本在南方的所有分区，然后随机挑选一个进行消息发送。</p><h2 id="DefaultPartitioner默认分区策略"><a href="#DefaultPartitioner默认分区策略" class="headerlink" title="DefaultPartitioner默认分区策略"></a>DefaultPartitioner默认分区策略</h2><p>key不为null时，对key进行hash（基于murmurHash2算法），根据最终得到的hash值计算分区号，有相同key的消息会被写入同样的分区；key为null时，2.3版本使用轮询分区策略，2.4版本使用黏性分区策略。</p><h1 id="生产者分区逻辑"><a href="#生产者分区逻辑" class="headerlink" title="生产者分区逻辑"></a>生产者分区逻辑</h1><p>指定partition的情况下，直接发往该分区；没有指定partition的情况下，根据分区策略确定发往的分区。配置了自定义分区策略的情况下使用自定义的分区策略，没有配置的情况下使用默认分区策略DefaultPartitioner。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>分区是实现负载均衡以及高吞吐量的关键，故在生产者这一端就要仔细盘算合适的分区策略，避免造成消息数据的“倾斜”，使得某些分区成为性能瓶颈，这样极易引发下游数据消费的性能下降。</p><ul><li><p>Kafka的消息组织方式实际上是三级结构：主题-分区-消息。主题下的每个消息只会保存在某一个分区中</p></li><li><p>为什么要分区</p></li><li><ul><li>无限存储</li><li>负载均衡</li><li>特定业务保序需求</li></ul></li><li><p>分区策略：是决定生产者将消息发送到那个分区的算法</p></li><li><ul><li>随机分区策略-randomness</li><li>轮询分区策略-round-robin</li><li>黏性分区策略-sticky partitioner</li><li>按消息键分区策略</li><li>自定义，比如：基于地理位置的分区策略</li><li>默认分区策略DefaultPartitioner：key不为null时，对key进行hash（基于murmurHash2算法），根据最终得到的hash值计算分区号，有相同key的消息会被写入同样的分区；key为null时，2.3版本使用轮询分区策略，2.4版本使用黏性分区策略。</li></ul></li><li><p>生产者分区逻辑：指定partition的情况下，直接发往该分区；没有指定partition的情况下，根据分区策略确定发往的分区。配置了自定义分区策略的情况下使用自定义的分区策略，没有配置的情况下使用默认分区策略DefaultPartitioner。</p></li></ul><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="KIP-480：黏性分区策略（Sticky-Partitioner）"><a href="#KIP-480：黏性分区策略（Sticky-Partitioner）" class="headerlink" title="KIP-480：黏性分区策略（Sticky Partitioner）"></a>KIP-480：黏性分区策略（Sticky Partitioner）</h2><p>原文：<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-480%3A+Sticky+Partitioner" target="_blank" rel="noopener">KIP480:Sticky Partitioner</a></p><p><strong>动机</strong></p><p>消息记录会成批的从生产者发送到Broker。生产者触发发送批记录（record batches）请求时机由批记录的大小参数和linger.ms参数决定，批记录的大小达到设定的值或linger.ms参数时间到，都会触发批记录的发送。因此，批记录的大小对生产者到Broker发送延迟是有影响的。较小的批记录会导致更多的请求和排队从而导致更高的延迟。这意味着，在关闭linger.ms的情况下（即将linger.ms参数设置为零），较大的批记录也会减少延迟。在启用linger.ms的情况下，低吞吐量通常会向系统中注入延迟，因为如果没有足够的记录来填充批记录，则需要等到linger.ms设定的参数值才会发送该批记录。在linger.ms值之前找到增加批记录大小以触发发送的方法将进一步减少延迟。</p><p>当前（2.3版本），在未指定分区（partition）且未指定key的情况下，默认分区器（DefaultPartitioner）将以循环方式对记录进行分区。这意味着一系列连续记录中的每个记录将被顺序发送到每一个分区，直到我们用尽分区再重新开始。尽管这会在各个分区之间平均分配记录，但也会导致更多批次，而这些批次的大小更小。将所有记录转到指定的分区（或几个分区）并以更大的批记录一起发送似乎会更好。</p><p>黏性分区器尝试在分区器中创建类似行为。通过“黏贴”到分区直到批记录已满（或在linger.ms启动时发送），与默认分区器相比，我们可以创建更大的批记录并减少系统中的延迟。即使在linger.ms为零我们立即发送的情况下，我们也可以看到改进的批处理和减少的延迟。发送批记录后，黏性分区会更改。随着时间的流逝，记录应该在所有分区之间是平均分配的。</p><p>Netflix有一个类似的想法，并创建了一个黏性分区器，该分区器选择一个分区并在给定的时间段内将所有记录发送到该分区，然后再切换到新分区。</p><p>另一种方法是在创建新批处理时更改粘性分区。这样做的目的是最大程度地减少可能在不合时宜的分区交换时机上参生更多的空批记录。我们介绍的黏性分区器将使用这个方法。</p><p><strong>Partitioner接口</strong></p><p>粘性分区将是默认分区器的一部分，因此不会直接有公共接口。<br>Partiton接口增加一个新的Public方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> *Executes right before a new batch will be created. </span><br><span class="line"> *For example, if a sticky partitioner is used, </span><br><span class="line"> *this method can change the chosen sticky partition for the new batch. </span><br><span class="line"> *@param topic The topic name </span><br><span class="line"> *@param cluster The current cluster metadata </span><br><span class="line"> *@param prevPartition The partition of the batch that was just completed </span><br><span class="line"> */</span><br><span class="line"> default public void onNewBatch(String topic, Cluster cluster, int prevPartition) &#123;&#125;</span><br></pre></td></tr></table></figure><p>onNewBatch方法将在创建新批记录之前立即执行代码。 粘性分区程序将定义此方法来更新粘性分区。 这包括更改粘性分区，即使键值上将有新批次也是如此。 测试结果表明，在键值情况下，此更改不会显著影响延迟。</p><p>此方法的默认设置将不会更改当前用户定义的其他分区器的分区行为。 如果用户想在自己的分区器中实现粘性分区，则可以重写此方法。</p><p><strong>拟议的变更</strong></p><ul><li>在无显式分区（key = null）的情况下更改默认分区器的行为。 选择给定Topic的“粘性”分区。 当记录累加器为给定分区上的主题分配新批次时，“粘性”分区将更改。</li><li>这些更改也会略微修改具有键的记录的代码路径，但是这些更改不会显着影响延迟。</li><li>将创建一个名为UniformStickyPartitioner的新分区器，以允许对所有记录进行粘性分区，即使是那些具有非空键的记录也是如此。 这将镜像到RoundRobinPartitioner如何对所有记录（包括具有键的记录）使用循环分区策略。</li></ul><p><strong>兼容性，弃用和迁移计划</strong></p><ul><li>无需兼容处理，无需弃用，无需迁移计划。</li><li>用户可以继续使用自己的分区器-如果要实现粘性分区程序，可以使用onNewBatch方法来实现功能，如果他们不想使用该功能，则行为是相同的。</li><li>默认分区器在key=null，未设置partition值时，用户应看到延迟和CPU使用率是相同的或有减少的</li></ul><p><strong>测试结果</strong></p><p>通常，与当前代码相比，粘性分区器通常会使延迟减少一半。 在最坏的情况下，也能达到默认代码标准。</p><p>随着分区的增加，看到更多的好处。 尽管如此，使用16个分区，仍然可以看到明显的好处。 在1000 msg / sec的吞吐量下，延迟仍然约为默认的一半。</p><img src="/posts/fabbb24d/006.jpeg"><img src="/posts/fabbb24d/007.jpeg"><p>观察到的另一个趋势，尤其是在刷新情况下，随着发送的消息数量从低吞吐量增加到中吞吐量，等待时间减少更多。好处部分取决于每秒消息与分区的比率。</p><p>最后，在linger.ms不为零且吞吐量低到足以让默认代码需要在linger.ms上等待的情况下，显然有好处。例如，以1个生产者，16个分区和1000 msg / sec以及linger.ms = 1000运行时，粘性分区器的p99延迟为204，而默认值为1017。这大约是等待时间的1/3，这是因为批处理不必等待linger.ms。</p><p>除了延迟之外，与默认代码相比，粘性分区程序还发现CPU利用率下降。在观察到的情况下，与默认分区相比，粘性分区的节点通常会降低多达5-15％的CPU使用率（例如，从9-17％降低到5-12.5％或从30-40％降低到15-25％）代码的节点。</p><img src="/posts/fabbb24d/008.jpeg"><img src="/posts/fabbb24d/009.jpeg"><p>（Vnl是默认情况，而Chc是粘性分区程序。这是1个生产者，16个分区，10,000 msg / sec无刷新情况的结果。）</p><p><strong>拒绝选择</strong></p><ul><li><p>拒绝选择可配置的粘性分区器的原因：测试表明，粘性分区程序在cpu利用率和延迟方面均达到或优于默认分区。 将粘性分区程序设置为可配置功能意味着某些用户可能会错过此有益功能</p></li><li><p>拒绝选择基于时间变化粘性分区的原因：变更时间将根据吞吐量而有所不同，需要针对不同的情况进行设置，吞吐量可能不一致</p></li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/kafka/" rel="tag"><i class="fa fa-tag"></i> kafka</a> <a href="/tags/kafka%E7%94%9F%E4%BA%A7%E8%80%85/" rel="tag"><i class="fa fa-tag"></i> kafka生产者</a> <a href="/tags/%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5/" rel="tag"><i class="fa fa-tag"></i> 分区策略</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/9bcd2d2c/" rel="next" title="kafka设计思想"><i class="fa fa-chevron-left"></i> kafka设计思想</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/posts/fcfde8ff/" rel="prev" title="MurmurHash算法">MurmurHash算法 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="tabs tabs-comment"><ul class="nav-tabs"><li class="tab"><a href="#comment-valine">valine</a></li><li class="tab"><a href="#comment-gitalk">gitalk</a></li></ul><div class="tab-content"><div class="tab-pane valine" id="comment-valine"><div class="comments" id="comments"></div></div><div class="tab-pane gitalk" id="comment-gitalk"><div class="comments" id="gitalk-container"></div></div></div></div><script>window.addEventListener('tabs:register', () => {
          let activeClass = 'valine';
            activeClass = localStorage.getItem('comments_active') || activeClass;
          if (activeClass) {
            let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
            if (activeTab) {
              activeTab.click();
            }
          }
        });
        window.addEventListener('tabs:click', event => {
          let commentClass = event.target.classList[1];
          localStorage.setItem('comments_active', commentClass);
        });</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#为什么分区（partition）？"><span class="nav-number">1.</span> <span class="nav-text">为什么分区（partition）？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#分区策略"><span class="nav-number">2.</span> <span class="nav-text">分区策略</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Partitioner接口"><span class="nav-number">2.1.</span> <span class="nav-text">Partitioner接口</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机策略"><span class="nav-number">2.2.</span> <span class="nav-text">随机策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#轮询策略"><span class="nav-number">2.3.</span> <span class="nav-text">轮询策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#黏性分区策略"><span class="nav-number">2.4.</span> <span class="nav-text">黏性分区策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#按消息key分区策略"><span class="nav-number">2.5.</span> <span class="nav-text">按消息key分区策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#自定义分区策略"><span class="nav-number">2.6.</span> <span class="nav-text">自定义分区策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DefaultPartitioner默认分区策略"><span class="nav-number">2.7.</span> <span class="nav-text">DefaultPartitioner默认分区策略</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#生产者分区逻辑"><span class="nav-number">3.</span> <span class="nav-text">生产者分区逻辑</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#小结"><span class="nav-number">4.</span> <span class="nav-text">小结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#附录"><span class="nav-number">5.</span> <span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#KIP-480：黏性分区策略（Sticky-Partitioner）"><span class="nav-number">5.1.</span> <span class="nav-text">KIP-480：黏性分区策略（Sticky Partitioner）</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="worry" src="/images/avatar.png"><p class="site-author-name" itemprop="name">worry</p><div class="site-description" itemprop="description">认真做事，踏实做人</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">2</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">10</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/huagetai" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;huagetai" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:xiaochawan@126.com" title="E-Mail &amp;rarr; mailto:xiaochawan@126.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">worry</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">70k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">2:55</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5dabd646b34cdc5c" async></script></div><div style="display:none"><script src="//s95.cnzz.com/z_stat.php?id=1000294390&web_id=1000294390"></script></div></div></footer></div><script src="/lib/anime.min.js?v=3.1.0"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script><script src="/js/schemes/pisces.js?v=7.4.1"></script><script src="/js/next-boot.js?v=7.4.1"></script><script src="/js/bookmark.js?v=7.4.1"></script><script src="/js/local-search.js?v=7.4.1"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '9a74dfd7f89e23278636',
      clientSecret: 'b094b8a3e7018d48e6399825701002decd060d45',
      repo: 'huagetai.github.io',
      owner: 'huagetai',
      admin: ['huagetai'],
      id: 'c43eeb6beceef33f116b98e8740bfe2d',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);</script><script>NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'MXw3KxWl0E0BqCmpb5hiJKlE-gzGzoHsz',
    appKey: 'ReqVUSvH190npS0g6Y7R9XIG',
    placeholder: "Just go go",
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: 'zh-cn' || 'zh-cn',
    path: location.pathname,
    recordIP: false,
    serverURLs: ''
  });
}, window.Valine);</script></body></html>